%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%g
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\input{local-macros.tex}

\mathversion{sans}

\newcommand{\arrow}{\rightsquigarrow}

\newcommand{\restrict}[1]{\lvert_{#1}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\Univ}{\mathbb{U}}
\newcommand{\Un}{\mathcal{U}}

\newcommand{\join}{\vee}

\newcommand{\conv}{^{\mspace{-2mu}\Downarrow\mspace{-2mu}}}

\newcommand{\meaningofconv}[1]{\left\llbracket{#1}\right\rrbracket\conv}

\newcommand{\arrowlift}{\ensuremath{lift}}
\newcommand{\arrowarr}{\ensuremath{arr}}
\newcommand{\arrowcomp}{\ensuremath{{>}\mspace{-6mu}{>}\mspace{-6mu}{>}}}
\newcommand{\arrowpair}{\ensuremath{\mathit{\&\mspace{-7.5mu}\&\mspace{-7.5mu}\&}}}
\newcommand{\arrowif}{\ensuremath{ifte}}
\newcommand{\arrowconvif}{\ensuremath{ifte\conv}}
\newcommand{\arrowlazy}{\ensuremath{lazy}}
\newcommand{\arrowapp}{\ensuremath{app}}
\newcommand{\arrowrun}{\ensuremath{run}}
\newcommand{\arrowget}{\ensuremath{get}}
\newcommand{\arrowerror}{\ensuremath{error}}
\newcommand{\arrowtrans}{\ensuremath{\eta}}

\newcommand{\gen}{_\mathrm{a}}
\newcommand{\genb}{_\mathrm{b}}
\newcommand{\genc}{_\mathrm{a^{\mspace{-2mu}*}}}
\newcommand{\gend}{_\mathrm{b^{\mspace{-2mu}*}}}

\DeclareMathOperator{\botto}{\arrow_{\mspace{-3mu}\bot}}
\newcommand{\arrbot}{\arrowarr_{\mspace{-3mu}\bot}}
\newcommand{\compbot}{\arrowcomp_{\mspace{-5mu}\bot}}
\newcommand{\pairbot}{\arrowpair_{\mspace{-3mu}\bot}}
\newcommand{\ifbot}{\arrowif_{\mspace{-2mu}\bot}}
\newcommand{\lazybot}{\arrowlazy_{\mspace{-2mu}\bot}}

\newcommand{\map}{_\mathrm{map}}
\DeclareMathOperator{\mapto}{\arrow_{\mspace{-21mu}\map}}
\newcommand{\liftmap}{\arrowlift\map}
\newcommand{\arrmap}{\arrowarr\map}
\newcommand{\compmap}{\arrowcomp\map}
\newcommand{\pairmap}{\arrowpair\map}
\newcommand{\ifmap}{\arrowif\map}
\newcommand{\lazymap}{\arrowlazy\map}

\newcommand{\pre}{_\mathrm{pre}}
\DeclareMathOperator{\preto}{\arrow_{\mspace{-19mu}\pre}}
\newcommand{\liftpre}{\arrowlift\pre}
\newcommand{\arrpre}{\arrowarr\pre}
\newcommand{\comppre}{\arrowcomp\pre}
\newcommand{\pairpre}{\arrowpair\pre}
\newcommand{\ifpre}{\arrowif\pre}
\newcommand{\lazypre}{\arrowlazy\pre}

\newcommand{\pbot}{{\bot^{\mspace{-4mu}*}}}
\DeclareMathOperator{\pbotto}{\arrow_{\mspace{-3mu}\pbot}}
\newcommand{\arrpbot}{\arrowarr_{\mspace{-3mu}\pbot}}
\newcommand{\comppbot}{\arrowcomp_{\mspace{-5mu}\pbot}}
\newcommand{\pairpbot}{\arrowpair_{\mspace{-3mu}\pbot}}
\newcommand{\ifpbot}{\arrowif_{\mspace{-2mu}\pbot}}
\newcommand{\convifpbot}{\arrowconvif_{\mspace{-2mu}\pbot}}
\newcommand{\lazypbot}{\arrowlazy_{\mspace{-2mu}\pbot}}

\newcommand{\pmap}{_\mathrm{map^{\mspace{-2mu}*}}}
\DeclareMathOperator{\pmapto}{\arrow_{\mspace{-22mu}_{\mathrm{map*}}}}
\newcommand{\liftpmap}{\arrowlift\pmap}
\newcommand{\arrpmap}{\arrowarr\pmap}
\newcommand{\comppmap}{\arrowcomp\pmap}
\newcommand{\pairpmap}{\arrowpair\pmap}
\newcommand{\ifpmap}{\arrowif\pmap}
\newcommand{\convifpmap}{\arrowconvif\pmap}
\newcommand{\lazypmap}{\arrowlazy\pmap}

\newcommand{\ppre}{_\mathrm{pre^{\mspace{-2mu}*}}}
\DeclareMathOperator{\ppreto}{\arrow_{\mspace{-19mu}_{\mathrm{pre*}}}}
\newcommand{\liftppre}{\arrowlift\ppre}
\newcommand{\arrppre}{\arrowarr\ppre}
\newcommand{\compppre}{\arrowcomp\ppre}
\newcommand{\pairppre}{\arrowpair\ppre}
\newcommand{\ifppre}{\arrowif\ppre}
\newcommand{\convifppre}{\arrowconvif\ppre}
\newcommand{\lazyppre}{\arrowlazy\ppre}

\newcommand{\prepto}{\pto_{\mspace{-19mu}\pre}}


\begin{document}

\conferenceinfo{POPL '14}{January 22-24, 2014, San Diego, CA, USA}
\copyrightyear{2014}
\copyrightdata{[to be supplied]} 

%\titlebanner{banner above paper title}        % These are ignored unless
%\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Running Probabilistic Programs Backward}
%\subtitle{Subtitle Text, if any}

\authorinfo{Neil Toronto \and Jay McCarthy}
           {PLT @ Brigham Young University}
           {ntoronto@racket-lang.org \and jay@cs.byu.edu}
%\authorinfo{Chris Grant}
%           {Brigham Young University}
%           {grant@math.byu.edu}
\maketitle

\begin{abstract}
It is primarily Bayesian statistical practice that drives probabilistic language development.
Unfortunately, languages developed by programming language researchers most often lack probabilitic conditioning, making them nearly useless in Bayesian practice.
Languages developed by Bayesian practitioners can be useful, but they lack specifications and are limited in seemingly arbitrary ways.

We develop a mathematical specification for a first-order language with recursion, extended with probabilistic choice and conditioning.
Our semantics interprets programs as functions that compute preimages of sets of outputs.
Distributions over outputs are defined by the probabilities of their preimages, a measure-theoretic approach that ensures the language is not artificially limited.

Measurability is a basic property similar to continuity that is critical, but often neglected.
As part of proving correctness, we prove that all programs, including partial programs with real arithmetic, are measurable.

Functions that compute preimages are generally uncomputable, so we develop an additional approximating semantics for computing rectangular covers of preimages.
We implement the approximating semantics directly in Typed Racket and Haskell.
\end{abstract}

\category{XXX-CR-number}{XXX-subcategory}{XXX-third-level}

\terms
XXX, XXX

\keywords
XXX, XXX

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

XXX: consider using paragraph headers instead of subsections and subsubsections

\section{Introduction}

There is currently no efficient probabilistic language implementation that simultaneously
\begin{enumerate}
	\item Places no extraneous restrictions on legal programs.
	\item Allows \keyword{conditioning}, or restricting the output in a way that preserves relative probabilities.
	\item Has a semantics, or a mathematical specification.
\end{enumerate}
In the field of programming language research, there are a few examples of languages that do not restrict legal programs and have a semantics (XXX: cite all).
Unfortunately, most of the demand for probabilistic languages comes from Bayesian practice, which requires conditioning.

Bayesian practitioners have implemented many probabilistic languages with conditioning (XXX: cite all).
Almost all lack a semantics, so it is impossible to distinguish between implementation errors and errors in users' understanding.
Almost all place restrictions on programs, most commonly disallowing recursion, allowing only continuous distributions, and allowing only very limited forms of conditioning.

These common restrictions arise from reasoning about probability using \keyword{densities}, which are functions from random values to \emph{changes} in probability.
While simple and convenient, densities have many limitations.
Densities for random values with different dimension are incomparable.
Densities cannot be defined on infinite products.
Densities can only be used to reason about conditioning in limited cases.
(XXX: make the last sentence more specific, or remove it because it communicates nothing extra)

Densities cannot define distributions of discontinuous functions of random variables.
For example, suppose we want to model a thermometer that reports in the range $[0,100]$, and that the temperature it would report (if it could) is distributed according to a bell curve.
We might encode the process like this:
\begin{equation}
	t'\ :=\ \lzfclet{t & normal~\mu~1}{max~0~(min~100~t)}
\label{eqn:thermometer-example}
\end{equation}
While $t$'s distribution has a density (a standard bell curve at mean $\mu$), the distribution of $t'$ does not.
%Using densities, this program cannot have a meaning.
%Placing a condition on the value of $t'$ to discover how restricting its value would affect $\mu$'s distribution---an activity that should be central to Bayesian practice---is unfathomable.

The restrictions placed on programs are indeed onerous, if such benign uses of $min$ and $max$ must be disallowed.
We cannot even model measuring devices correctly.

\subsection{Measure-Theoretic Probability}

Measure-theoretic probability (XXX: cite) is widely believed to be able to define everything reasonable that densities cannot.
It mainly does this by \emph{assigning probabilities to sets} instead of \emph{assigning changes in probability to values}.

If $P$ assigns probabilities to subsets of $X$ and $f : X \to Y$, then the \keyword{preimage measure}
\begin{equation}
	P~(preimage~f~B)
\end{equation}
defines the distribution over subsets $B$ of $Y$, where $preimage$ returns the subset of $f$'s domain $X$ for which $f$ yields a value in $B$.
In the thermometer example~\eqref{eqn:thermometer-example}, $f$ would be an interpretation of the program as a function, $X$ would be the set of all random sources, and $Y$ would be the set of the program's possible outputs.
For any $B \subseteq Y$, $preimage~f~B$ would be well-defined, regardless of discontinuities.

Unfortunately, there is a complicated technical restriction: only \emph{measurable} subsets of $X$ and $Y$ can be assigned probabilities.
Conditioning on zero-probability sets can also be quite difficult.
These complexities tend to drive practitioners to densities, even though they are so limited.

\subsection{Measure-Theoretic Semantics}

Because purely functional languages do not allow side-effects (except usually divergence), programmers must write probabilistic programs as functions from a random source to outputs.
Monads and other categorical classes such as idioms and arrows can make doing so easier. (XXX: cite me, bunch of others)

It seems this approach should make it easy to interpret probabilistic programs measure-theoretically.
For a probabilistic program $f : X \to Y$, the probability measure on output sets $B \subseteq Y$ should be defined by preimages of $B$ under $f$ and the probability measure on $X$.
Unfortunately, it is difficult to turn this simple-sounding idea into a compositional semantics, for the following reasons.
\begin{enumerate}
	\item Preimages can be defined only for functions with observable domains, which excludes lambdas. \label{problem:observable-domain}
	\item Requiring subsets of $X$ and $Y$ to be measurable constrains $f$: preimages of measurable subsets of $Y$ must be measurable subsets of $X$. Proving the conditions under which this is true is difficult, especially when $f$ may diverge. \label{problem:measurability}
	\item It is very difficult to define probability measures for arbitrary spaces of measurable functions (XXX: cite Aumann). \label{problem:higher-orderness}
\end{enumerate}
Implementing a language based on such a semantics is complicated because
\begin{enumerate}
	\setcounter{enumi}{3}
	\item Contemporary mathematics is unlike any implementation's host language. \label{problem:different-language}
	\item It requires running Turing-equivalent programs backward, efficiently, on possibly uncountable sets of outputs.\label{problem:backward-efficient}
\end{enumerate}

We address both~\ref{problem:observable-domain} and~\ref{problem:different-language} by developing our semantics in \lzfclang~\cite{cit:toronto-2012flops-lzfc}, a $\lambda$-calculus with infinite sets, and both extensional and intensional functions.
We address~\ref{problem:backward-efficient} by deriving and implementing a \emph{conservative approximation} of the semantics.

There seems to be no way to simplify difficulty~\ref{problem:measurability}, so we work through it in Section~\ref{sec:measurability}.
The outcome is worth it: we prove that all probabilistic programs are measurable, regardless of which inputs they diverge on.
This includes uncomputable programs; for example, those that contain real equality tests and limits.
We believe this result is the first of its kind, and is general enough to apply to almost all past and future work on probabilistic programming languages.

For difficulty~\ref{problem:higher-orderness}, we have discovered that the ``first-orderness'' of arrows is a perfect fit for the ``first-orderness'' of measure theory.

\subsection{Arrow Solution Overview}

\newcommand{\youarehere}[1]%
{%
\begin{equation}%
\begin{CD}%
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\%
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\%
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y%
\end{CD}%
\label{#1}%
\end{equation}%
}

Using arrows, we define an \emph{exact} semantics and an \emph{approximating} semantics.
Our exact semantics consists of
\begin{itemize}
	\item A semantic function which, like the semantic function for the arrow calculus~(XXX: cite Hughes or Lindley), transforms first-order programs into the computations of an arbitrary arrow.
	\item Arrows for evaluating expressions in different ways.
\end{itemize}
This commutative diagram describes the relationships among the arrows used to define the exact semantics:
\youarehere{eqn:roadmap-diagram1}
From top-left to top-right, $X \botto Y$ computations are intensional functions that may raise errors, $X \mapto Y$ computations produce extensional functions, and $X \preto Y$ computations compute preimages.
The computations of the arrows in the bottom row are equivalent to those in the top, except they always converge.
We can do this because in \lzfclang, Turing-uncomputable programs are definable.

Our approximating semantics consists of the same semantic function and an arrow $X \ppreto' Y$, derived from $X \ppreto Y$, for computing conservative approximations of preimages.

An implementation implements (XXX: badness) the semantic function, and the $X \botto Y$ and $X \ppreto' Y$ arrows' combinators.

Most of our correctness theorems rely on proofs that every $\arrowlift$ and $\arrowtrans$ in~\eqref{eqn:roadmap-diagram1} is a homomorphism.
We use $\arrowlift$ and $\arrowtrans$ to define the correctness of one arrow in terms of another arrow.
Homomorphism properties allow $\arrowlift$ and $\arrowtrans$ to distribute over the other arrow's computations.

From here on, significant terms are introduced in \keyword{bold}, with those we invent introduced in \mykeyword{bold italics}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Operational Metalanguage}

We write all of the programs in this paper in \lzfclang~\cite{cit:toronto-2012flops-lzfc}, an untyped, call-by-value $\lambda$-calculus designed for deriving implementable programs from contemporary mathematics.

Generally, contemporary mathematics---measure theory in particular---is done in \keyword{ZFC}: \keyword{Zermelo-Fraenkel} set theory extended with the axiom of \keyword{Choice} (equivalently unique \keyword{Cardinality}).
ZFC has only first-order functions and no general recursion, which makes implementing a language defined by a transformation into ZFC quite difficult.
The problem is exacerbated if implementing the language requires approximation.
Targeting \lzfclang instead allows creating a precise mathematical specification and deriving an approximating specification without changing languages.

In \lzfclang, essentially every set is a value, as well as every lambda and every set of lambdas.
All operations, including operations on infinite sets, are assumed to complete instantly if they converge.\footnote{An example of a diverging \lzfclang function is one that attempts to decide whether arbitrary \lzfclang expressions converge.}

Almost everything definable in ZFC can be formally defined by a finite \lzfclang program, except objects that most mathematicians would agree are nonconstructive.
More precisely, any set that \emph{must} be defined by a statement of existence and uniqueness without giving a bounding set is not definable by a \emph{finite} \lzfclang program.

Because \lzfclang includes an inner model of ZFC, essentially every ZFC theorem applies to \lzfclang's set values without alteration.
Further, proofs about \lzfclang's set values apply to ZFC sets.\footnote{Assuming the existence of an inaccessible cardinal.}

In \lzfclang, algebraic data structures are encoded as sets; e.g. a \mykeyword{primitive ordered pair} of $x$ and $y$ is $\set{\set{x},\set{x,y}}$.
Only the \emph{existence} of encodings into sets is important, as it means data structures inherit a defining characteristic of sets: strictness.
More precisely, the lengths of paths to data structure leaves is unbounded, but each path must be finite.
Less precisely, data may be ``infinitely wide'' (such as $\Re$) but not ``infinitely tall'' (such as infinite trees and lists).

%We assume data structures, including pairs, are encoded as \emph{primitive} ordered pairs with the first element a unique tag, so they can be distinguished by checking tags.
%sAccessors such as $fst$ and $snd$ are trivial to define.

\lzfclang is untyped so its users can define an auxiliary type system that best suits their application area.
For this work, we use a manually checked, polymorphic type system characterized by these rules:
\begin{itemize}
	\item A free lowercase type variable is universally quantified.
	\item A free uppercase type variable is a set.
	\item A set denotes a member of that set.
	\item $x \tto y$ denotes a partial function.
	\item $\pair{x,y}$ denotes a pair of values with types $x$ and $y$.
	\item $Set~x$ denotes a set with members of type $x$.
\end{itemize}
Because the type $Set~X$ denotes the same values as the set $\powerset~X$ (i.e. subsets of $X$) we regard them as equivalent.
Similarly, the type $\pair{X,Y}$ is equivalent to the set $X \times Y$.

We write \lzfclang programs in heavily sugared $\lambda$-calculus syntax, with an $if$ expression and these additional primitives:
\begin{equation}
\begin{aligned}
	\begin{aligned}
		true &: Bool \\
		false &: Bool \\
		\emptyset &: Set~x \\
		\omega &: Ord \\
		take &: Set~x \tto x \\
	\end{aligned}
	&\tab
	\begin{aligned}
		(\in) &: x \tto Set~x \tto Bool \\
		\powerset &: Set~x \tto Set~(Set~x) \\
		\U &: Set~(Set~x) \tto Set~x \\
		image &: (x \tto y) \tto Set~x \tto Set~y \\
		|\cdot| &: Set~x \tto Ord \\
	\end{aligned} \\
\end{aligned}
\label{eqn:lzfc-prims}
\end{equation}
Shortly, $\emptyset$ is the empty set, $\omega$ is the cardinality of the natural numbers, $take~\set{x}$ reduces to $x$ and diverges for non-singleton sets, $x \in A$ decides membership, $\powerset~A$ reduces to the set of subsets of $A$, $\U\A$ reduces to the union of the sets in $\A$, $image~f~A$ applies $f$ to each member of $A$ and reduces to the set of results, and $|A|$ reduces to the cardinality of $A$.

We assume literal set notation such as $\set{0,1,2}$ is already defined in terms of the set primitives.

We import applicable ZFC theorems as lemmas.

\subsection{Internal and External Equality}

Set theory extends first-order logic with an axiom that defines equality to be extensional, and with axioms that ensure the existence of sets in the domain of discourse.
\lzfclang is defined the same way as any other operational $\lambda$-calculus: by (conservatively) extending the domain of discourse with expressions and defining a reduction relation.

While \lzfclang does not have an equality primitive, set theory's extensional equality can be recovered internally using $(\in)$.
\emph{Internal} extensional equality is defined by either of the following equivalent statements:
\begin{equation}
\begin{aligned}
	x = y &\ := \ x \in \set{y} \\
	(=) &\ := \ \fun{x}\fun{y}{x \in \set{y}}
\end{aligned}
\end{equation}
Thus, $1 = 1$ reduces to $1 \in \set{1}$, which reduces to $true$.\footnote{Technically, \lzfclang has a big-step semantics, and
the derivation tree for $1 = 1$ contains the derivation tree for $1 \in \set{1}$.}
Because of the particular way \lzfclang's lambda terms are defined, for two lambda terms $f$ and $g$, $f = g$ reduces to $true$ when $f$ and $g$ are structurally identical modulo renaming.
For example, $(\fun{x}{x}) = (\fun{y}{y})$ reduces to $true$, but $(\fun{x}{2}) = (\fun{x}{1+1})$ reduces to $false$.

We understand any \lzfclang term $\mathit{e}$ used as a truth statement as shorthand for ``$\mathit{e}$ reduces to $true$.''
Therefore, while the terms $(\fun{x}{x})~1$ and $1$ are (externally, extensionally) unequal, we can say that $(\fun{x}{x})~1 = 1$.

Any truth statement $\mathit{e}$ implies that $\mathit{e}$ converges.
In particular, the truth statement $\mathit{e}_1 = \mathit{e}_2$ implies that both $\mathit{e}_1$ and $\mathit{e}_2$ converge.
However, we often want to say that $\mathit{e}_1$ and $\mathit{e}_1$ are equivalent when they both diverge.
In these cases, we use a slightly weaker equivalence.

\begin{definition}[observational equivalence]
Two \lzfclang terms $\mathit{e_1}$ and $\mathit{e_2}$ are \keyword{observationally equivalent}, written $\mathit{e_1} \equiv \mathit{e_2}$, when $\mathit{e_1} = \mathit{e_2}$ or both $\mathit{e_1}$ and $\mathit{e_2}$ diverge.
\end{definition}

It might seem helpful to introduce even coarser notions of equivalence, such as applicative or logical bisimilarity (XXX: cite).
However, we do not want internal equality and external equivalence to differ too much, and we want the flexibility of extending ``$\equiv$'' with type-specific rules.

\subsection{Additional Functions and Forms}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&domain : (X \pto Y) \tto Set~X \\
		&domain \ := \ image~fst \\
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&range : (X \pto Y) \tto Set~Y \\
		&range \ := \ image~snd
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&preimage : (X \pto Y) \tto Set~Y \tto Set~X \\
		&preimage~f~B \ :=\ \setb{a \in domain~f}{f~a \in B}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&restrict : (X \pto Y) \tto Set~X \tto (X \pto Y) \\
		&restrict~f~A \ := \ \fun{a \in (A \i domain~f)}{f~a}
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\map : (X \pto Y_1) \tto (X \pto Y_2) \tto (X \pto Y_1 \times Y_2) \\
		&\pair{g_1,g_2}\map \ := \ 
			\lzfclet{
				A & (domain~g_1) \i (domain~g_2)
			}{\fun{a \in A}{\pair{g_1~a,g_2~a}}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\map) : (Y \pto Z) \tto (X \pto Y) \tto (X \pto Z) \\
		&g_2 \circ\map g_1 \ := \ 
			\lzfclet{
				A & preimage~g_1~(domain~g_2)
			}{\fun{a \in A}{g_2~(g_1~a)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\map) : (X \pto Y) \tto (X \pto Y) \tto (X \pto Y) \\
		&g_1 \uplus\map g_2 \ := \ 
			\lzfclet{
				A & (domain~g_1) \uplus (domain~g_2)
			}{\fun{a \in A}{if~(a \in domain~g_1)~(g_1~a)~(g_2~a)}}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Operations on mappings.}
\label{fig:mapping-defs}
\end{figure*}

We assume a desugaring pass over \lzfclang expressions, which automatically curries (including for the two-argument primitives $(\in)$ and $image$), and interprets special binding forms such as indexed unions $\U_{\mathit{x} \in \mathit{e_A}}\mathit{e}$, destructuring binds as in $swap~\pair{x,y} := \pair{y,x}$, and comprehensions like $\setb{x \in A}{x \in B}$.
We assume we have logical operators, bounded quantifiers, and typical set operations.

A less typical set operation we use is disjoint union:
\begin{equation}
\begin{aligned}
	&(\uplus) : Set~x \tto Set~x \tto Set~x \\
	&A \uplus B \ := \ if~(A \i B = \emptyset)~(A \u B)~(take~\emptyset)
\end{aligned}
\end{equation}
$A \uplus B$ diverges when $A$ and $B$ overlap.

In set theory, functions are extensional---everything about them is observable---because they are encoded as sets of input-output pairs.
The increment function for the natural numbers, for example, is $\set{\pair{0,1},\pair{1,2},\pair{2,3},...}$.
We call extensional functions \mykeyword{mappings} and intensional functions \keyword{lambdas}, and use the word \keyword{function} to mean either.
For convenience, as with lambdas, we use adjacency (e.g. $(f~x)$) to apply mappings.

Syntax for unnamed mappings is defined by
\begin{align}
	&\fun{\mathit{x_a} \in \mathit{e_A}}{\mathit{e_b}} \ :\equiv\ mapping~(\fun{\mathit{x_a}}\mathit{e_b})~\mathit{e_A} \\
\nonumber\\[-6pt]
	&\begin{aligned}
		&mapping : (X \tto Y) \tto Set~X \tto (X \pto Y) \\
		&mapping~f~A \ := \ image~(\fun{a}{\pair{a,f~a}})~A
	\end{aligned}
\end{align}
Figure~\ref{fig:mapping-defs} defines other common operations on mappings: $domain$, $range$, $preimage$, $restrict$, pairing, composition, and disjoint union.
The latter three are particularly important in the preimage arrow's derivation, and $preimage$ is critical in measure theory's account of probability.
For symmetry with partial functions $x \tto y$, they are defined on $X \pto Y$, where $X \pto Y$ is the set of all partial mappings from any domain set $X$ to any codomain set $Y$.

The set $X \to Y$ contains all the \emph{total} mappings from $X$ to $Y$.
We use total mappings as possibly infinite vectors, with application for indexing.
Indexing functions are produced by
\begin{equation}
\begin{aligned}
	&\pi : J \tto (J \to X) \tto X \\
	&\pi~j~f \ := \ f~j
\end{aligned}
\end{equation}
which is particularly useful when $f$ is unnamed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Arrows and First-Order Semantics}

Like monads and idioms (XXX: cite Wadler, McBride), arrows (XXX: cite Hughes) are used in functional programming to thread effects through computations in a way that imposes structure on the computations.
Unlike monad and idiom computations, arrow computations are always
\begin{itemize}
	\item Function-like: An arrow computation of type $x \arrow y$ must behave like a corresponding function of type $x \tto y$ (in a sense we formalize shortly).
	\item First-order: There is no way to derive a computation $app : \pair{x \arrow y, x} \arrow y$ from an arrow's minimal definition.
\end{itemize}
The first property makes arrows a perfect fit for a compositional translation from expressions to intensional or extensional functions---or, as we will see, to computations that compute preimages.
The second property makes them a perfect fit for a measure-theoretic semantics in particular, as $app$ in the function arrow is generally not measurable (XXX: cite Aumann).
Targeting arrows in the semantics therefore gives some assurance that we can meet measure theory's requirement that preimage measure be defined only for measurable functions.
We prove in Section~\ref{sec:measurability} that it is sufficient.

\subsection{Alternative Arrow Definitions and Laws}
\label{sec:arrow-definitions}

We do not give typical minimal arrow definitions.
For each arrow $a$, instead of $first\gen$, we define $(\arrowpair\gen)$---typically called \keyword{fanout}, but its use will be clearer if we call it \keyword{pairing}---which applies two functions to an input and returns the pair of their outputs.
Though $first\gen$ may be defined in terms of $(\arrowpair\gen)$ and vice-versa~\cite{cit:hughes-2005afp-arrows}, we give $(\arrowpair\gen)$ definitions because the applicable measure-theoretic theorems are in terms of pairing functions.

One way to strengthen an arrow $a$ is to define an additional combinator $left\gen$, which can be used to choose an arrow computation based on the result of another.
Again, we define a different combinator, $\arrowif\gen$ (``if-then-else''), to make applying measure-theoretic theorems easier.

In a nonstrict $\lambda$-calculus, simply defining a choice combinator allows writing recursive functions using nothing but arrow combinators and lifted, pure functions.
However, any strict $\lambda$-calculus (such as \lzfclang) requires an extra combinator to defer computations in conditional branches.

For example, define the \keyword{function arrow} with choice:
\begin{equation}
\begin{aligned}
	\arrowarr~f &\ := \ f \\
	(f_1~\arrowcomp~f_2)~a &\ := \ f_2~(f_1~a) \\
	(f_1~\arrowpair~f_2)~a &\ := \ \pair{f_1~a,f_2,a} \\
	\arrowif~f_1~f_2~f_3~a &\ := \ if~(f_1~a)~(f_2~a)~(f_3~a) \\
\end{aligned}
\label{eqn:function-arrow}
\end{equation}
and try to define the following recursive function:
\begin{equation}
	halt!on!true \ := \ \arrowif~(\arrowarr~id)~(\arrowarr~id)~halt!on!true
\end{equation}
The defining expression diverges in a strict $\lambda$-calculus.
In a nonstrict $\lambda$-calculus, it diverges only when applied to $false$.

Using $\arrowlazy~f~a := f~0~a$, which receives thunks and returns arrow computations, we can write $halt!on!true$ as
\begin{equation}
\begin{aligned}
	&halt!on!true \ := \ 
	\arrowif~(\arrowarr~id)~(\arrowarr~id)~(\arrowlazy~\fun{0}{halt!on!true})
\end{aligned}
\end{equation}
which diverges only when applied to $false$ in any $\lambda$-calculus.

\begin{definition}[arrow with choice]A binary type constructor $(\arrow\gen)$ and the combinators
\begin{equation}
\begin{aligned}
	\arrowarr\gen &: (x \tto y) \tto (x \arrow\gen y)
\\
	(\arrowcomp\gen) &: (x \arrow\gen y) \tto (y \arrow\gen z) \tto (x \arrow\gen z)
\\
	(\arrowpair\gen) &: (x \arrow\gen y) \tto (x \arrow\gen z) \tto (x \arrow\gen \pair{y,z})
\end{aligned}
\label{eqn:arrow-combinators}
\end{equation}
define an \keyword{arrow} if certain monoid, homomorphism, and structural laws hold.
(XXX: need to require $\arrowarr\gen$'s argument to be total)
The additional combinators
\begin{equation}
\begin{aligned}
	\arrowif\gen &: (x \arrow\gen Bool) \tto (x \arrow\gen y) \tto (x \arrow\gen y) \tto (x \arrow\gen y)
\\
	\arrowlazy\gen &: (1 \tto (x \arrow\gen y)) \tto (x \arrow\gen y)
\end{aligned}
\end{equation}
where $1 = \set{0}$, define an \keyword{arrow with choice} if certain additional homomorphism and structural laws hold.
\end{definition}

From here on, as all of our arrows are arrows with choice, we simply call them arrows.

The necessary homomorphism laws ensure that $\arrowarr\gen$ distributes over function arrow combinators.
These laws can be put in terms of more general homomorphism properties that deal with distributing an arrow-to-arrow lift, which we use extensively to prove correctness.

\begin{definition}[arrow homomorphism]
A function $lift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an \mykeyword{arrow homomorphism} from arrow $\mathrm{a}$ to arrow $\mathrm{b}$ if the following distributive laws hold for appropriately typed $f$, $f_1$, $f_2$ and $f_3$:
\begin{align}
	lift\genb~(\arrowarr\gen~f) &\ \equiv \ \arrowarr\genb~f
	\label{eqn:lift-distributes-over-arr}
\\
	lift\genb~(f_1~\arrowcomp\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowcomp\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-comp}
\\
	lift\genb~(f_1~\arrowpair\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowpair\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-pair}
\\
	\arrowlift\genb~(\arrowif\gen~f_1~f_2~f_3) &\ \equiv \ 
		\arrowif\genb~(lift\genb~f_1)~(lift\genb~f_2)~(lift\genb~f_3)
	\label{eqn:lift-distributes-over-if}
\\
	\arrowlift\genb~(\arrowlazy\gen~f) &\ \equiv \
		\arrowlazy\genb~\fun{0}{\arrowlift\genb~(f~0)}
	\label{eqn:lift-distributes-over-lazy}
\end{align}
\label{def:arrow-homomorphism}
\end{definition}

The arrow homomorphism laws state that $\arrowarr\gen$ must be a homomorphism from the function arrow~\eqref{eqn:function-arrow} to arrow $a$.
Roughly, arrow computations that do not use additional combinators can be transformed into $\arrowarr\gen$ applied to a pure computation.
They must be \emph{function-like}.

Only a few of the other arrow laws play a role in our semantics and its correctness.
We need associativity of $(\arrowcomp\gen)$:
\begin{equation}
	(f_1~\arrowcomp\gen~f_2)~\arrowcomp\gen~f_3 \ \equiv \ f_1~\arrowcomp\gen~(f_2~\arrowcomp\gen~f_3)
\label{eqn:comp-is-associative}
\end{equation}
a pair extraction law:
\begin{equation}
	(\arrowarr\gen~f_1~\arrowpair\gen~f_2)~\arrowcomp\gen~\arrowarr\gen~snd \ \equiv \ f_2
\label{eqn:pair-extraction}
\end{equation}
and distribution of pure computations over effectful:
\begin{align}
	\arrowarr\gen~f_1~\arrowcomp\gen~(f_2~\arrowpair\gen~f_3) &\ \equiv \ 
		\lzfcsplit{
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_2)~\arrowpair\gen \\
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_3)}
\label{eqn:pure-distributes-over-pair}
\\
	\arrowarr\gen~f_1~\arrowcomp\gen~\arrowif\gen~f_2~f_3~f_4 &\ \equiv \
		\arrowif\gen~\lzfcsplit{
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_2) \\
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_3) \\
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_4)}
\label{eqn:pure-distributes-over-if}
\\
	\arrowarr\gen~f_1~\arrowcomp\gen~\arrowlazy\gen~f_2 &\ \equiv \
		\arrowlazy\gen~\fun{0}{\arrowarr\gen~f_1~\arrowcomp\gen~f_2~0}
\label{eqn:pure-distributes-over-lazy}
\end{align}
Because arrows have traditionally been defined in nonstrict and strongly normalizing $\lambda$-calculii, we could not derive~\eqref{eqn:pure-distributes-over-lazy} from existing arrow laws.
We are sure it is reasonable. (XXX: need better reason)

Rather than prove each necessary arrow law, we prove the arrows are \emph{epimorphic} (not necessarily \emph{isomorphic}) to arrows for which the laws hold.

\begin{definition}[arrow epimorphism]
An arrow homomorphism $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an \mykeyword{arrow epimorphism} from arrow $a$ to $b$ if it has a right inverse.
\label{def:arrow-epimorphism}
\end{definition}

\begin{theorem}
If $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an arrow epimorphism and the combinators of $a$ define an arrow, then the combinators of $b$ define an arrow.
\label{thm:arrow-epimorphism}
\end{theorem}
\begin{proof}
For the pair extraction law~\eqref{eqn:pair-extraction}, rewrite in terms of $\arrowlift\genb$, apply homomorphism laws, and apply the pair extraction law for arrow $a$:
\begin{align*}
	&(\arrowarr\genb~f_1~\arrowpair\genb~f_2)~\arrowcomp\genb~\arrowarr\genb~snd
\\
	&\tab\equiv\ (\arrowlift\genb~(\arrowarr\gen~f_1)~\arrowpair\genb~(\arrowlift\genb~(\arrowlift\genb^{-1}~f_2)))~\arrowcomp\genb~\arrowarr\genb~snd
\\
	&\tab\equiv\ \arrowlift\genb~(\arrowarr\gen~f_1~\arrowpair\gen~\arrowlift\genb^{-1}~f_2)~\arrowcomp\genb~\arrowlift\genb~(\arrowarr\gen~snd)
\\
	&\tab\equiv\ \arrowlift\genb~((\arrowarr\gen~f_1~\arrowpair\gen~\arrowlift\genb^{-1}~f_2)~\arrowcomp\genb~\arrowarr\gen~snd)
\\
	&\tab\equiv\ \arrowlift\genb~(\arrowlift\genb^{-1}~f_2)
	\ \equiv\ f_2
\end{align*}
The proofs for every other law are similar.
\end{proof}


\subsection{First-Order Let-Calculus Semantics}

Figure~\ref{fig:semantic-function} defines a transformation $\meaningof{\cdot}\gen$ from a first-order let-calculus to arrow computations for any arrow $a$.

\begin{figure*}[t]
\begin{align*}
	\mathit{p} &\ ::\equiv \ \mathit{x := e};\ ...\ ; \mathit{e} \\
	\mathit{e} &\ ::\equiv \ \mathit{x~e}\ |\ let~\mathit{e~e}\ |\ env~\mathit{n}\ |\ \mathit{\pair{e,e}}\ |\ fst~\mathit{e}\ |\ snd~\mathit{e}\ |\ if~\mathit{e~e~e}\ |\ \mathit{v}\ |\ \cdots \\
	\mathit{v} &\ ::\equiv \ \text{[first-order constants]}
\end{align*}
\begin{align*}
\begin{aligned}[t]
	\meaningof{\mathit{x} := \mathit{e};\ ...\ ; \mathit{e_{body}}}\gen &\ :\equiv\
		\mathit{x} := \meaningof{\mathit{e}}\gen;\ ...\ ; \meaningof{\mathit{e_{body}}}\gen \\
	\meaningof{\mathit{x}~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\pair{\mathit{e},\pair{}}}\gen~\arrowcomp\gen~\mathit{x}
\\
	\meaningof{let~\mathit{e}~\mathit{e_{body}}}\gen &\ :\equiv\ 
		(\meaningof{\mathit{e}}\gen~\arrowpair\gen~\arrowarr\gen~id)~
			\arrowcomp\gen~
		\meaningof{\mathit{e_{body}}}\gen
\\
	\meaningof{env~0}\gen &\ :\equiv\ \arrowarr\gen~fst
\\
	\meaningof{env~(\mathit{n}+1)}\gen &\ :\equiv\ \arrowarr\gen~snd~\arrowcomp\gen~\meaningof{env~\mathit{n}}\gen
\\[6pt]
	id &\ := \ \fun{a} a
\\
	const~b &\ := \ \fun{a} b
\\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen &\ :\equiv\
		\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen
\\
	\meaningof{fst~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~fst
\\
	\meaningof{snd~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~snd
\\
	\meaningof{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\gen &\ :\equiv\
		\arrowif\gen~
			\meaningof{\mathit{e_c}}\gen~
			(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_t}}\gen})~
			(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_f}}\gen})
\\
	\meaningof{\mathit{v}}\gen &\ :\equiv\ \arrowarr\gen~(const~\mathit{v})
\\
	&\ \cdots
\\[6pt]
	\text{subject to} &\ \meaningof{\mathit{p}}\gen : \pair{} \arrow\gen \ \text{for some $y$}
\end{aligned}
\end{align*}
\hrule
\caption{Transformation from a let-calculus with first-order definitions and De-Bruijn-indexed bindings to computations in arrow $\mathrm{a}$.
%The type of a transformed expression is $1 \arrow\gen X$, or an arrow from the empty stack $\gamma = 0$ to a value of type $X$.
}
\label{fig:semantic-function}
\end{figure*}

A program is a sequence of definition statements followed by a final expression.
$\meaningof{\cdot}\gen$ compositionally transforms each defining expression and the final expression into arrow computations.
Functions are named, but local variables and arguments are not.
Instead, variables are referred to by De Bruijn indexes, with $0$ referring to the innermost binding.

Perhaps unsurprisingly, the interpretation acts like a stack machine.
The final expression has type $\pair{} \arrow\gen y$, where $y$ is the type of the program's value, and $\pair{}$ denotes an empty list.
Let-bindings push values onto the stack.
First-order functions have type $\pair{x,\pair{}} \arrow\gen y$ where $x$ is the argument type and $y$ is the return type.
Application sends a stack containing just an $x$.

Unless there is a reason to distinguish programs and expressions, we regard programs as if they were their final expressions.
Thus, the following definition applies to both.

\begin{definition}[well-defined expression]
An expression $\mathit{e}$ is \keyword{well-defined} under arrow $a$ if $\meaningof{\mathit{e}}\gen : x \arrow\gen y$ for some $x$ and $y$, and $\meaningof{\mathit{e}}\gen$ converges.
\label{def:well-defined-expression}
\end{definition}

From here on, we assume all expressions are well-defined.
(The arrow $a$ will be clear from context.)
This does not guarantee that \emph{running} any given interpretation converges; it just simplifies unqualified statements about expressions.

An example is the following theorem, on which most of our semantic correctness theorems rely.

\begin{theorem}[homomorphisms distribute over expressions]
Let $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ be an arrow homomorphism.
For all expressions $\mathit{e}$, $\meaningof{\mathit{e}}\genb \equiv \arrowlift\genb~\meaningof{\mathit{e}}\gen$.
\label{thm:homomorphism-implies-correct}
\end{theorem}
\begin{proof}
By structural induction.

Bases cases proceed by expansion and using $\arrowarr\genb \equiv \arrowlift\genb \circ \arrowarr\gen$~\eqref{eqn:lift-distributes-over-arr}. For example, for constants:
\begin{align*}
	\meaningof{\mathit{v}}\genb
		&\ \equiv\ \arrowarr\genb~(const~\mathit{v}) \\
		&\ \equiv\ \arrowlift\genb~(\arrowarr\gen~(const~\mathit{v})) \\
		&\ \equiv\ \arrowlift\genb~\meaningof{\mathit{v}}\gen
\end{align*}
Inductive cases proceed by expansion, applying the inductive hypothesis on subterms, and applying one or more distributive laws~\eqref{eqn:lift-distributes-over-comp}--\eqref{eqn:lift-distributes-over-lazy}.
For example, for pairing:
\begin{align*}
	\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\genb
		&\ \equiv\ \meaningof{\mathit{e}_1}\genb~\arrowpair\genb~\meaningof{\mathit{e}_2}\genb \\
		&\ \equiv\ (\arrowlift\genb~\meaningof{\mathit{e}_1}\gen)~\arrowpair\genb~(\arrowlift\genb~\meaningof{\mathit{e}_2}\gen) \\
		&\ \equiv\ \arrowlift\genb~(\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen) \\
		&\ \equiv\ \arrowlift\genb~\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen
\end{align*}
It is not hard to check the remaining cases.
\end{proof}

If we assume that $\arrowlift\genb$ defines correct behavior for arrow $b$ in terms of arrow $a$, and prove that $\arrowlift\genb$ is a homomorphism, then by Theorem~\ref{thm:homomorphism-implies-correct}, $\meaningof{\cdot}\genb$ is correct.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bottom and Mapping Arrows}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&x \botto y \ ::= \ x \tto y_\bot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrbot : (x \tto y) \tto (x \botto y) \\
		&\arrbot~f \ := \ f
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compbot) : (x \botto y) \tto (y \botto z) \tto (x \botto z) \\
		&(f_1~\compbot~f_2)~a \ := \ if~(f_1~a = \bot)~\bot~(f_2~(f_1~a))
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairbot) : (x \botto {y_1}) \tto (x \botto {y_2}) \tto (x \botto \pair{y_1,y_2}) \\
		&(f_1~\pairbot~f_2)~a \ := \ if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifbot : (x \botto Bool) \tto (x \botto y) \tto (x \botto y) \tto (x \botto y) \\
		&\ifbot~f_1~f_2~f_3~a \ := \
			\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ \bot & \bot}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazybot : (1 \tto (x \botto y)) \tto (x \botto y) \\
		&\lazybot~f~a \ := \ f~0~a
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Bottom arrow definitions.}
\label{fig:bottom-arrow-defs}
\end{figure*}

Using the diagram in~\eqref{eqn:roadmap-diagram1} as a sort of map, we are starting in the upper-left corner:
\youarehere{eqn:roadmap-diagram2}
Through Section~\ref{sec:preimage-arrow}, we move across the top to $X \preto Y$.

To use Theorem~\ref{thm:homomorphism-implies-correct} to prove correct the interpretations of expressions as preimage arrow computations, we need the preimage arrow to be homomorphic to a simpler arrow whose behavior is obviously correct.
One obvious candidate is the function arrow~\eqref{eqn:function-arrow}.
However, we will need to explicitly handle divergence as an error value, so we need a slightly more complicated arrow for which running computations may raise an error.

Figure~\ref{fig:bottom-arrow-defs} defines the \mykeyword{bottom arrow}.
Its computations are of type $x \botto y ::= x \tto y_\bot$, where the inhabitants of $y_\bot$ are the error value $\bot$ as well as the inhabitants of $y$.
The type $Bool_\bot$, for example, denotes the members of $Bool \uplus \set{\bot}$.

If we wish to claim that $x~\botto~y$ computations obey the arrow laws, we need a notion of equivalence for lambdas that is coarser than observational equivalence.
\begin{definition}[bottom arrow equivalence]
Two bottom arrow computations $f_1 : x \botto y$ and $f_2 : x \botto y$ are equivalent, or $f_1 \equiv f_2$, when $f_1~a \equiv f_2~a$ for all $a : x$.
\end{definition}

\begin{theorem}
$\arrbot$, $(\pairbot)$, $(\compbot)$, $\ifbot$ and $\lazybot$ define an arrow.
\end{theorem}
\begin{proof}
The bottom arrow is the Maybe monad's Kleisli arrow with $Nothing = \bot$.
The proof of~\eqref{eqn:pure-distributes-over-lazy} (pure computations distribute over $\lazybot$) is trivial.
\end{proof}

\subsection{Deriving the Mapping Arrow}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrmap : (X \tto Y) \tto (X \mapto Y) \\
		&\arrmap \ := \ \liftmap \circ \arrbot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compmap) : (X \mapto Y) \tto (Y \mapto Z) \tto (X \mapto Z) \\
		&(g_1~\compmap~g_2)~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(range~g_1')
			}{g_2' \circ\map g_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairmap) : (X \mapto Y_1) \tto (X \mapto Y_2) \tto (X \mapto \pair{Y_1,Y_2}) \\
		&(g_1~\pairmap~g_2)~A \ := \ \pair{g_1~A,g_2~A}\map
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifmap : (X \mapto Bool) \tto (X \mapto Y) \tto (X \mapto Y) \tto (X \mapto Y) \\
		&\ifmap~g_1~g_2~g_3~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(preimage~g_1'~\set{true}) \\
				g_3' & g_3~(preimage~g_1'~\set{false})
			}{g_2' \uplus\map g_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazymap : (1 \tto (X \mapto Y)) \tto (X \mapto Y) \\
		&\lazymap~g~A \ := \ if~(A = \emptyset)~\emptyset~(g~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A \ := \ \setb{\pair{a,b} \in mapping~f~A}{b \neq \bot}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Mapping arrow definitions.}
\label{fig:mapping-arrow-defs}
\end{figure*}

Theorems about functions in set theory tend to be about mappings, not about lambdas that may raise errors.
As in intermediate step, then, we need an arrow whose computations produce mappings or are mappings themselves.

It is tempting to try to make the mapping arrow's computations mapping-valued; i.e. define it using $X \mapto Y ::= X \pto Y$, with $f_1~\compmap~f_2 := f_2 \circ\map f_1$ and $f_1~\pairmap~f_2 := \pair{f_1,f_2}\map$.
Unfortunately, we could not define $\arrmap : (X \tto Y) \tto (X \pto Y)$: to define a mapping, we need a domain, but lambdas' domains are unobservable.

To parameterize mapping arrow computations on a domain, we define the \mykeyword{mapping arrow} computation type as
\begin{equation}
	X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
\end{equation}
The fact that $\bot$ is absent from $Y$ in $Set~X \tto (X \pto Y)$ will make it easier to exclude diverging inputs.
Its absence and the fact that the type parameters denote sets will make it easier to apply well-known theorems from measure theory, which know nothing of lambda types and propagating error values.

To use Theorem~\ref{thm:homomorphism-implies-correct} to prove that expressions interpreted using $\meaningof{\cdot}\map$ behave correctly, we need to define correctness using a lift from the bottom arrow to the mapping arrow.
It is helpful to have a standalone function $domain_\bot$ that computes the subset of $A$ on which $f$ does not return $\bot$.
We define that first, and then define $\liftmap$ in terms of it:
\begin{align}
	&\begin{aligned}
		&domain_\bot : (X \botto Y) \tto Set~X \tto Set~X \\
		&domain_\bot~f~A \ := \ \setb{a \in A}{f~a \neq \bot}
	\end{aligned} \\
\nonumber \\[-6pt]
	&\begin{aligned}
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A \ := \ mapping~f~(domain_\bot~f~A)
	\end{aligned}
\end{align}
So $\liftmap~f~A$ is like $mapping~f~A$, but without inputs that produce errors---a good notion of correctness.

If $\liftmap$ is to be a homomorphism, mapping arrow computation equivalence needs to be more extensional.

\begin{definition}[mapping arrow equivalence]
Two mapping arrow computations $g_1 : X \mapto Y$ and $g_2 : X \mapto Y$ are equivalent, or $g_1 \equiv g_2$, when $g_1~A \equiv g_2~A$ for all $A \subseteq X$.
\end{definition}

Clearly $\arrmap := \liftmap \circ \arrbot$ meets the first homomorphism law~\eqref{eqn:lift-distributes-over-arr}.
The following subsections derive $(\pairmap)$, $(\compmap)$, $\ifmap$ and $\lazymap$ from bottom arrow combinators, in a way that ensures $\liftmap$ is an arrow homomorphism.
Figure~\ref{fig:mapping-arrow-defs} contains the resulting definitions.

\subsubsection{Case: Pairing}

Starting with the left side of~\eqref{eqn:lift-distributes-over-pair}, we first expand definitions.
For any $f_1 : X \botto Y$, $f_2 : X \botto Z$, and $A \subseteq X$,
\begin{align*}
	&\liftmap~(f_1~\pairbot~f_2)~A
\\
	&\tab \equiv \ \liftmap~(\fun{a}{if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}})~A
\\
	&\tab \equiv \ 
		\lzfclet{
			f & \fun{a}{if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}}
		}{mapping~f~(domain_\bot~f~A)}
\numberthis
\end{align*}
Next, we replace the definition of $A'$ (XXX: fix) with one that does not depend on $f$, and rewrite in terms of $\liftmap~f_1$ and $\liftmap~f_2$:
\begin{align*}
	&\liftmap~(f_1~\pairbot~f_2)~A
\\
	&\tab \equiv \ 
		\lzfclet{
			A_1 & (domain_\bot~f_1~A) \\
			A_2 & (domain_\bot~f_2~A) \\
			A' & A_1 \i A_2
		}{\fun{a \in A'}{\pair{f_1~a,f_2~a}}}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			g_2 & \liftmap~f_2~A \\
			A' & (domain~g_1) \i (domain~g_2)
		}{\fun{a \in A'}{\pair{g_1~a,g_2~a}}}
\\
	&\tab \equiv \ \pair{\liftmap~f_1~A, \liftmap~f_2~A}\map
\numberthis
\end{align*}
Substituting $g_1$ for $\liftmap~f_1$ and $g_2$ for $\liftmap~f_2$ gives a definition for $(\pairmap)$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsubsection{Case: Composition}

The derivation of $(\compmap)$ is similar to that of $(\pairmap)$ but a little more involved.

XXX: add this, maybe cut later

\subsubsection{Case: Conditional}

Starting with the left side of~\eqref{eqn:lift-distributes-over-if}, we expand definitions, and simplify $f$ by restricting it to a domain for which $f_1~a \neq \bot$:
\begin{align*}
	&\liftmap~(\ifbot~f_1~f_2~f_3)~A \\
	&\tab \equiv \ 
		\lzfclet{
			f & \fun{a}{\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ \bot & \bot}}
		}{mapping~f~(domain_\bot~f~A)} \\
	&\tab \equiv \ 
		\lzfclet{
			g_1 & mapping~f~A \\
			A_2 & preimage~g_1~\set{true} \\
			A_3 & preimage~g_1~\set{false} \\
			f & \fun{a}{if~(f_1~a)~(f_2~a)~(f_3~a)}
		}{mapping~f~(domain_\bot~f~(A_2 \uplus A_3))}
\numberthis
\end{align*}
We finish by converting bottom arrow computations to the mapping arrow and rewriting in terms of $(\uplus\map)$:
\begin{align*}
	&\liftmap~(\ifbot~f_1~f_2~f_3)~A \numberthis
\\
	&\tab \equiv \ 
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false}) \\
		A' & (domain~g_2) \uplus (domain~g_3)
	}{\fun{a \in A'}{if~(a \in domain~g_2)~(g_2~a)~(g_3~a)}}
\\
	&\tab \equiv \
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false})
	}{g_2 \uplus\map g_3}
\end{align*}
Substituting $g_1$ for $\liftmap~f_1$, $g_2$ for $\liftmap~f_2$, and $g_3$ for $\liftmap~f_3$ gives a definition for $\ifmap$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsubsection{Case: Laziness}

Starting with the left side of~\eqref{eqn:lift-distributes-over-lazy}, we first expand definitions:
\begin{align*}
	&\liftmap~(\lazybot~f)~A
\\
	&\tab \equiv \
		\lzfclet{
			A' & domain_\bot~(\fun{a}{f~0~a})~A
		}{mapping~(\fun{a}{f~0~a})~A'}
\end{align*}
\lzfclang does not have an $\eta$ rule (i.e. $\fun{\mathit{x}}{\mathit{e}~\mathit{x}} \not\equiv \mathit{e}$ because $\mathit{e}$ may diverge), but we can use weaker facts.
If $A \neq \emptyset$, then $domain_\bot~(\fun{a}{f~0~a})~A \equiv domain_\bot~(f~0)~A$.
Further, it diverges if and only if $mapping~(f~0)~A'$ diverges.
Therefore, if $A \neq \emptyset$, we can replace $\fun{a}{f~0~a}$ with $f~0$.
If $A = \emptyset$, then $\liftmap~(\lazybot~f)~A = \emptyset$ (the empty mapping), so
\begin{align*}
	&\liftmap~(\lazybot~f)~A
\\
	&\tab \equiv \
		if~(A = \emptyset)~\emptyset~(mapping~(f~0)~(domain_\bot~(f~0)~A))
\\
	&\tab \equiv \
		if~(A = \emptyset)~\emptyset~(\liftmap~(f~0)~A)
\end{align*}
Substituting $g~0$ for $\liftmap~(f~0)$ gives a definition for $\lazymap$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsubsection{Correctness}

\begin{theorem}[mapping arrow correctness]
$\liftmap$ is an arrow homomorphism.
\label{thm:mapping-arrow-correctness}
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
For all expressions $\mathit{e}$, $\meaningof{\mathit{e}}\map \equiv \liftmap~\meaningof{\mathit{e}}_\bot$.
\end{corollary}

\subsubsection{Arrow Laws}

Without restrictions, mapping arrow computations can be quite unruly.
For example, the following computation is well-typed, but returns the identity mapping on $Bool$ when applied to an empty domain, and the empty mapping when applied to any other domain:
\begin{equation}
\begin{aligned}
	&nonmonotone : Bool \mapto Bool \\
	&nonmonotone~A \ := \ if~(A = \emptyset)~(mapping~id~Bool)~\emptyset
\end{aligned}
\end{equation}
It would be nice if we could be sure that every $X \mapto Y$ is not only monotone, but acts as if it returned restricted mappings.
The following equivalent property is easier to state, and makes proving the arrow laws simple.

\begin{definition}[mapping arrow law]
Let $g : X \mapto Y$. If there exists an $f : X \botto Y$ such that $g \equiv \liftmap~f$, then $g$ obeys the \mykeyword{mapping arrow law}.
\label{def:mapping-arrow-law}
\end{definition}

XXX: mapping arrow restriction theorem

We assume from here on that the mapping arrow law holds for all $g : X \mapto Y$.
By homomorphism of $\liftmap$, mapping arrow combinators return computations that obey this law.

\begin{theorem}
$\liftmap$ is an arrow epimorphism.
\end{theorem}
\begin{proof}
Follows from Theorem~\ref{thm:mapping-arrow-correctness} and Definition~\ref{def:mapping-arrow-law}.
\end{proof}

\begin{corollary}
$\arrmap$, $(\pairmap)$, $(\compmap)$, $\ifmap$ and $\lazymap$ define an arrow.
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lazy Preimage Mappings}
\label{sec:lazy-preimage-mappings}

\begin{figure*}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \prepto Y ::= \pair{Set~Y, Set~Y \tto Set~X}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&pre : (X \mapto Y) \tto (X \prepto Y) \\
		&pre~g \ := \ \pair{range~g, \fun{B}{preimage~g~B}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&ap\pre : (X \prepto Y) \tto Set~Y \tto Set~X \\
		&ap\pre~\pair{Y',p}~B \ := \ p~(B \i Y') 
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&domain\pre : (X \prepto Y) \tto Set~X \\
		&domain\pre~\pair{Y',p} \ := \ p~Y'
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&range\pre : (X \prepto Y) \tto Set~Y \\
		&range\pre~\pair{Y',p} \ := \ Y'
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\pre : (X \prepto Y_1) \tto (X \prepto Y_2) \tto (X \prepto Y_1 \times Y_2) \\
		&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre \ := \ 
		\lzfclet{
			Y' & Y_1' \times Y_2' \\
			p & \fun{B}{\U\limits_{\pair{b_1,b_2} \in B}(p_1~\set{b_1}) \i (p_2~\set{b_2})} \\
		}{\pair{Y',p}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\pre) : (Y \prepto Z) \tto (X \prepto Y) \tto (X \prepto Z) \\
		&\pair{Z',p_2} \circ\pre h_1 \ := \ \pair{Z', \fun{C}{ap\pre~h_1~(p_2~C)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\pre) : (X \prepto Y) \tto (X \prepto Y) \tto (X \prepto Y) \\
		&\lzfcsplit{
			&h_1 \uplus\pre h_2 \ := \ 
			\lzfclet{
					Y' & (range\pre~h_1) \u (range\pre~h_2) \\
					p & \fun{B}{(ap\pre~h_1~B) \uplus (ap\pre~h_2~B)}
				}{\pair{Y',p}}
		}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Lazy preimage mappings and operations.}
\label{fig:preimage-mapping-defs}
\end{figure*}

On a computer, we do not often have the luxury of testing each function input to see whether it belongs to a preimage set.
Even for finite domains, doing so is often intractable.

If we wish to compute with infinite sets in the language implementation, we will need an abstraction that makes it easy to replace computation on points with computation on sets whose representations allow efficient operations.
Therefore, in the preimage arrow, we will confine computation on points to instances of
\begin{equation}
	X \prepto Y \ ::= \ \pair{Set~Y, Set~Y \tto Set~X}
\end{equation}
Like a mapping, an $X \prepto Y$ has an observable domain---but computing the table of input-output pairs is delayed.
We therefore call these \mykeyword{lazy preimage mappings}.

Converting a mapping to a lazy preimage mapping requires constructing a delayed application of $preimage$:
\begin{equation}
\begin{aligned}
	&pre : (X \pto Y) \tto (X \prepto Y) \\
	&pre~g \ := \ \pair{range~g,\fun{B}{preimage~g~B}}
\end{aligned}
\end{equation}
Applying a preimage mapping to any subset of its codomain: (XXX: unclear prose)
\begin{equation}
\begin{aligned}
	&ap\pre : (X \prepto Y) \tto Set~Y \tto Set~X \\
	&ap\pre~\pair{Y',p}~B \ := \ p~(B \i Y')
\end{aligned}
\end{equation}
The necessary property here is that using $ap\pre$ to compute preimages is the same as computing them from a mapping using $preimage$.

\begin{lemma}
Let $g \in X \pto Y$.
For all $B \subseteq Y$ and $Y'$ such that $range~g \subseteq Y' \subseteq Y$,
$preimage~g~(B \i Y') = preimage~g~B$.
\label{lem:preimage-restricted-range}
\end{lemma}

\begin{theorem}[$ap\pre$ computes preimages]
Let $g \in X \pto Y$. For all $B \subseteq Y$, $ap\pre~(pre~g)~B = preimage~g~B$.
\label{thm:pre-like-preimage}
\end{theorem}
\begin{proof}
Expand definitions and apply Lemma~\ref{lem:preimage-restricted-range} with $Y' = range~g$.
\end{proof}

Figure~\ref{fig:preimage-mapping-defs} defines more operations on preimage mappings, including pairing, composition, and disjoint union operations corresponding to the mapping operations in Figure~\ref{fig:mapping-defs}.
The next three theorems establish that $pre$ is a homomorphism (though not an arrow homomorphism): it distributes over mapping operations to yield preimage mapping operations.
We will use these facts to derive the preimage arrow from the mapping arrow.

First, we need preimage mappings to be equivalent when they compute the same preimages.

\begin{definition}[preimage mapping equivalence]
Two preimage mappings $h_1 : X \prepto Y$ and $h_2 : X \prepto Y$ are equivalent, or $h_1 \equiv h_2$, when $ap\pre~h_1~B \equiv ap\pre~h_2~B$ for all $B \subseteq Y$.
\end{definition}

The following subsections prove distributive laws for preimage mapping pairing, composition, and disjoint union.

XXX: moar text in following subsections

\subsection{Preimage Mapping Pairing}

\begin{lemma}[$preimage$ distributes over $\pair{\cdot,\cdot}\map$ and $(\times)$]
Let $g_1 \in X \pto Y_1$ and $g_2 \in X \pto Y_2$.
For all $B_1 \subseteq Y_1$ and $B_2 \subseteq Y_2$, $preimage~\pair{g_1,g_2}\map~(B_1 \times B_2) = (preimage~g_1~B_1) \i (preimage~g_2~B_2)$.
\label{lem:preimage-under-pairing}
\end{lemma}

\begin{theorem}[$pre$ distributes over $\pair{\cdot,\cdot}\map$]
Let $g_1 \in X \pto Y_1$ and $g_2 \in X \pto Y_2$. Then $pre~\pair{g_1,g_2}\map \equiv \pair{pre~g_1,pre~g_2}\pre$.
\label{thm:preimage-mapping-pairing}
\end{theorem}
\begin{proof}
Let $\pair{Y_1',p_1} := pre~g_1$ and $\pair{Y_2',p_2} := pre~g_2$.
Starting from the right side, for all $B \in Y_1 \times Y_2$,
\begin{align*}
	&ap\pre~\pair{pre~g_1,pre~g_2}\pre~B 
\\
	&\tab\equiv \ 
		\lzfclet{
			Y' & Y_1' \times Y_2' \\
			p & \fun{B}{\U\limits_{\pair{y_1,y_2} \in B}(p_1~\set{y_1}) \i (p_2~\set{y_2})} \\
		}{p~(B \i Y')}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} (p_1~\set{y_1}) \i (p_2~\set{y_2})
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} (preimage~g_1~\set{y_1}) \i (preimage~g_2~\set{y_2})
\\
	&\tab\equiv \U\limits_{y \in B \i (Y_1' \times Y_2')} (preimage~\pair{g_1,g_2}\map~\set{y})
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~(B \i (Y_1' \times Y_2'))
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~B
\\
	&\tab\equiv \ ap\pre~(pre~\pair{g_1,g_2}\map)~B
\end{align*}
\end{proof}

\subsection{Preimage Mapping Composition}

\begin{lemma}[$preimage$ distributes over $(\circ\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in Y \pto Z$.
For all $C \subseteq Z$, $preimage~(g_2 \circ\map g_1)~C = preimage~g_1~(preimage~g_2~C)$.
\label{lem:preimage-under-composition}
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\circ\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in Y \pto Z$.
Then $pre~(g_2 \circ\map g_1) \equiv (pre~g_2) \circ\pre (pre~g_1)$.
\label{thm:preimage-mapping-composition}
\end{theorem}
\begin{proof}
Let $\pair{Z',p_2} := pre~g_2$.
Starting from the right side, for all $C \subseteq Z$,
\begin{align*}
	&ap\pre~((pre~g_2) \circ\pre (pre~g_1))~C
\\
	&\equiv\ 
		\lzfclet{
			h & \fun{C}{ap\pre~(pre~g_1)~(p_2~C)} \\
			}{h~(C \i Z')}
\\
	&\equiv\ ap\pre~(pre~g_1)~(p_2~(C \i Z'))
\\
	&\equiv\ ap\pre~(pre~g_1)~(ap\pre~(pre~g_2)~C)
\\
	&\equiv\ preimage~g_1~(preimage~g_2~C)
\\
	&\equiv\ preimage~(g_2 \circ\map g_1)~C
\\
	&\equiv\ ap\pre~(pre~(g_2 \circ\map g_1))~C
\end{align*}
\end{proof}

\subsection{Preimage Mapping Disjoint Union}

\begin{lemma}[$preimage$ distributes over $(\uplus\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in X \pto Y$ have disjoint domains.
For all $B \subseteq Y$, $preimage~(g_1 \uplus\map g_2)~B = (preimage~g_1~B) \uplus (preimage~g_2~B)$.
\label{lem:preimage-under-piecewise}
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\uplus\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in X \pto Y$ have disjoint domains.
Then $pre~(g_1 \uplus\map g_2) \equiv (pre~g_1) \uplus\pre (pre~g_2)$.
\label{thm:piecewise-preimage-mappings}
\end{theorem}
\begin{proof}
Let $Y_1' := range~g_1$ and $Y_2' := range~g_2$.
Starting from the right side, for all $B \subseteq Y$,
\begin{align*}
	&ap\pre~((pre~g_1) \uplus\pre (pre~g_2))~B
\\
	&\equiv\ 
		\lzfclet{
			\! Y' & Y_1' \u Y_2' \\
			h & \fun{B}{(ap\pre~(pre~g_1)~B) \uplus (ap\pre~(pre~g_2)~B)}
		}{h~(B \i Y')}
\\
	&\equiv\ \lzfcsplit{&(ap\pre~(pre~g_1)~(B \i (Y_1' \u Y_2')))\ \uplus\\ &(ap\pre~(pre~g_2)~(B \i (Y_1' \u Y_2')))}
\\
	&\equiv\ \lzfcsplit{&(preimage~g_1~(B \i (Y_1' \u Y_2')))\ \uplus\\ &(preimage~g_2~(B \i (Y_1' \u Y_2')))}
\\
	&\equiv\ preimage~(g_1 \uplus\map g_2)~(B \i (Y_1' \u Y_2'))
\\
	&\equiv\ preimage~(g_1 \uplus\map g_2)~B
\\
	&\equiv\ ap\pre~(pre~(g_1 \uplus\map g_2))~B
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deriving the Preimage Arrow}
\label{sec:preimage-arrow}

We are ready to define an arrow that runs expressions backward on sets of outputs.
Its computations should produce preimage mappings or be preimage mappings themselves.

As with the mapping arrow and mappings, we cannot have $X \preto Y ::= X \prepto Y$: we run into trouble trying to define $\arrpre$ because a preimage mapping needs an observable range.
To get one, it is easiest to parameterize preimage computations on a $Set~X$; therefore the \mykeyword{preimage arrow} type constructor is
\begin{equation}
	X \preto Y \ ::= \ Set~X \tto (X \prepto Y)
\end{equation}
or $Set~X \tto \pair{Set~Y, Set~Y \tto Set~X}$.
To deconstruct the type, a preimage arrow computation computes a range first, and returns the range and a lambda that computes preimages.

To use Theorem~\ref{thm:homomorphism-implies-correct}, we need to define correctness using a lift from the mapping arrow to the preimage arrow:
\begin{equation}
\begin{aligned}
	&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
	&\liftpre~g~A \ := \ pre~(g~A)
\end{aligned}
\end{equation}
By Theorem~\ref{thm:pre-like-preimage}, for all $g : X \mapto Y$, $A \subseteq X$ and $B \subseteq Y$,
\begin{equation}
	ap\pre~(\liftpre~g~A)~B \equiv preimage~(g~A)~B
\end{equation}
Roughly, lifted mapping arrow computations compute correct preimages, exactly as we should expect them to.

Again, we need a coarser notion of equivalence.

\begin{definition}[Preimage arrow equivalence]
Two preimage arrow computations $h_1 : X \preto Y$ and $h_2 : X \preto Y$ are equivalent, or $h_1 \equiv h_2$, when 
$h_1~A \equiv h_2~A$ for all $A \subseteq X$.
\end{definition}

As with $\arrmap$, defining $\arrpre$ as a composition meets~\eqref{eqn:lift-distributes-over-arr}.
The following subsections derive $(\pairpre)$, $(\comppre)$, $\ifpre$ and $\lazypre$ from mapping arrow combinators, in a way that ensures $\liftpre$ is an arrow homomorphism from the mapping arrow to the preimage arrow. Figure~\ref{fig:preimage-arrow-defs} contains the resulting definitions.

XXX: try paragraph headings instead of subsections

\begin{figure*}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \preto Y ::= Set~X \tto (X \prepto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrpre : (X \tto Y) \tto (X \preto Y) \\
		&\arrpre \ := \ \liftpre \circ \arrmap
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\comppre) : (X \preto Y) \tto (Y \preto Z) \tto (X \preto Z) \\
		&(h_1~\comppre~h_2)~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(range\pre~h_1')
			}{h_2' \circ\pre h_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairpre) : (X \preto Y) \tto (X \preto Z) \tto (X \preto Y \times Z) \\
		&(h_1~\pairpre~h_2)~A \ := \ \pair{h_1~A,h_2~A}\pre
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifpre: (X \preto Bool) \tto (X \preto Y) \tto (X \preto Y) \tto (X \preto Y) \\
		&\ifpre~h_1~h_2~h_3~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(ap\pre~h_1'~\set{true}) \\
				h_3' & h_3~(ap\pre~h_1'~\set{false})
			}{h_2' \uplus\pre h_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazypre : (1 \tto (X \preto Y)) \tto (X \preto Y) \\
		&\lazypre~h~A \ := \ if~(A = \emptyset)~(pre~\emptyset)~(h~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
		&\liftpre~g~A \ := \ pre~(g~A)
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Preimage arrow definitions.}
\label{fig:preimage-arrow-defs}
\end{figure*}

\subsection{Case: Pairing}

Starting with the left side of~\eqref{eqn:lift-distributes-over-pair}, we expand definitions, apply Theorem~\ref{thm:preimage-mapping-pairing}, and rewrite in terms of $\liftpre$:
\begin{align*}
	&ap\pre~(\liftpre~(g_1~\pairmap~g_2)~A)~B
\\
	&\tab \equiv \ ap\pre~(pre~\pair{g_1~A, g_2~A}\map)~B
\\
	&\tab \equiv \ ap\pre~\pair{pre~(g_1~A), pre~(g_2~A)}\pre~B
\\
	&\tab \equiv \ ap\pre~\pair{\liftpre~g_1~A, \liftpre~g_2~A}\pre~B
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $(\pairpre)$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsection{Case: Composition}

Starting with the left side of~\eqref{eqn:lift-distributes-over-comp}, we expand definitions, apply Theorem~\ref{thm:preimage-mapping-composition} and rewrite in terms of $\liftpre$:
\begin{align*}
	&ap\pre~(\liftpre~(g_1~\compmap~g_2)~A)~C
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{ap\pre~(pre~(g_2' \circ\map g_1'))~C}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{ap\pre~((pre~g_1') \circ\pre (pre~g_2'))~C}
\\
	&\tab \equiv \
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(range\pre~h_1)
		}{ap\pre~(h_2 \circ\pre h_1)~C}
\numberthis
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $(\comppre)$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-comp} holds.

\subsection{Case: Conditional}

Starting with the left side of~\eqref{eqn:lift-distributes-over-if}, we expand terms, apply Theorem~\ref{thm:piecewise-preimage-mappings}, rewrite in terms of $\liftpre$, and apply Theorem~\ref{thm:pre-like-preimage} in the definitions of $h_2$ and $h_3$:
\begin{align*}
	&ap\pre~(\liftpre~(\ifmap~g_1~g_2~g_3)~A)~B
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{ap\pre~(pre~(g_2' \uplus\map g_3'))~B}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{ap\pre~((pre~g_2') \uplus\pre (pre~g_3'))~B}
\\
	&\tab \equiv \ 
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(ap\pre~h_1~\set{true}) \\
			h_3 & \liftpre~g_3~(ap\pre~h_1~\set{false})
		}{ap\pre~(h_2 \uplus\pre h_3)~B}
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$, $h_2$ for $\liftpre~g_2$ and $h_3$ for $\liftpre~g_3$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $\ifpre$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsection{Case: Laziness}

Starting with the left side of~\eqref{eqn:lift-distributes-over-lazy}, expand definitions, distribute $pre$ over the branches of $if$, and rewrite in terms of $\liftpre~(g~0)$:
\begin{align*}
	&ap\pre~(\liftpre~(\lazymap~g)~A)~B
\\
	&\tab\equiv \
		\lzfclet{
			g' & if~(A = \emptyset)~\emptyset~(g~0~A)
		}{ap\pre~(pre~g')~B}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(pre~(g~0~A))
		}{ap\pre~h~B}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(\liftpre~(g~0)~A)
		}{ap\pre~h~B}
\end{align*}
Substituting $h~0$ for $\liftpre~(g~0)$ and removing the application of $ap\pre$ from both sides of the equivalence gives a definition for $\lazypre$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsection{Correctness}

\begin{theorem}[preimage arrow correctness]
$\liftpre$ is an arrow homomorphism.
\label{thm:preimage-arrow-correctness}
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
For all expressions $\mathit{e}$, $\meaningof{\mathit{e}}\pre \equiv \liftpre~\meaningof{\mathit{e}}\map$.
\label{cor:preimage-arrow-correctness}
\end{corollary}

\subsection{Arrow Laws}

As with the mapping arrow, preimage arrow computations can be unruly.
We would like to assume that each $h : X \preto Y$ acts as if it always computes preimages under restricted mappings.
The following equivalent property is easier to state, and makes proving the arrow laws simple.

\begin{definition}[preimage arrow law]
Let $h : X \preto Y$. If there exists a $g : X \mapto Y$ such that $h \equiv \liftpre~g$, then $h$ obeys the \mykeyword{preimage arrow law}.
\label{def:preimage-arrow-law}
\end{definition}

We assume from here on that the preimage arrow law holds for all $h : X \preto Y$.
By homomorphism of $\liftpre$, preimage arrow combinators return computations that obey this law.

\begin{theorem}
$\liftpre$ is an arrow epimorphism.
\end{theorem}
\begin{proof}
Follows from Theorem~\ref{thm:preimage-arrow-correctness} and Definition~\ref{def:preimage-arrow-law}.
\end{proof}

\begin{corollary}
$\arrpre$, $(\pairpre)$, $(\comppre)$, $\ifpre$ and $\lazypre$ define an arrow.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preimages Under Partial Functions}

We have defined everything on the top of our roadmap:
\youarehere{eqn:roadmap-diagram3}
and proved that $\liftmap$ and $\liftpre$ are homomorphisms.
Now we move down from all three top arrows simultaneously, and prove every morphism in~\eqref{eqn:roadmap-diagram3} is an arrow homomorphism.

\subsection{Motivation}

Probabilistic functions that may diverge, but converge with probability 1, are common.
They come up not only when practitioners want to build data with random size or structure, but in simpler circumstances as well.

Suppose $random$ retrieves a number $r~j \in [0,1]$ at index $j$ in an implicit random source $r$.
The following function, which defines the well-known \keyword{geometric distribution} with parameter $p$, counts the number of times $random < p$ is $false$:
\begin{equation}
	geometric~p \ := \ if~(random < p)~0~(1 + geometric~p)
\label{eqn:geometric-def}
\end{equation}
For any $p > 0$, $geometric~p$ may diverge, but the probability of always taking the false branch is $(1-p) \times (1-p) \times (1-p) \times \cdots = 0$.
Therefore, for $p > 0$, $geometric~p$ converges with probability $1$.

Suppose we interpret~\eqref{eqn:geometric-def} as $h : R \preto \Nat$, a preimage arrow computation from random sources in $R$ to natural numbers, and that we have a probability measure $P \in \powerset~R \pto [0,1]$.
We could compute the probability of any output set $N \subseteq \Nat$ using $P~(h~R'~N)$, where $R' \subseteq R$ and $P~R' = 1$. We have three hurdles to overcome:
\begin{enumerate}
	\item Ensuring $h~R'$ converges.
	\item Ensuring each $r \in R$ contains enough random numbers.
	\item Determining how $random$ indexes numbers in $r$.
\end{enumerate}
Ensuring $h~R'$ converges is the most difficult, but doing the other two will provide structure that makes it much easier.

\subsection{Threading and Indexing}

XXX: `Prior to defn 7.1, you need a transition that communicates ``I've just given you the intuition, now let's do the work'''

We clearly need a new arrow that threads a random source through its computations.
To ensure it contains enough random numbers, the source should be infinite.

In a pure $\lambda$-calculus, random sources are typically infinite streams, threaded monadically: each computation receives and produces a random source.
A new combinator is defined that removes the head of the random source and passes the tail along.
This is likely preferred because pseudorandom number generators are almost universally monadic.

A little-used alternative is for the random source to be a tree, threaded applicatively:
each computation receives, but does not produce, a random source.
Multi-argument combinators split the tree and pass subtrees to subcomputations.

With either alternative, for arrows defined using pairing, the resulting definitions are large, conceptually difficult, and hard to manipulate.
Fortunately, assigning each subcomputation a unique index into a tree-shaped random source, and passing the random source unchanged, is relatively easy.

We need a way to assign unique indexes to expressions.

\begin{definition}[binary indexing scheme]
Let $J$ be an index set, $j_0 \in J$ a distinguished element, and $left : J \tto J$ and $right : J \tto J$ be total, injective functions. If for all $j \in J$, $j = next~j_0$ for some finite composition $next$ of $left$ and $right$, then $J$, $j_0$, $left$ and $right$ define a \mykeyword{binary indexing scheme}.
\end{definition}

For example, let $J$ be the set of lists of $\set{0,1}$, $j_0 := \pair{}$, and $left~j := \pair{0,j}$ and $right~j := \pair{1,j}$.

Alternatively, let $J$ be the set of dyadic rationals in $(0,1)$ (i.e. those with power-of-two denominators), $j_0 := \tfrac{1}{2}$ and
\begin{equation}
\begin{aligned}
	left~(p/q) &\ := \ (p-\tfrac{1}{2})/q
\\
	right~(p/q) &\ := \ (p+\tfrac{1}{2})/q
\end{aligned}
\end{equation}
With this alternative, left-to-right evaluation order can be made to correspond with the natural order $(<)$ over $J$.

In any case, the index set $J$ is always countable, and can be thought of as a set of indexes into an infinite binary tree.
Values of type $J \to A$ encode an infinite binary tree of $A$ values as an infinite vector (i.e. total mapping).

\subsection{Applicative, Associative Store Transformer}

We thread a random store through bottom, mapping, and preimage arrow computations by defining an \keyword{arrow transformer}: a type constructor that receives and produces an arrow type, and combinators for arrows of the produced type. (XXX: ask Dan B for a cite or Brent Yorgey (tell him I'm Jay's student, who is friends with his advisor))

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		x \arrow\genc y \ ::= \ AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowarr\genc : (x \tto y) \tto (x \arrow\genc y) \\
		&\arrowarr\genc \ := \ \arrowtrans\genc \circ \arrowarr\gen
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowcomp\genc) : (x \arrow\genc y) \tto (y \arrow\genc z) \tto (x \arrow\genc z) \\
		&(k_1~\arrowcomp\genc~k_2)~j \ := \\
			&\tab(\arrowarr\gen~fst~\arrowpair\gen~k_1~(left~j))~\arrowcomp\gen~k_2~(right~j)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowpair\genc) : (x \arrow\genc y_1) \tto (x \arrow\genc y_2) \tto (x \arrow\genc \pair{y_1,y_2}) \\
		&(k_1~\arrowpair\genc~k_2)~j \ := \ k_1~(left~j)~\arrowpair\gen~k_2~(right~j)
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\arrowif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowif\genc~k_1~k_2~k_3~j \ := \
			\lzfcsplit{\arrowif\gen~&(k_1~(left~j)) \\ &(k_2~(left~(right~j))) \\ &(k_3~(right~(right~j)))}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowlazy\genc : (1 \tto (x \arrow\genc y)) \tto (x \arrow\genc y) \\
		&\arrowlazy\genc~k~j \ := \ \arrowlazy\gen~\fun{0}{k~0~j}
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowtrans\genc : (x \arrow\gen y) \tto (x \arrow\genc y) \\
		&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{$AStore$ (associative store) arrow transformer definitions.}
\label{fig:astore-arrow-defs}
\end{figure*}

The applicative store arrow transformer's type constructor takes a store type $s$ and an arrow type $x \arrow\gen y$:
\begin{equation}
	AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
\end{equation}
Reading the type, we see that computations receive an index $j \in J$ and produce a computation that receives a store as well as an $x$.
Lifting extracts the $x$ from the input pair and sends it on to the original computation:
\begin{equation}
\begin{aligned}
	&\arrowtrans\genc : (x \arrow\gen y) \tto AStore~s~(x \arrow\gen y) \\
	&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
\end{aligned}
\end{equation}
Because $f$ never accesses the store, $j$ is ignored.

Figure~\ref{fig:astore-arrow-defs} defines the remaining combinators.
Each subcomputation receives $left~j$, $right~j$, or some other unique binary index.
We thus think of programs interpreted as $AStore$ arrows as being completely unrolled into an infinite binary tree, with each expression labeled with its tree index.

\subsection{Partial, Probabilistic Programs}
\label{sec:probabilistic-programs}

We interpret partial and probabilistic programs using combinators that read a store at an expression index.

\paragraph{Probabilitic Programs} To interpret probabilitic programs, we use a random tree as the store.

\begin{definition}[random source]
Let $R := J \to [0,1]$.
A \keyword{random source} is any infinite binary tree $r \in R$.
\end{definition}

Let $x \arrow\genc y ::= AStore~R~(x \arrow\gen y)$.
This combinator returns the number at its tree index in the random source:
\begin{equation}
\begin{aligned}
	&random\genc : x \arrow\genc [0,1] \\
	&random\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~\pi~j)
\end{aligned}
\end{equation}
We extend the let-calculus semantic function with
\begin{equation}
	\meaningof{random}\genc :\equiv random\genc
\end{equation}
for arrows $a^*$ for which $random\genc$ is defined.

\paragraph{Partial Programs}

One utimately implementable way to avoid divergence is to use the store to dictate which branch of each conditional, if any, is allowed to be taken.

\begin{definition}[branch trace]
A \mykeyword{branch trace} is any $t \in J \to Bool_\bot$ such that $t~j = true$ or $t~j = false$ for no more than finitely many $j \in J$.
\end{definition}

Let $T \subset J \to Bool_\bot$ be the set of all branch traces, and $x \arrow\genc y ::= AStore~T~(x \arrow\gen y)$.
The following combinator returns $t~j$ using its own index $j$:
\begin{equation}
\begin{aligned}
	&branch\genc : x \arrow\genc Bool \\
	&branch\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~\pi~j)
\end{aligned}
\end{equation}
Using $branch\genc$, we define an if-then-else combinator that ensures its test expression agrees with the branch trace:
\begin{align}
	&\begin{aligned}
		&agrees : \pair{Bool,Bool} \tto Bool_\bot \\
		&agrees~\pair{b_1,b_2} \ := \ if~(b_1 = b_2)~b_1~\bot
	\end{aligned} \\
\nonumber \\[-6pt]
	&\begin{aligned}
		&\arrowconvif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowconvif\genc~k_1~k_2~k_3~j \ := \\
		&\tab \arrowif\gen~\lzfcsplit{
				&((k_1~(left~j)~\arrowpair\gen~branch\genc~j)~\arrowcomp\gen~\arrowarr\gen~agrees) \\
				&(k_2~(left~(right~j))) \\
				&(k_3~(right~(right~j)))
			}
	\end{aligned}
	%&\begin{aligned}
	%	&\arrowconvif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
	%	&\arrowconvif\genc~k_1~k_2~k_3 \ := \\ 
	%		&\tab\arrowconvif\genc~((k_1~\arrowpair\genc~branch\genc)~\arrowcomp\genc~\arrowarr\genc~agrees)~k_2~k_3
	%\end{aligned}
\label{eqn:ifppre-def}
\end{align}
If the branch trace agrees with the test expression, it computes a branch; otherwise, it returns an error.

We assume every expression is well-defined (Definition~\ref{def:well-defined-expression}), so every expression must have its recurrences guarded by $if$.
Thus, to ensure running their interpretations always converges, we should only need to replace $\arrowif\genc$ with $\arrowconvif\genc$.
We define a new semantic function $\meaningofconv{\cdot}\genc$ by
\begin{equation}
\begin{aligned}
	\meaningofconv{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\genc &\ :\equiv\
		\lzfcsplit{\arrowconvif\genc~
			&\meaningofconv{\mathit{e_c}}\genc \\
			&(\arrowlazy\gen~\fun{0}{\meaningofconv{\mathit{e_t}}\genc}) \\
			&(\arrowlazy\gen~\fun{0}{\meaningofconv{\mathit{e_f}}\genc})}
\end{aligned}
\end{equation}
with the remaining rules similar to those of $\meaningof{\cdot}\genc$.

\paragraph{Partial, Probabilistic Programs}

Let $S ::= R \times T$ and $x \arrow\genc y ::= AStore~S~(x \arrow\gen y)$, and update the $random\genc$ and $branch\genc$ combinators to reflect that the store is now a pair:
\begin{align}
	&\begin{aligned}
		&random\genc : x \arrow\genc [0,1] \\
		&random\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~fst~\arrowcomp~\pi~j)
	\end{aligned} \\
\nonumber\\[-6pt]
	&\begin{aligned}
		&branch\genc : x \arrow\genc Bool \\
		&branch\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~snd~\arrowcomp~\pi~j)
	\end{aligned}
\end{align}
The definitions of $\arrowconvif\genc$ and $\meaningofconv{\cdot}\genc$ remain the same.

\subsection{Correctness}

We have two arrow lifts to prove homomorphic: one from pure computations to effectful (i.e. from those that do not access the store to those that do), and one from effectful computations to effectful.
For both, we need $AStore$ arrow equivalence to be more extensional.

\begin{definition}[$AStore$ arrow equivalence]
Two $AStore$ arrow computations $k_1$ and $k_2$ are equivalent, or $k_1 \equiv k_2$, when $k_1~j \equiv k_2~j$ for all $j \in J$.
\end{definition}

\paragraph{Pure Expressions}
Proving $\arrowtrans\genc$ is a homomorphism proves $\meaningof{\cdot}\genc$ correctly interprets pure expressions.
Because $AStore$ accepts any arrow type $x \arrow\gen y$, we can do so using only general properties.
From here on, we assume every $AStore$ arrow's base type's combinators obey the arrow laws listed in Section~\ref{sec:arrow-definitions}.

\begin{theorem}[pure $AStore$ arrow correctness]
$\arrowtrans\genc$ is an arrow homomorphism.
\end{theorem}
\begin{proof}
Defining $\arrowarr\genc$ as a composition clearly meets the first homomorphism law~\eqref{eqn:lift-distributes-over-arr}.
For homomorphism laws~(\ref{eqn:lift-distributes-over-comp}--\ref{eqn:lift-distributes-over-if}), start from the right side, expand definitions, and use arrow laws~(\ref{eqn:pair-extraction}--\ref{eqn:pure-distributes-over-if}) to factor out $\arrowarr\gen~snd$.

For~\eqref{eqn:lift-distributes-over-lazy}, additionally $\beta$-expand within the outer thunk, then use the lazy distributive law~\eqref{eqn:pure-distributes-over-lazy} to extract $\arrowarr\gen~snd$.
\end{proof}

\begin{corollary}[pure semantic correctness]
For all pure expressions $\mathit{e}$, $\meaningof{\mathit{e}}\genc \equiv \arrowtrans\genc~\meaningof{\mathit{e}}\gen$ and $\meaningofconv{\mathit{e}}\genc \equiv \arrowtrans\genc~\meaningofconv{\mathit{e}}\gen$.
\label{cor:pure-astore-semantic-correctness}
\end{corollary}

\paragraph{Effectful Expressions} To prove all interpretations of effectful expressions correct, we need a lift between $AStore$ arrows.
Let $x \arrow\genc y ::= AStore~s~(x \arrow\gen y)$ and $x \arrow\gend y ::= AStore~s~(x \arrow\gend y)$.
Define
\begin{equation}
\begin{aligned}
	&\arrowlift\gend : (x \arrow\genc y) \tto (x \arrow\gend y) \\
	&\arrowlift\gend~f~j \ := \ \arrowlift\genb~(f~j)
\end{aligned}
\end{equation}
where $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$.

The relationships are more clearly expressed by
\begin{equation}
\begin{CD}
	x \arrow\gen y @>{\arrowlift\genb}>> x \arrow\genb y \\
	@V{\arrowtrans\genc}VV @VV{\arrowtrans\gend}V \\
	x \arrow\genc y @>>{\arrowlift\gend}> x \arrow\gend y
\end{CD}
\label{eqn:lift-diagram}
\end{equation}
At minimum, we should expect to produce equivalent $x \arrow\gend y$ computations from $x \arrow\gen y$ computations whether a $\arrowlift$ or an $\arrowtrans$ is done first.

\begin{theorem}[natural transformation]
If $\arrowlift\genb$ is an arrow homomorphism, then~\eqref{eqn:lift-diagram} commutes.
\end{theorem}
\begin{proof}
Expand definitions and apply homomorphism laws~\eqref{eqn:lift-distributes-over-comp} and~\eqref{eqn:lift-distributes-over-arr} for $\arrowlift\genb$:
\begin{align*}
	\arrowlift\gend~(\arrowtrans\genc~f)
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd~\arrowcomp\gen~f)}
\\
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd)~\arrowcomp\genb~\arrowlift\genb~f}
\\
	&\ \equiv\ \fun{j}{\arrowarr\genb~snd~\arrowcomp\genb~\arrowlift\genb~f}
\\
	&\ \equiv\ \arrowtrans\gend~(\arrowlift\genb~f)
\end{align*}
\end{proof}

\begin{theorem}[effectful $AStore$ arrow correctness]
If $\arrowlift\genb$ is an arrow homomorphism, then $\arrowlift\gend$ is an arrow homomorphism.
\end{theorem}
\begin{proof}
XXX: do this
\end{proof}

\begin{corollary}[effectful semantic correctness]
If $\arrowlift\genb$ is an arrow homomorphism, then for all expressions $\mathit{e}$, $\meaningof{\mathit{e}}\gend \equiv \arrowlift\gend~\meaningof{\mathit{e}}\genc$ and $\meaningofconv{\mathit{e}}\gend \equiv \arrowlift\gend~\meaningofconv{\mathit{e}}\genc$.
\label{cor:astore-semantic-correctness}
\end{corollary}

From here on, let $x \pbotto y ::= AStore~(R \times T)~(x \botto y)$, called the \mykeyword{bottom* arrow};
similarly for $X \pmapto Y$ (the \mykeyword{mapping* arrow}) and $X \ppreto Y$ (the \mykeyword{preimage* arrow}).

\begin{corollary}[mapping* and preimage* arrow correctness]
The following diagram commutes:
\begin{equation}
\begin{CD}
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y
\end{CD}
\end{equation}
Further, $\liftpmap$ and $\liftppre$ are arrow homomorphisms.
\end{corollary}

\begin{corollary}[effectful semantic correctness]
For all expressions $\mathit{e}$,
\begin{equation}
\begin{aligned}
	\meaningof{\mathit{e}}\ppre &\ \equiv \ \liftppre~(\liftpmap~\meaningof{\mathit{e}}_\pbot)
\\
	\meaningofconv{\mathit{e}}\ppre &\ \equiv \ \liftppre~(\liftpmap~\meaningofconv{\mathit{e}}_\pbot)
\end{aligned}
\end{equation}
\end{corollary}

\subsection{Convergence}

To relate $\meaningofconv{\mathit{e}}\genc$ computations to $\meaningof{\mathit{e}}\genc$ computations, we need to find the largest domain on which they should agree.

\begin{definition}[maximal domain]
A computation's \mykeyword{maximal domain} is the largest $A^*$ for which
\begin{itemize}
	\item For $f : X \botto Y$, $domain_\bot~f~A^* = A^*$.
	\item For $g : X \mapto Y$, $domain~(g~A^*) = A^*$.
	\item For $h : X \preto Y$, $domain\pre~(h~A^*) = A^*$.
\end{itemize}
The maximal domain of $k : X \arrow\genc Y$ is that of $k~j_0$.
\label{def:maximal-domain}
\end{definition}

Because the above statements imply convergence, $A^*$ is a subset of the largest domain for which the computations converge.
It is not too hard to show (but is a bit tedious) that lifting computations preserves the maximal domain; e.g. the maximal domain of $\liftmap~f$ is the same as $f$'s, and the maximal domain of $\liftppre~g$ is the same as $g$'s.

To ensure maximal domains exist, we need the domain operations above to have certain properties.

\begin{theorem}[domain closure operators]
If $f : X \botto Y$, $g : X \mapto Y$ and $h : X \preto Y$, then $domain_\bot~f$, $domain \circ g$, and $domain\pre \circ h$ are monotone, nonincreasing, and idempotent in the subdomains on which they converge.
\label{thm:domain-closure-operators}
\end{theorem}
\begin{proof}
These properties follow from the same properties of selection, restriction, and of preimages of images.
\end{proof}

For any input for which $\meaningof{\mathit{e}}_\pbot$ converges, there should be a branch trace for which $\meaningofconv{\mathit{e}}_\pbot$ returns the correct output; it should otherwise return $\bot$.

\begin{theorem}
Let $f := \meaningof{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f' := \meaningofconv{\mathit{e}}_\pbot$.
For all $\pair{\pair{r,t},a} \in A^*$, there exists a $T' \subseteq T$ such that
\begin{itemize}
	\item If $t' \in T'$ then $f'~j_0~\pair{\pair{r,t'},a} = f~j_0~\pair{\pair{r,t},a}$.
	\item If $t' \in T \w T'$ then $f'~j_0~\pair{\pair{r,t'},a} = \bot$.
\end{itemize}
\end{theorem}
\begin{proof}
Define $T'$ as the set of all $t' \in J \to Bool_\bot$ such that $t'~j = z$ if the subcomputation with index $j$ is an $if$ whose test returns $z$.
Because $f~j_0~\pair{\pair{r,t},a}$ converges, $t'~j \neq \bot$ for at most finitely many $j$, so each $t' \in T$.

Let $t' \in T'$.
Because the test of every $if$ subcomputation at index $j$ agrees with $t'~j$ and $f$ ignores branch traces, $f'~j_0~\pair{\pair{r,t'},a} = f~j_0~\pair{\pair{r,t},a}$.

Let $t' \in T \w T'$.
There exists an $if$ subexpression with a test that does not agree with $t'$; therefore $f'~j_0~\pair{\pair{r,t'},a} = \bot$.
\end{proof}

For any input for which $\meaningof{\mathit{e}}_\pbot$ diverges or returns $\bot$, $\meaningofconv{\mathit{e}}_\pbot$ should return $\bot$.
Proving this is a little easier if we first identify subsets of $J$ that correspond with finite prefixes of an infinite binary tree.

\begin{definition}[index prefix]
$J' \subset J$ is an \mykeyword{index prefix} of $J$ if $J' = \set{j_0}$ or at least one of the following is true:
\begin{itemize}
	\item For some finite prefix $J''$ and $j \in J''$, $J' = J'' \uplus \set{left~j}$.
	\item For some finite prefix $J''$ and $j \in J''$, $J' = J'' \uplus \set{right~j}$.
\end{itemize}
\end{definition}

The corresponding \mykeyword{index suffix} is $J \w J'$, which is closed under $left$ and $right$.

For a given $t \in T$, an index prefix $J'$ serves as a convenient bounding set for the finitely many indexes $j$ for which $t~j \neq \bot$.
Applying $left$ and/or $right$ repeatedly to any $j \in J'$ eventually yields a $j' \in J \w J'$, for which $t~j' = \bot$.

\begin{theorem}
Let $f := \meaningof{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f' := \meaningofconv{\mathit{e}}_\pbot$.
For all $a \in ((R \times T) \times X) \w A^*$, $f'~j_0~a = \bot$.
\end{theorem}
\begin{proof}
Let $t := snd~(fst~a)$ be the branch trace element of $a$.

Suppose $f~j_0~a$ converges.
If an $if$ subcomputation's test does not agree with $t$, then $f'~j_0~a = \bot$.
If every $if$'s test agrees, $f'~j_0~a = f~j_0~a = \bot$.

Suppose $f~j_0~a$ diverges.
The set of all indexes $j$ for which $t~j \neq \bot$, because it is finite, is contained within an index prefix $J'$.
By hypothesis, there is an $if$ subcomputation at some index $j'$ such that $j' \in J \w J'$.
Because $t~j' = \bot$, $f'~j_0~a = \bot$.
\end{proof}

\begin{corollary}
For all expressions $\mathit{e}$, the maximal domain of $\meaningofconv{\mathit{e}}_\pbot$ is a subset of that of $\meaningof{\mathit{e}}_\pbot$.
\end{corollary}

\begin{corollary}
Let $f' := \meaningofconv{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f := \meaningof{\mathit{e}}_\pbot$.
For all $a \in A^*$, $f'~j_0~a = f~j_0~a$.
\end{corollary}


\begin{corollary}[correct convergence everywhere]
Suppose $\meaningofconv{\mathit{e}}_\pbot : X \pbotto Y$ has maximal domain $A^*$.
Let $X' := (R \times T) \times X$.
For all $a \in X'$, $A \subseteq X'$ and $B \subseteq Y$,
\begin{equation}
\begin{aligned}
	&\meaningofconv{\mathit{e}}_\pbot &&\!\!\!\!j_0~a &&\!\!\!\!= \ if~(a \in A^*)~(\meaningof{\mathit{e}}_\pbot~j_0~a)~\bot \\
	&\meaningofconv{\mathit{e}}\pmap &&\!\!\!\!j_0~A &&\!\!\!\!= \ \meaningof{\mathit{e}}\pmap~j_0~(A \i A^*) \\
	ap\pre~(\!&\meaningofconv{\mathit{e}}\ppre &&\!\!\!\!j_0~A)~B &&\!\!\!\!= \ ap\pre~(\meaningof{\mathit{e}}\ppre~j_0~(A \i A^*))~B
\end{aligned}
\end{equation}
\label{cor:correct-convergence}
\end{corollary}

In other words, preimages computed using $\meaningofconv{\cdot}\ppre$ always converge, never include inputs that give rise to errors or divergence, and are correct.

\begin{comment}
\begin{corollary}[computed maximal domain]
For any well-defined program $\mathit{e}$, the maximal domain of $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$ can be computed as
\begin{equation}
	A^* \ = \ domain_\bot~(\meaningofconv{\mathit{e}}_\pbot~j_0)~((R \times T) \times X)
\end{equation}
\end{corollary}

\begin{corollary}[semantic correctness (final)]
Let $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$ converge.
For all $A \subseteq (R \times T) \times X$ and $B \subseteq Y$, $ap\pre~(\meaningofconv{\mathit{e}}\ppre~j_0~A)~B = preimage~(\meaningof{\mathit{e}}\pmap~j_0~(A \i A^*))~B$, where $A^*$ is the maximal domain.
\end{corollary}

\begin{align*}
	ap\pre~(\meaningofconv{\mathit{e}}\ppre~j_0~A)~B
	&\ =\
		ap\pre~(\meaningofconv{\mathit{e}}\ppre~j_0~(A \i A^*))~B
\\
	&\ =\
		ap\pre~(\liftppre~\meaningofconv{\mathit{e}}\pmap~j_0~A)~B
\\
	&\ =\
		ap\pre~(\liftpre~(\meaningofconv{\mathit{e}}\pmap~j_0)~A)~B
\\
	&\ =\
		preimage~(\meaningofconv{\mathit{e}}\pmap~j_0~A)~B
\\
	&\ =\
		preimage~(\meaningof{\mathit{e}}\pmap~j_0~(A \i A^*))~B
\end{align*}

\subsection{Older Theorems}

To compare preimages computed by arrow instances produced by $\meaningof{\cdot}\ppre$ and $\meaningofconv{\cdot}\ppre$, we need a set of inputs on which they should obviously always agree.
First, we need to identify the largest domains on which non-$AStore$ arrows converge.

\begin{definition}[halting domain]
A computation's \keyword{halting domain} is the largest $A\conv \subseteq X$ for which
\begin{itemize}
	\item For $f : X \botto Y$, $image~f~A\conv$ converges.
	\item For $g : X \mapto Y$, $g~A\conv$ converges.
	\item For $h : X \preto Y$, $h~A\conv$ converges.
\end{itemize}
\end{definition}

The next two theorems say that lifting a computation preserves its halting domain, as we should expect.

\begin{theorem}
Let $f : X \botto Y$ with $A\conv$ its halting domain.
The halting domain of $\liftmap~f$ is $A\conv$.
\label{thm:liftmap-halting-set}
\end{theorem}
\begin{proof}
Case ``$\subseteq$''.
If $image~f~A\conv$ converges, $domain_\bot~f~A\conv$ converges, and thus $mapping~f~(domain_\bot~f~A\conv)$ converges.

Case ``$\supseteq$''.
Suppose $a \notin A\conv$.
For any $A$ with $a \in A$, $domain_\bot~f~A$ diverges.
\end{proof}

\begin{theorem}
Let $g : X \mapto Y$ with $A\conv$ its halting domain.
The halting domain of $\liftpre~g$ is $A\conv$.
\label{thm:liftpre-halting-set}
\end{theorem}
\begin{proof}
$\liftpre~g~A \equiv pre~(g~A)$, and $pre$ always converges.
\end{proof}

A bottom arrow application $f~a$ can be undefined in another way besides diverging: $f~a = \bot$.
Similarly, for $g : X \mapto Y$, $domain~(g~A) \subset A$.

\begin{definition}[maximal domain]
A computation's \mykeyword{maximal domain} is the largest $A^* \subseteq X$ for which
\begin{itemize}
	\item For $f : X \botto Y$, $domain_\bot~f~A^* = A^*$.
	\item For $g : X \mapto Y$, $domain~(g~A^*) = A^*$.
	\item For $h : X \preto Y$, $domain\pre~(h~A^*) = A^*$.
\end{itemize}
Because these statements imply convergence, $A^* \subseteq A\conv$.
\end{definition}

\begin{theorem}[domain properties]
Let $f : X \botto Y$, $g : X \mapto Y$ and $h : \mapto Y$.
Then $domain_\bot~f$, $domain \circ g$, and $domain\pre \circ h$ are monotone, nonincreasing, and idempotent.
\label{thm:domain-properties}
\end{theorem}
\begin{proof}
These properties follow from the same properties of selection, $restrict$, and of preimages of images.
\end{proof}

\begin{corollary}[computed maximal domain]
A computation's maximal domain is
\begin{itemize}
	\item For $f : X \botto Y$: $A^* = domain_\bot~f~A\conv$.
	\item For $g : X \mapto Y$: $A^* = domain~(g~A\conv)$.
	\item For $h : X \preto Y$: $A^* = domain\pre~(h~A\conv)$.
\end{itemize}
\label{thm:computed-domain-of-definition}
\end{corollary}

\begin{theorem}
Let $f : X \botto Y$ with $A^*$ its maximal domain.
The maximal domain of $\liftmap~f$ is $A^*$.
\end{theorem}
\begin{proof}
By Theorem~\ref{thm:liftmap-halting-set}, its halting domain $A\conv$ is the same as that of $f$.
By Corollary~\ref{thm:computed-domain-of-definition}, its maximal domain is $domain~(\liftmap~f~A\conv)$; expand definitions and reduce.
\end{proof}

\begin{theorem}
Let $g : X \mapto Y$ with $A^*$ its maximal domain.
The maximal domain of $\liftpre~g$ is $A^*$.
\end{theorem}
\begin{proof}
By Theorem~\ref{thm:liftpre-halting-set}, its halting domain $A\conv$ is the same as that of $g$.
By Corollary~\ref{thm:computed-domain-of-definition}, its maximal domain is $domain\pre~(\liftpre~g~A\conv) = preimage~(g~A\conv)~Y$; expand definitions and reduce.
\end{proof}

\begin{corollary}[computed maximal domain]
Let $\meaningof{\mathit{e}}_\pbot : X \pmapto Y$ converge.
Its maximal domain can be computed as
$A^* = domain_\bot~(\meaningofconv{\mathit{e}}_\pbot~j_0)~((R \times T) \times X)$.
\end{corollary}

\begin{corollary}[semantic correctness (final)]
Let $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$ converge, with maximal domain $A^*$.
For all $A \subseteq (R \times T) \times X$ and $B \subseteq Y$, $ap\pre~(\meaningofconv{\mathit{e}}\ppre~j_0~A)~B = preimage~(\meaningof{\mathit{e}}\pmap~j_0~(A \i A^*))~B$.
\end{corollary}

\begin{align*}
	ap\pre~(\meaningofconv{\mathit{e}}\ppre~j_0~A)~B
	&\ =\
		ap\pre~(\liftppre~\meaningofconv{\mathit{e}}\pmap~j_0~A)~B
\\
	&\ =\
		ap\pre~(\liftpre~(\meaningofconv{\mathit{e}}\pmap~j_0)~A)~B
\\
	&\ =\
		preimage~(\meaningofconv{\mathit{e}}\pmap~j_0~A)~B
\\
	&\ =\
		preimage~(\meaningof{\mathit{e}}\pmap~j_0~(A \i A^*))~B
\end{align*}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probabilities}
\label{sec:measurability}

We have not assigned probabilities to any output sets yet.

Typically, for $g \in X \pto Y$, the probability of $B \subseteq Y$ is
\begin{equation}
	P~(preimage~g~B)
\end{equation}
where $P \in \powerset~X \pto [0,1]$.

However, a mapping* computation's domain is $(R \times T) \times X$, not $X$.
We assume each $r \in R$ is randomly chosen, but not each $t \in T$ nor each $x \in X$; therefore, neither $T$ nor $X$ should affect the probabilities of output sets.
We clearly must measure \emph{projections} of preimage sets, or
\begin{equation}
	P~(image~(fst~\arrowcomp~fst)~A)
\end{equation}
for preimage sets $A \subseteq (R \times T) \times X$. 

$P$ is partial, so not every preimage set has a sensible measure.
Sets that do are called \emph{measurable}.
Computing preimages and projecting onto $R$ must preserve measurability.

\subsection{Measurability}

We assume readers are familiar with topology.
Readers unfamiliar with topology or measure theory may wish to skip to Section~\ref{sec:approximating-semantics}.

Many topological concepts have analogues in measure theory; e.g. the analogue of a topology is a $\sigma$-algebra.

\begin{definition}[$\sigma$-algebra, measurable set]
A collection of sets $\A \subseteq \powerset~X$ is called a \keyword{$\sigma$-algebra} on $X$ if it contains $X$ and is closed under complements and countable unions.
The sets in $\A$ are called \keyword{measurable sets}.
\end{definition}

$X \w X = \emptyset$, so $\emptyset \in \A$.
Additionally, it follows from De Morgan's law that $\A$ is closed under countable intersections.

The analogue of continuity is measurability.

\begin{definition}[measurable mapping]
Let $\A$ and $\B$ be $\sigma$-algebras respectively on $X$ and $Y$.
A mapping $g : X \pto Y$ is $\A!\B$-\keyword{measurable} if for all $B \in \B$, $preimage~g~B \in \A$.
\end{definition}

Measurability is usually a weaker condition than continuity.
For example, with respect to the $\sigma$-algebra generated from $\Re$'s standard topology, measurable $\Re \pto \Re$ functions may have countably many discontinuities.
Likewise, real equality and inequality functions are measurable.

Product spaces are defined the same way as in topology.

\begin{definition}[finite product $\sigma$-algebra]
Let $\A_1$ and $\A_2$ be $\sigma$-algebras on $X_1$ and $X_2$, and $X := \pair{X_1,X_2}$.
The \keyword{product $\sigma$-algebra} $\A_1 \otimes \A_2$ is the smallest $\sigma$-algebra for which $mapping~fst~X$ and $mapping~snd~X$ are measurable.
\label{def:finite-product-sigma-algebra}
\end{definition}

\begin{definition}[arbitrary product $\sigma$-algebra]
Let $\A$ be a $\sigma$-algebra on $X$.
The \keyword{product $\sigma$-algebra} $\A^{\otimes J}$ is the smallest $\sigma$-algebra for which, for all $j \in J$, $mapping~(\pi~j)~(J \to X)$ is measurable.
\label{def:arbitrary-product-sigma-algebra}
\end{definition}

\subsection{Measurable Pure Computations}

It is easier to prove measurability of pure computations than to prove measurability of partial, probabilistic ones.
Further, we can use the resulting theorems to prove that the interpretations of all partial, probabilistic expressions are measurable.

We first need to define what it means for a \emph{computation} to be measurable.

\begin{definition}[measurable mapping arrow computation]
Let $\A$ and $\B$ be $\sigma$-algebras on $X$ and $Y$.
A computation $g : X \mapto Y$ is $\A!\B$-\keyword{measurable} if $g~A^*$ is an $\A!\B$-measurable mapping, where $A^*$ is $g$'s maximal domain.
\label{def:measurable-mapping-arrow-computation}
\end{definition}

\begin{theorem}[maximal domain measurability]
Let $g : X \mapto Y$ be an $\A!\B$-measurable mapping arrow computation.
Its maximal domain $A^*$ is in $\A$.
\end{theorem}
\begin{proof}
By definition, $g~A^*$ is a measurable mapping. $Y \in \B$, and $preimage~(g~A^*)~Y = domain~(g~A^*) = A^*$.
\end{proof}

Of course, mapping arrow computations can be applied to sets other than their maximal domains.
We need to ensure doing so yields a measurable mapping, at least for measurable subsets of $A^*$.
Fortunately, that is true without any extra conditions.

\begin{lemma}
Let $g : X \pto Y$ be an $\A!\B$-measurable mapping.
For any $A \in \A$, $restrict~g~A$ is $\A!\B$-measurable.
\label{lem:restricted-mappings-are-measurable}
\end{lemma}

\begin{theorem}
Let $g : X \mapto Y$ be an $\A!\B$-measurable mapping arrow computation with maximal domain $A^*$.
For all $A \subseteq A^*$ with $A \in \A$, $g~A$ is an $\A!\B$-measurable mapping.
\label{thm:restricted-computations-are-measurable}
\end{theorem}
\begin{proof}
Use the mapping arrow law~(Definition~\ref{def:mapping-arrow-law}) and Lemma~\ref{lem:restricted-mappings-are-measurable}.
\end{proof}

We do not need to prove that all interpretations using $\meaningof{\cdot}\gen$ are measurable.
However, we do need to prove that all the mapping arrow combinators preserve measurability.

\subsubsection{Case: Composition}

Proving compositions are measurable takes the most work.
The main complication is that, under measurable mappings, while \emph{preimages} of measurable sets are measurable, \emph{images} of measurable sets may not be.
We need the following four extra theorems to get around this.

\begin{lemma}[images of preimages]
Let $g : X \pto Y$ and $B \subseteq Y$. Then $image~g~(preimage~g~B) \subseteq B$.
\label{lem:images-of-preimages}
\end{lemma}

\begin{lemma}[expanded post-composition]
Let $g_1 : X \pto Y$ and $g_2 : Y \pto Z$ such that $range~g_1 \subseteq domain~g_2$, and let $g_2' : Y \pto Z$ such that $g_2 \subseteq g_2'$.
Then $g_2 \circ\map g_1 = g_2' \circ\map g_1$.
\label{lem:composition-expansion}
\end{lemma}

\begin{theorem}[mapping arrow monotonicity]
Let $g : X \mapto Y$.
For any $A' \subseteq A \subseteq A^*$, $g~A' \subseteq g~A$.
\label{thm:mapping-arrow-monotonicity}
\end{theorem}
\begin{proof}
Follows from Definition~\ref{def:mapping-arrow-law}.
\end{proof}

\begin{theorem}[maximal domain subsets]
Let $g : X \mapto Y$. For any $A \subseteq A^*$, $domain~(g~A) = A$.
\label{thm:maximal-domain-subsets}
\end{theorem}
\begin{proof}
Follows from Theorem~\ref{thm:domain-closure-operators}.
\end{proof}

Now we can prove measurability.

\begin{lemma}[measurability under $\circ\map$]
If $g_1 : X \pto Y$ is $\A!\B$-measurable and $g_2 : Y \pto Z$ is $\B!\C$-measurable, then $g_2 \circ\map g_1$ is $\A!\C$-measurable.
\label{lem:compositions-are-measurable}
\end{lemma}

\begin{theorem}[measurability under $(\compmap)$]
If $g_1 : X \mapto Y$ is $\A!\B$-measurable and $g_2 : Y \mapto Z$ is $\B!\C$-measurable, then $g_1~\compmap~g_2$ is $\A!\C$-measurable.
\end{theorem}
\begin{proof}
Let $A^* \in \A$ and $B^* \in \B$ be respectively $g_1$'s and $g_2$'s maximal domains.
The maximal domain of $g_1~\compmap~g_2$ is $A^{**} := preimage~(g_1~A^*)~B^*$, which is in $\A$.
By definition,
\begin{equation}
	(g_1~\compmap~g_2)~A^{**} \ = \ 
		\lzfclet{
			g_1' & g_1~A^{**} \\
			g_2' & g_2~(range~g_1')
		}{g_2' \circ\map g_1'}
\end{equation}
By Theorem~\ref{thm:restricted-computations-are-measurable}, $g_1'$ is an $\A!\B$-measurable mapping.
Unfortunately, $g_2'$ may not be $\B!\C$-measurable when $range~g_1' \not\in \B$.

Let $g_2'' := g_2~B^*$, which is a $\B!\C$-measurable mapping.
By Lemma~\ref{lem:compositions-are-measurable}, $g_2'' \circ\map g_1'$ is $\A!\C$-measurable.
We need only show that $g_2' \circ\map g_1' = g_2'' \circ\map g_1'$, which by Lemma~\ref{lem:composition-expansion} is true if $range~g_1' \subseteq domain~g_2'$ and $g_2' \subseteq g_2''$.

By Theorem~\ref{thm:maximal-domain-subsets}, $A^{**} \subseteq A^*$ implies $domain~g_1' = A^{**}$.
By Theorem~\ref{thm:mapping-arrow-monotonicity} and Lemma~\ref{lem:images-of-preimages},
\begin{align*}
	range~g_1'
		%&\ =\ image~g_1'~A^{**} \\
		&\ =\ image~(g_1~A^{**})~(preimage~(g_1~A^*)~B^*) \\
		&\ =\ image~(g_1~A^*)~(preimage~(g_1~A^*)~B^*) \\
		&\ \subseteq\ B^*
\end{align*}
$range~g_1' \subseteq B^*$ implies (by Theorem~\ref{thm:maximal-domain-subsets}) that $domain~g_2' = range~g_1'$, and (by Theorem~\ref{thm:mapping-arrow-monotonicity}) that $g_2' \subseteq g_2''$.
\end{proof}

\subsubsection{Case: Pairing}

\begin{lemma}[measurability under $\pair{\cdot,\cdot}\map$]
If $g_1 : X \pto Y_1$ is $\A!\B_1$-measurable and $g_2 : X \pto Y_2$ is $\A!\B_2$-measurable, then $\pair{g_1,g_2}\map$ is $\A!(\B_1 \otimes \B_2)$-measurable.
\label{lem:pairings-are-measurable}
\end{lemma}

\begin{theorem}[measurability under $(\pairmap)$]
If $g_1 : X \mapto Y_1$ is $\A!\B_1$-measurable and $g_2 : X \mapto Y_2$ is $\A!\B_2$-measurable, then $g_1~\pairmap~g_2$ is $\A!(\B_1 \otimes \B_2)$-measurable.
\end{theorem}
\begin{proof}
Let $A_1^*$ and $A_2^*$ be respectively $g_1$'s and $g_2$'s maximal domains.
The maximal domain of $g_1~\pairmap~g_2$ is $A^{**} := A_1^* \i A_2^*$, which is in $\A$.
By definition, $(g_1~\pairmap~g_2)~A^{**} = \pair{g_1~A^{**},g_2~A^{**}}\map$, which by Lemma~\ref{lem:pairings-are-measurable} is $\A!(\B_1 \otimes \B_2)$-measurable.
\end{proof}

\subsubsection{Case: Conditional}

\begin{lemma}[union of disjoint, measurable mappings]
Let $G : Set~(X \pto Y)$ be a countable set of $\A!\B$-measurable mappings with disjoint domains.
Their union is $\A!\B$-measurable.
\label{lem:union-of-measurable-mappings}
\end{lemma}

\begin{theorem}[measurability under $\ifmap$]
If $g_1 : X \mapto Bool$ is $\A!(\powerset~Bool)$-measurable, and $g_2 : X \mapto Y$ and $g_3 : X \mapto Y$ are $\A!\B$-measurable, then $\ifmap~g_1~g_2~g_3$ is $\A!\B$-measurable.
\end{theorem}
\begin{proof}
Let $\A_1^*$, $\A_2^*$ and $\A_3^*$ be $g_1$'s, $g_2$'s and $g_3$'s maximal domains.
The maximal domain of $\ifmap~g_1~g_2~g_3$ is
\begin{equation}
\begin{aligned}
	A_2^{**} &\ :=\ A_2^* \i preimage~(g_1~\A_1^*)~\set{true} \\
	A_3^{**} &\ :=\ A_3^* \i preimage~(g_1~\A_1^*)~\set{false} \\
	A^{**} &\ :=\ A_2^{**} \uplus A_3^{**}
\end{aligned}
\end{equation}
Because $preimage~(g_1~\A_1^*)~B \in \A$ for any $B \subseteq Bool$, $A^{**} \in \A$.
By definition,
\begin{equation}
	\ifmap~g_1~g_2~g_3~A^{**} \ = \ 
		\lzfclet{
			g_1' & g_1~A^{**} \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{g_2' \uplus\map g_3'}
\end{equation}
By hypothesis, $g_1'$, $g_2'$ and $g_3'$ are measurable mappings, and the mapping arrow law imples $g_2'$ and $g_3'$ have disjoint domains.
Apply Lemma~\ref{lem:union-of-measurable-mappings}.
\end{proof}

\subsubsection{Case: Laziness}

\begin{theorem}[measurability of $\emptyset$]
For any $\sigma$-algebras $\A$ and $\B$, the empty mapping $\emptyset$ is $\A!\B$-measurable.
\label{thm:empty-mapping-measurable}
\end{theorem}
\begin{proof}
For any $B \in \B$, $preimage~\emptyset~B = \emptyset$, and $\emptyset \in \A$.
\end{proof}

\begin{theorem}[measurability under $\lazymap$]
Let $g : 1 \tto (X \mapto Y)$. If $g~0$ is $\A!\B$-measurable, then $\lazymap~g$ is $\A!\B$-measurable.
\end{theorem}
\begin{proof}
The maximal domain $A^{**}$ of $\lazymap~g$ is the same as that of $g~0$.
By definition,
\begin{equation}
	\lazymap~g~A^{**} \ = \ if~(A^{**} = \emptyset)~\emptyset~(g~0~A^{**})
\end{equation}
If $A^{**} = \emptyset$, then $\lazymap~g~A^{**} = \emptyset$; apply Theorem~\ref{thm:empty-mapping-measurable}.
If $A^{**} \neq \emptyset$, then $\lazymap~g = g~0$, which is $\A!\B$-measurable.
\end{proof}

\subsection{Measurable Probabilistic Computations}

As before, we first need to define what it means for a computation to be measurable.

\begin{definition}[measurable mapping* arrow computation]
Let $\A$ and $\B$ be $\sigma$-algebras on $(R \times T) \times X$ and $Y$.
A computation $g : X \pmapto Y$ is $\A!\B$-\keyword{measurable} if $g~j_0$ is an $\A!\B$-measurable mapping arrow computation.
\end{definition}

Clearly, if any $g~j$ is measurable, so are $g~(left~j)$ and $g~(right~j)$.
By induction, if $g$ is a measurable mapping* arrow computation, then for any $j \in J$, $g~j$ is an $\A!\B$-measurable mapping arrow computation.

To make general measurability statements about computations, whether they have flat or product types, it helps to have a notion of a standard $\sigma$-algebra.

\begin{definition}[standard $\sigma$-algebra]
For a set $X$ used as a type, $\Sigma~X$ denotes its \mykeyword{standard $\sigma$-algebra}, which must be defined under the following constraints:
\begin{align}
	\Sigma~\pair{X_1,X_2} & = \Sigma~X_1 \otimes \Sigma~X_2
	\label{eqn:standard-finite-product-rule}
\\
	\Sigma~(J \to X) & = (\Sigma~X)^{\otimes J}
	\label{eqn:standard-arbitrary-product-rule}
%\\
%	X' \in \Sigma~X \ \implies \ \Sigma~X' & = \setb{X' \i A}{A \in \Sigma~X}
%	\label{eqn:standard-subset-rule}
\end{align}
The predicate ``is measurable'' means ``is measurable with respect to standard $\sigma$-algebras.''
\label{def:standard-sigma-algebra}
\end{definition}

So that we can measure boolean singletons and any set of branch traces, we define
\begin{align}
	\Sigma~Bool &\ ::=\ \powerset~Bool
	\label{eqn:standard-boolean-rule}
\\
	\Sigma~T &\ ::=\ \powerset~T
	\label{eqn:standard-traces-rule}
\end{align}

\begin{lemma}[measurable mapping arrow lifts]
$\arrmap~id$, $\arrmap~fst$ and $\arrmap~snd$ are measurable.
$\arrmap~(const~b)$ is measurable if $\set{b}$ is a measurable set.
For all $j \in J$, $\arrmap~(\pi~j)$ is measurable.
\end{lemma}

%XXX: should that really be a lemma? $\arrmap$ removes $\bot$ from the domain

\begin{corollary}
$\arrpmap~id$, $\arrpmap~fst$ and $\arrpmap~snd$ are measurable.
$\arrpmap~(const~b)$ is measurable if $\set{b}$ is a measurable set.
$random\pmap$ and $branch\pmap$ are measurable.
\end{corollary}

\begin{theorem}[$AStore$ measurability transfer]
Every $AStore$ arrow combinator produces measurable mapping* computations from measurable mapping* computations.
\label{thm:astore-measurability-transfer}
\end{theorem}
\begin{proof}
$AStore$'s combinators are defined in terms of the base arrow's combinators and $\arrmap~fst$ and $\arrmap~snd$.
\end{proof}

\begin{theorem}
$\convifpmap$ is measurable.
\end{theorem}
\begin{proof}
$branch\pmap$ is measurable, and $\arrmap~agrees$ is measurable by~\eqref{eqn:standard-boolean-rule}.
\end{proof}

\begin{theorem}[finite expressions are measurable]
For any expression $\mathit{e}$ lacking first-order applications, $\meaningof{\mathit{e}}\pmap$ is measurable.
\label{thm:nonrecursive-programs-are-measurable}
\end{theorem}
\begin{proof}
By structural induction and the above theorems.
\end{proof}

\begin{theorem}[approximation with expressions]
Let $g := \meaningofconv{\mathit{e}}\pmap : X \pmapto Y$.
For all $t \in T$, let $A := (R \times \set{t}) \times X$.
There is an expression $\mathit{e'}$ for which $\meaningof{\mathit{e'}}\pmap~j_0~A = g~j_0~A$.
\label{thm:nonrecursive-approximation}
\end{theorem}
\begin{proof}
Let the index prefix $J'$ contain every $j$ for which $t~j \neq \bot$.
To construct $\mathit{e'}$, exhaustively apply first-order functions in $\mathit{e}$, but replace any $\convifpmap$ whose index $j$ is not in $J$ with the equivalent expression $\bot$.
Because $\mathit{e}$ is well-defined, recurrences must be guarded by $if$, so this process terminates after finitely many applications.
\end{proof}

\begin{theorem}[all probabilistic expressions are measurable]
For all expressions $\mathit{e}$, $\meaningofconv{\mathit{e}}\pmap$ is measurable.
\label{thm:everything-is-measurable}
\end{theorem}
\begin{proof}
Let $g := \meaningofconv{\mathit{e}}\pmap$ and $g' := g~j_0~((R \times T) \times X)$.
By Corollary~\ref{cor:correct-convergence}, $g' = g~j_0~A^*$ where $A^*$ is $g$'s maximal domain; thus we need only show that $g'$ is a measurable mapping.

By mapping arrow monotonicity (XXX?) (Theorem~\ref{thm:mapping-arrow-monotonicity}),
\begin{equation}
	g' \ =\ \U\limits_{t \in T} g~j_0~((R \times \set{t}) \times X)
\end{equation}
By Theorem~\ref{thm:nonrecursive-approximation}, for every $t \in T$, there is an expression that computes $g~((R \times \set{t}) \times X)$.
By~\eqref{eqn:standard-traces-rule} and Theorem~\ref{thm:nonrecursive-programs-are-measurable}, each is measurable.
By the mapping arrow law (Definition~\ref{def:mapping-arrow-law}), each is disjoint.
By Lemma~\ref{lem:union-of-measurable-mappings}, their union is measurable.
\end{proof}

Theorem~\ref{thm:everything-is-measurable} remains true when $\meaningof{\cdot}\gen$ is extended with any rule whose right side is measurable, including rules for real arithmetic, equality, inequality and limits.
More generally, any continuous or (countably) piecewise continuous function can be made available as a language primitive, as long as its domain's and codomain's standard $\sigma$-algebras are generated from their topologies.

It is not difficult to compose $\meaningof{\cdot}\gen$ with another semantic function that lifts and defunctionalizes lambda expressions.
Thus, the interpretations of all expressions in higher-order languages are measurable.

\subsection{Measurable Projections}

If $g := \meaningofconv{\mathit{e}}\pmap : X \pmapto Y$, then the probability of a measurable output set $B \in \Sigma~Y$ is
\begin{equation}
	P~(image~(fst~\arrowcomp~fst)~(preimage~(g~j_0~A^*)~B))
\end{equation}
if $domain~P = \Sigma~R$. 
Unfortunately, projected sets are generally not measurable.
Fortunately, for interpretations of programs $\meaningofconv{\mathit{p}}\pmap$, for which $X = \set{\pair{}}$, we have a special case.

\begin{theorem}[measurable finite projections]
Let $A \in \Sigma~\pair{X_1,X_2}$.
If $X_2$ is at most countable and $\Sigma~X_2 = \powerset~X_2$, then $image~fst~A \in \A_1$.
\label{thm:measurable-projections}
\end{theorem}
\begin{proof}
Because $\Sigma~X_2 = \powerset~X_2$, $A$ is a countable union of rectangles of the form $A_1 \times \set{a_2}$, where $A_1 \in \Sigma~X_1$ and $a_2 \in X_2$.
Because $image~fst$ distributes over unions, $image~fst~A$ is a countable union of sets in $\Sigma~X_1$.
\end{proof}

\begin{theorem}
Let $g : X \pmapto Y$ be measurable.
If $X$ is at most countable and $\Sigma~X = \powerset~X$, then for all $B \in \Sigma~Y$,
\begin{equation}
	image~(fst~\arrowcomp~fst)~(preimage~(g~j_0~A^*)~B) \ \in\ \Sigma~R
\end{equation}
\end{theorem}
\begin{proof}
$T$ is countable and $\Sigma~T = \powerset~T$ by definition~\eqref{eqn:standard-traces-rule}; apply Theorem~\ref{thm:measurable-projections} twice.
\end{proof}

In particular, for any well-defined $\meaningofconv{\mathit{p}}\pmap : \set{\pair{}} \pmapto Y$, the probabilities of measurable output sets are well-defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approximating Semantics}
\label{sec:approximating-semantics}

If we were to confine preimage computation to finite sets, we could implement the preimage arrow directly.
But we would like something that works efficiently on infinite sets, even if it means approximating.

Trying to generalize all useful approximation methods would result in a specification that cannot be directly implemented.
Instead, we focus on a specific method: approximating product sets with covering rectangles.
We recover some generality by stating correctness theorems in terms of general properties such as monotonicity.

\subsection{Implementable Lifts}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		id\pre~A &\ := \ \arrpre~id~A &&\!\!\!\!\equiv \ \pair{A,\fun{B}{B}} \\
		fst\pre~A &\ := \ \arrpre~fst~A &&\!\!\!\!\equiv \ \pair{proj_{fst}~A,unproj_{fst}~A} \\
		snd\pre~A &\ := \ \arrpre~snd~A &&\!\!\!\!\equiv \ \pair{proj_{snd}~A,unproj_{snd}~A}
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&proj_{fst} := image~fst;\ \ proj_{snd} := image~snd
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&unproj_{fst} : Set~\pair{X_1,X_2} \tto Set~X_1 \tto Set~\pair{X_1,X_2} \\
		&unproj_{fst}~A~B \lzfcsplit{
			&\ :=\ preimage~(mapping~fst~A)~B \\[2pt]
			&\ \;\equiv\ A \i (B \times proj_{snd}~A)
			%&\ :=\ A \i \prod_{j' \in J} if~(j' = j)~B~(project~j'~A)
		}
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		const\pre~b~A &\ := \ \arrpre~(const~b)~A &&\!\!\!\!\equiv \ \pair{\set{b},\fun{B}{if~(B = \emptyset)~\emptyset~A}} \\
		\pi\pre~j~A &\ := \ \arrpre~(\pi~j)~A &&\!\!\!\!\equiv \ \pair{project~j~A, unproject~j~A}
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&project : J \tto Set~(J \to X) \tto Set~X \\
		&project~j~A \ := \ image~(\pi~j)~A
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&unproject : J \tto Set~(J \to X) \tto Set~X \tto Set~(J \to X) \\
		&unproject~j~A~B \lzfcsplit{
			&\ :=\ preimage~(mapping~(\pi~j)~A)~B \\[2pt]
			&\ \;\equiv\ A \i \prod_{i \in J} if~(j = i)~B~(project~j~A)
			%&\ :=\ A \i \prod_{j' \in J} if~(j' = j)~B~(project~j'~A)
		}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Preimage arrow lifts needed to interpret probabilistic programs.
The definition of $unproj_{snd}$ is like $unproj_{fst}$'s.}
\label{fig:extra-preimage-arrow-defs}
\end{figure*}

We would like to be able to compute preimages of uncountable sets, such as real intervals.
This would seem to be a show-stopper: $preimage~g~B$ is uncomputable for most uncountable sets $B$ no matter how cleverly they are represented.
Further, because $pre$, $\liftpre$ and $\arrpre$ are ultimately defined in terms of $preimage$, we cannot implement them.

Fortunately, we need only certain lifts.
Figure~\ref{fig:semantic-function} (which defines $\meaningof{\cdot}\gen$) lifts $id$, $const~b$, $fst$ and $snd$.
Section~\ref{sec:probabilistic-programs}, which defines the combinators used to interpret partial, probabilistic programs, lifts $\pi~j$ and $agrees$.
Measurable functions made available as language primitives of course must be lifted to the preimage arrow.

Figure~\ref{fig:extra-preimage-arrow-defs} gives expressions equivalent to $\arrpre~id$, $\arrpre~fst$, $\arrpre~snd$, $\arrpre~(const~b)$ and $\arrpre~(\pi~j)$.
(We will deal with $agrees$ separately.)
By inspecting these expressions, we see that we need to model sets in a way that
the following are representable and can be computed in finite time:
\begin{equation}
\begin{aligned}
	&\text{\textbullet\ $A \i B$, $\emptyset$, $\set{true}$, $\set{false}$ and $\set{b}$ for every $const~b$} \\
	&\text{\textbullet\ $A_1 \times A_2$, $proj_{fst}~A$ and $proj_{snd}~A$} \\
	&\text{\textbullet\ $J \to X$, $project~j~A$ and $unproject~j~A~B$} \\
	&\text{\textbullet\ $A = \emptyset$} \\
\end{aligned}
\label{eqn:exact-rectangle-ops}
\end{equation}
Before addressing computability, we need to define families of sets under which these operations are closed.

\subsection{Rectangular Families}

\begin{definition}[rectangular family]
For a set $X$ used as a type, $Rect~X$ denotes the \mykeyword{rectangular family} of subsets of $X$, which must be satisfy the following rules:
\begin{align}
	Rect~\pair{X_1,X_2} &\ = \ (Rect~X_1) \boxtimes (Rect~X_2)
	\label{eqn:standard-rect-finite-product-rule}
\\
	Rect~(J \to X) &\ = \ (Rect~X)^{\boxtimes J}
	\label{eqn:standard-rect-arbitrary-product-rule}
%\\
%	X' \in Rect~X \ \implies \ Rect~X' &\ =\ \setb{X' \i A}{A \in Rect~X}
%	\label{eqn:standard-rect-subset-rule}
\end{align}
where
\begin{align}
	\A_1 \boxtimes \A_2 &\ := \ \setb{A_1 \times A_2}{A_1 \in \A_1, A_2 \in \A_2} \\
	\A^{\boxtimes J} &\ := \ \setb{\textstyle\prod_{j \in J} A_j}{\Forall{j \in J}{A_j \in \A}}
\end{align}
lift cartesian products to sets of sets.
\label{def:standard-rectangle}
\end{definition}

XXX: consider baking ``no more than finitely many non-full axes'' into the definition of rectangular family

For example, if $Rect~\Re$ contains all the closed real intervals, then by~\eqref{eqn:standard-rect-finite-product-rule}, $[0,2] \times [1,\pi] \in Rect~\pair{\Re,\Re}$.

For every non-product type $X$, we require $\emptyset \in Rect~X$, a universal set $X \in Rect~X$, singletons $\set{a} \in Rect~X$ for all $a \in X$, and for $Rect~X$ to be closed under intersection.
It is not hard to show that these properties extend to rectangular families, and that the collection of all rectangular families is closed under products, projections, and $unproject$.

Further, all of the operations in~\eqref{eqn:exact-rectangle-ops} can be exactly implemented if finite sets are modeled directly, sets in an ordered space (such as $\Re$) are modeled by intervals, and sets in $Rect~\pair{X_1,X_2}$ are modeled by pairs of type $\pair{Rect~X_1,Rect~X_2}$.
Though $J$ is infinite, sets in $Rect~(J \to X)$ can be modeled by \emph{finite} binary trees of sets in $Rect~X$, because a converging preimage computation can apply $unproject$ only finitely many times to $J \to X$.

We defined one nonrectangular domain: the set of branch traces $T$, which contains every $t \in J \to Bool_\bot$ for which $t~j \neq \bot$ for no more than finitely many $J$.
Fortunately, under conditions that are always met while computing approximate preimages, we can represent $T$ subsets as $J \to Bool_\bot$ rectangles, implicitly intersected with $T$.

\begin{theorem}
Let $T' \in Rect~(J \to Bool_\bot)$ such that $\bot \notin project~j~T'$ for no more than finitely many $j \in J$.
Then $project~j~(T' \i T) = project~j~T'$.
\end{theorem}
\begin{proof}
The subset case is by monotonicity of projections.
For the superset case, let $b \in project~j~T'$.
Define $t$ by
\begin{equation}
	t~j' =
	\begin{cases}
		b & j' = j \\
		\text{any member of } project~j'~T' & \bot \notin project~j'~T' \\
		\bot & \bot \in project~j'~T'
	\end{cases}
\end{equation}
For no more than finitely many $j' \in J$, $t~j' \neq \bot$, so $t \in T$; also $t \in T'$ by construction.
Thus, there exists a $t \in T' \i T$ such that $t~j = b$, so $b \in project~j~(T' \i T)$.
\end{proof}

\begin{corollary}
Under the same conditions, for all $B \subseteq Bool$, $unproject~j~(T' \i T)~B = T \i unproject~j~T'~B$.
\end{corollary}

\subsection{Approximate Preimage Mapping Operations}

Implementing $\lazypre$ (defined in Figure~\ref{fig:preimage-arrow-defs}) requires computing $pre$, but only for the empty mapping, which is trivial: $pre~\emptyset \equiv \pair{\emptyset,\fun{B}\emptyset}$.
Implementing the other combinators requires implementing the preimage mapping operations $(\circ\pre)$, $\pair{\cdot,\cdot}\pre$ and $(\uplus\pre)$.

From the preimage mapping definitions (Figure~\ref{fig:preimage-mapping-defs}), we see that $ap\pre$ is defined in terms of $(\i)$ and that $(\circ\pre)$ is defined in terms of $ap\pre$, so $(\circ\pre)$ is directly implementable.
Unfortunately, we hit a snag with $\pair{\cdot,\cdot}\pre$: it loops over possibly uncountably many members of $B$ in a big union.
At this point, we need to approximate.

\begin{theorem}[pair preimage overapproximation]
Let $g_1 \in X \pto Y_1$ and $g_2 \in X \pto Y_2$.
For all $B \subseteq Y_1 \times Y_2$, $preimage~\pair{g_1,g_2}\map~B \subseteq preimage~g_1~(proj_{fst}~B) \i preimage~g_2~(proj_{snd}~B)$.
\end{theorem}
\begin{proof}
By monotonicity of preimages and projections, and by Lemma~\ref{lem:preimage-under-pairing}.
\end{proof}

It is not hard to show that the following replacement:
\begin{equation}
\begin{aligned}
	&\pair{\cdot,\cdot}\pre' : (X \prepto Y_1) \tto (X \prepto Y_2) \tto (X \prepto Y_1 \times Y_2) \\
	&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre' \ := \ \\
	&\tab\pair{Y_1' \times Y_2',\fun{B}{p_1~(proj_{fst}~B) \i p_2~(proj_{snd}~B)}}
\end{aligned}
\end{equation}
computes covering rectangles of preimages under pairing.

For $(\uplus\pre)$, we need an approximating replacement for $(\u)$ under which rectangular families are closed.
In other words, we need a lattice join with respect to $(\subseteq)$, with the following additional properties:
\begin{equation}
\begin{aligned}
	(A_1 \times A_2) \join (B_1 \times B_2) &\ = \ (A_1 \join B_1) \times (A_2 \join B_2) \\
	(\textstyle\prod_{j \in J} A_j) \join (\textstyle\prod_{j \in J} B_j) &\ = \ \textstyle\prod_{j \in J} A_j \join B_j
\label{eqn:join-laws}
\end{aligned}
\end{equation}
If for every non-product type $X$, $Rect~X$ is closed under $(\join)$, then rectangular families are clearly closed under $(\join)$. Further, for any $A$ and $B$, $A \u B \subseteq A \join B$.

Replacing each union in $(\uplus\pre)$ with a join results in
\begin{equation}
\begin{aligned}
	&(\uplus\pre') : (X \prepto Y) \tto (X \prepto Y) \tto (X \prepto Y) \\
	&\lzfcsplit{
		&h_1 \uplus\pre' h_2 \ := \ 
		\lzfclet{
				Y' & (range\pre~h_1) \join (range\pre~h_2) \\
				p & \fun{B}{(ap\pre~h_1~B) \join (ap\pre~h_2~B)}
			}{\pair{Y',p}}
	}
\end{aligned}
\end{equation}
which overapproximates $(\uplus\pre)$.


\subsection{Partial Programs}

XXX: too much yanking the user's chain here

We have enough of the specification to implement the preimage arrow and the $AStore$ arrow transformer.
But we cannot yet deal with programs that may diverge, or that converge with probability $1$.
For that, we need to approximate $\convifppre$~\eqref{eqn:ifppre-def}.

Directly implementing $\convifppre$, and thus $agrees$, cannot work.
Turning $agrees$ into a mapping shows why:
\begin{equation}
\begin{aligned}
	&\arrmap~agrees~(Bool \times Bool) = \\
	&\tab\set{\pair{\pair{true,true},true},\pair{\pair{false,false},false}}
\end{aligned}
\end{equation}
The preimage of $Bool$ is $\set{\pair{true,true},\pair{false,false}}$, which is not rectangular.

A lengthy (elided) sequence of substitutions to the defining expression for $\convifppre$ results in
\begin{equation}
\begin{aligned}
	&\convifppre~k_1~k_2~k_3~j~A\ \equiv \\
	&\tab\lzfclet{
		\pair{C_k,p_k} & k_1~j_1~A \\
		\pair{C_b,p_b} & branch\ppre~j~A \\
		C_2 & C_k \i C_b \i \set{true} \\
		C_3 & C_k \i C_b \i \set{false} \\
		A_2 & p_k~C_2 \i p_b~C_2 \\
		A_3 & p_k~C_3 \i p_b~C_3 \\
	}{(k_2~j_2~A_2) \uplus\pre (k_3~j_3~A_3)}
\end{aligned}
\label{eqn:expanded-convifppre}
\end{equation}
where $j_1 = left~j$ and so on.
This has no trace of $agrees$ and clearly preserves rectangularity if $k_1$, $k_2$ and $k_3$ do.
Yet it is still not good enough.
When $A_2$ and $A_3$ overapproximate $\emptyset$, it takes unnecessary branches, which can lead to divergence.
In the exact semantics, a well-defined program interpreted using $\convifppre$ never diverges.

Suppose we defined the approximating ${\convifppre}'$ to take \emph{no branches} when $branch\ppre~j~A$ contains both $true$ and $false$.
Because $branch\ppre~j~A$ can return $\set{true}$ or $\set{false}$ for at most finitely many $j \in J$, there exists a suffix set $J' \subseteq J$ for which no branches will be taken.

The returned preimage mapping's range is a subset of $Y$, and the preimages it computes must be subsets of $A_2 \join A_3$.
Therefore, we might replace the $let$ body in~\eqref{eqn:expanded-convifppre} with
\begin{equation}
\begin{aligned}
	\lzfcsplit{if~
		&(C_b = \set{true,false}) \\
		&\pair{Y,\fun{B}\lzfcsplit{A_2 \join A_3}} \\
		&(k_2~j_2~A_2 \uplus\pre k_3~j_3~A_3)}
\end{aligned}
\end{equation}
which computes the same preimages if $C_b \subset \set{true,false}$, and takes no branches and overapproximates otherwise.
A well-defined program interpreted using a conditional defined this way should always converge.

Unfortunately, we cannot refer to $Y$ in a function definition: it is only part of the type of $\convifppre$.
We need a value $\top \in Rect~X$ for every $X$ to represent $X$ itself.
It should behave exactly as $X$; e.g. $x \in \top$ for all $x \in X$ and $\top \i X' = X'$ for all $X' \subseteq X$.
Thus, the approximating ${\convifppre}'$ can be written in terms of $\top$ instead of in terms of its (erased) type $Y$.

\begin{figure*}[t]\centering
\begin{minipage}{\textwidth}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \prepto' Y ::= \pair{Rect~Y, Rect~Y \tto Rect~X}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&ap\pre' : (X \prepto' Y) \tto Rect~Y \tto Rect~X \\
		&ap\pre'~\pair{Y',p}~B \ := \ p~(B \i Y') 
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\pre') : (Y \prepto' Z) \tto (X \prepto' Y) \tto (X \prepto' Z) \\
		&\pair{Z',p_2} \circ\pre' h_1 \ := \ \pair{Z', \fun{C}{ap\pre'~h_1~(p_2~C)}}
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\pre' : (X \prepto' Y_1) \tto (X \prepto' Y_2) \tto (X \prepto' Y_1 \times Y_2) \\
		&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre' \ := \\
		&\tab\pair{Y_1' \times Y_2',\fun{B}{p_1~(proj_{fst}~B) \i p_2~(proj_{snd}~B)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\pre') : (X \prepto' Y) \tto (X \prepto' Y) \tto (X \prepto' Y) \\
		&\pair{Y_1',p_1} \uplus\pre' \pair{Y_2',p_2} \ := \\
		&\tab\pair{Y_1' \join Y_2',\fun{B}{(ap\pre'~\pair{Y_1',p_1}~B) \join (ap\pre'~\pair{Y_2',p_2}~B)}
		}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\subcaption{Definitions for approximating preimage mappings that compute rectangular preimage covers.}
\end{minipage}
\begin{minipage}{\textwidth}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \preto' Y ::= Rect~X \tto (X \prepto' Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\comppre') : (X \preto' Y) \tto (Y \preto' Z) \tto (X \preto' Z) \\
		&(h_1~\comppre'~h_2)~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(range\pre'~h_1')
			}{h_2' \circ\pre' h_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairpre') : (X \preto' Y_1) \tto (X \preto' Y_2) \tto (X \preto' \pair{Y_1,Y_2}) \\
		&(h_1~\pairpre'~h_2)~A \ := \ \pair{h_1~A,h_2~A}\pre'
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifpre' : (X \preto' Bool) \tto (X \preto' Y) \tto (X \preto' Y) \tto (X \preto' Y) \\
		&\ifpre'~h_1~h_2~h_3~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(ap\pre'~h_1'~\set{true}) \\
				h_3' & h_3~(ap\pre'~h_1'~\set{false})
			}{h_2' \uplus\pre' h_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazypre' : (1 \tto (X \preto' Y)) \tto (X \preto' Y) \\
		&\lazypre'~h~A \ := \ if~(A = \emptyset)~\pair{\emptyset,\fun{B}{\emptyset}}~(h~0~A)
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\subcaption{An approximating preimage arrow, defined in terms of approximating preimage mappings.}
\end{minipage}
\begin{minipage}{\textwidth}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		X \ppreto' Y &\ ::= \ J \tto (\pair{S,X} \preto' Y) \\
		S &\ ::= \ (J \to [0,1]) \times (J \to Bool_\bot)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compppre') : (X \ppreto' Y) \tto (Y \ppreto' Z) \tto (X \ppreto' Z) \\
		&(k_1~\compppre'~k_2)~j \ := \\
			&\tab(fst\pre~\pairpre'~k_1~(left~j))~\comppre'~k_2~(right~j)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairppre') : (X \ppreto' Y_1) \tto (X \ppreto' Y_2) \tto (X \ppreto' \pair{Y_1,Y_2}) \\
		&(k_1~\pairppre'~k_2)~j \ := \ k_1~(left~j)~\pairpre'~k_2~(right~j)
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifppre' : (X \ppreto' Bool) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \\
		&\ifppre'~k_1~k_2~k_3~j \ := \
			\lzfcsplit{\ifpre'~&(k_1~(left~j)) \\ &(k_2~(left~(right~j))) \\ &(k_3~(right~(right~j)))}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazyppre' : (1 \tto (X \ppreto' Y)) \tto (X \ppreto' Y) \\
		&\lazyppre'~k~j \ := \ \lazypre'~\fun{0}{k~0~j}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowtrans\ppre' : (X \preto' Y) \tto (X \ppreto' Y) \\
		&\arrowtrans\ppre'~f~j \ := \ snd\pre~\comppre'~f
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\subcaption{An approximating preimage* arrow, defined in terms of the approximating preimage arrow.}
\end{minipage}
\begin{minipage}{\textwidth}
\begin{align*}
\begin{aligned}[t]
 	&\begin{aligned}[t]
		&random\ppre' : X \ppreto' [0,1] \\
		&random\ppre'~j \ := \ fst\pre~\comppre'~fst\pre~\comppre'~\pi\pre~j
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&branch\ppre' : X \ppreto' Bool \\
		&branch\ppre'~j \ := \ fst\pre~\comppre'~snd\pre~\comppre'~\pi\pre~j
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		fst\ppre' &:= \arrowtrans\ppre'~fst\pre \\
		snd\ppre' &:= \arrowtrans\ppre'~snd\pre;\ \cdots
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&{\convifppre}' : (X \ppreto' Bool) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \\
		&{\convifppre}'~k_1~k_2~k_3~j \ := \\
		&\tab\lzfclet{
			\pair{C_k,p_k} & k_1~(left~j)~A \\
			\pair{C_b,p_b} & branch\ppre~j~A \\
			A_2 & p_k~(C_k \i C_b \i \set{true}) \i p_b~(C_k \i C_b \i \set{true}) \\
			A_3 & p_k~(C_k \i C_b \i \set{false}) \i p_b~(C_k \i C_b \i \set{false}) \\
		}{if~\lzfcsplit{
				&(C_b = \set{true,false}) \\
				&\pair{\top,\fun{\underline{\ \ }}\lzfcsplit{A_2 \join A_3}} \\
				&(k_2~(left~(right~j))~A_2 \uplus\pre' k_3~(right~(right~j))~A_3)}}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\subcaption{Additional preimage* arrow combinators, for retrieving random numbers and branch traces, and computing without diverging.}
\end{minipage}
\caption{Implementable arrows that approximate preimage arrows.
Because $\arrpre$ is generally uncomputable, there is no corresponding $\arrpre'$ combinator.
However, specific lifts such as $fst\pre := \arrpre~fst$ are computable, and are defined in Figure~\ref{fig:extra-preimage-arrow-defs}.}
\label{fig:approximating-preimage-arrow-defs}
\end{figure*}

Figure~\ref{fig:approximating-preimage-arrow-defs} defines the final approximating preimage arrow.
This arrow, the lifts in Figure~\ref{fig:extra-preimage-arrow-defs}, and the semantic function $\meaningof{\cdot}\gen$ in Figure~\ref{fig:semantic-function} define an approximating semantics for partial, probabilistic programs.

\subsection{Correctness}

From here on, ${\meaningof{\cdot}\conv\ppre}'$ interprets programs as approximating preimage* arrow computations using ${\convifppre}'$.

The following theorems assume $h := \meaningof{\mathit{e}}\conv\ppre : X \ppreto Y$ and $h' := {\meaningof{\mathit{e}}\conv\ppre}' : X \ppreto' Y$ for some program $\mathit{e}$.
Further, define
\begin{equation}
\begin{aligned}
	refine~A &:= ap\pre~(h~j_0~A)~B \\
	refine'~A &:= ap\pre'~(h'~j_0~A)~B
\end{aligned}
\end{equation}

\begin{theorem}[approximation]
For all $A \in Rect~\pair{S,X}$ and $B \in Rect~Y$, $refine~A \subseteq refine'~A$.
\end{theorem}
\begin{proof}By construction.\end{proof}

\begin{theorem}[monotonicity]
$ap\pre'~(h'~j_0~A)~B$ is monotone in both $A$ and $B$.
\end{theorem}
\begin{proof}
XXX: todo
%Monotonicity of $ap\pre~(h~j_0~A)~B$ is a property of preimages.
%For $ap\pre'~(h'~j_0~A)~B$, lattice operations, which replace set operations in the approximating combinators, are monotone.
\end{proof}

\begin{theorem}[approximate preimages are nonincreasing]
For all $A \in Rect~\pair{S,X}$ and $B \in Rect~Y$, $refine'~A \subseteq A$.
\end{theorem}
\begin{proof}
XXX: todo
\end{proof}


\begin{corollary}[disjointness of approximate preimages]
If $A_1$ and $A_2$ are disjoint, then $refine'~A_1$ and $refine'~A_2$ are disjoint.
\end{corollary}

\subsection{Preimage Refinement}
\label{sec:discretization}

With disjointness and monotonicity properties, it is natural to suppose that we can compute probabilities of preimages of $B$ by computing preimages with respect to increasingly fine discretizations of $A$.
For example, starting with any partition $\A : Set~(Rect~\pair{S,X})$, we might repeat the following:
\begin{enumerate}
	\item Refine every rectangle in $\A$: let $\A' := image~refine'~\A$.
	\item Partition the rectangles in $\A'$: let $\A := \bigcup\limits_{A' \in \A'} partition~A'$.
\end{enumerate}
If the second step yields finer partitions, it seems the sum of the measures of each rectangle in $\A$ should approach the probability of $B$.

In general, however, we are computing the \keyword{Jordan outer measure}~(XXX: cite) of the preimage of $B$, which is not always equal to its measure.
An example of a program for which preimage refinement diverges is $rational?~random$, where $rational?$ returns $true$ when its argument is rational and diverges otherise: the preimage of $\set{true}$ has measure $0$, but its Jordan outer measure is $1$.

We conjecture that a minimal requirement for preimage refinement's measures to converge is that a program must converge with probability $1$.
There are certainly other requirements.
We leave these and proof of convergence of measures to future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementations}
\label{sec:implementation}

We have four implementations: one of the exact semantics, two direct implementations of the approximating semantics, and a less direct but more efficient implementation of the approximating semantics, which we call \mykeyword{Dr. Bayes}.

\subsection{Direct Implementations}

If sets are restricted to be finite, the arrows used as translation targets in the exact semantics, defined in Figures~\ref{fig:mapping-defs}, \ref{fig:bottom-arrow-defs}, \ref{fig:mapping-arrow-defs}, \ref{fig:preimage-mapping-defs}, \ref{fig:preimage-arrow-defs} and~\ref{fig:astore-arrow-defs}, can be implemented directly in any practical $\lambda$-calculus with a set data type.
Computing exact preimages is very inefficient, even under the interpretations of very small programs.
However, we have found our Typed Racket~(XXX: cite) implementation useful for finding theorem candidates.

Given a rectangular set library, the approximating preimage arrows defined in Figures~\ref{fig:extra-preimage-arrow-defs} and~\ref{fig:approximating-preimage-arrow-defs} can be implemented with few changes in any practical $\lambda$-calculus.
We have done so in Typed Racket and Haskell~(XXX: cite).
Both implementations' arrow combinator definitions are almost line-for-line transliterations from the figures.

Making the rectangular set type polymorphic seems to require the equivalent of a typeclass system.
In Haskell, it also requires GHC's multi-parameter typeclasses or indexed type families~(XXX: cite) to associate set types with the types of their members.
Using indexed type families, the only significant differences between the Haskell implementation and the approximating semantics are type contexts, \texttt{newtype} wrappers for arrow types, and using \texttt{Maybe} types as bottom arrow return types.

Typed Racket has no typeclass system on top of its type system, so the rectangular set type is monomorphic; thus, so are the arrow types.
The lack of type variables in the combinator types is the only significant difference between the implementation and the approximating semantics.

All three direct implementations can currently be found at XXX: URL.

\subsection{Dr. Bayes}

Our main implementation, \mykeyword{Dr. Bayes}, is written in Typed Racket.
It consists of the semantic function $\meaningof{\cdot}\genc$ from Figure~\ref{fig:semantic-function} and its extension $\meaningof{\cdot}\genc\conv$, the bottom* arrow as defined in Figures~\ref{fig:bottom-arrow-defs} and~\ref{fig:astore-arrow-defs}, the approximating preimage and preimage* arrows as defined in Figures~\ref{fig:extra-preimage-arrow-defs} and~\ref{fig:approximating-preimage-arrow-defs}, and algorithms to compute approximate probabilities.
We use it to test the feasibility of solving real-world problems by computing the measures of approximate preimages.

Dr. Bayes's preimage arrow implementation operates on a monomorphic rectangular set data type.
It includes floating-point intervals to overapproximate real intervals, with which we compute approximate preimages under arithmetic and inequalities.
Finding the smallest covering rectangle for images and preimages under $add : \pair{\Re,\Re} \tto \Re$ and other monotone functions is fairly straightforward.
For piecewise monotone functions, we distinguish cases using $\ifpre$; e.g.
\begin{equation}
	mul\pre := 
		\lzfcsplit{\ifpre~
			&(fst\pre~\comppre~positive?\pre) \\
			&(\lzfcsplit{\ifpre~
				&(snd\pre~\comppre~positive?\pre) \\
				&mul^{++}\pre \\
				&(\lzfcsplit{\ifpre~
					&(snd\pre~\comppre~negative?\pre) \\
					&mul^{+-}\pre \\
					&(const\pre~0)))
				}
			} \\
			&\cdots
		}
\end{equation}
To support data types, the set type includes tagged rectangles; for ad-hoc polymorphism, it includes disjoint unions.

Section~\ref{sec:discretization} outlines preimage refinement: a discretization algorithm that seems to converge for programs that halt with probability 1, consisting of repeatedly refining a program's domain and repartitioning it.
We do not use this algorithm directly in our main implementation because it is inefficient.
Good accuracy requires fine discretization, which is \emph{exponential} in the number of discretized axes.
For example, a nonrecursive program that contains only 10 uses of $random$ would need to partition 10 axes of $R$, the set of random sources.
Splitting each axis into only 4 disjoint intervals yields a partition of $R$ of size $4^{10} = 1,048,576$.

Fortunately, Bayesian practitioners do not care much about measuring preimages for its own sake.
They are concerned with the renormalized distribution of outputs given some condition.
Further, they are perfectly satisfied with sampling methods, which are usually much more efficient than methods based on enumeration.

Let $g : X \mapto Y$ be the interpretation of a program as a mapping arrow computation.
A Bayesian is primarily interested in the probability of $B^* \subseteq Y$ given some condition set $B \subseteq Y$.
If $A := preimage~(g~X)~B$ and $A^* := preimage~(g~X)~B^*$, the probability of $B^*$ given $B$ is
\begin{equation}
	Pr[A^*|A] \ := \ P~(A^* \i A) / P~A
\label{eqn:conditional-probability}
\end{equation}

One way to approximately compute probabilities is using \keyword{rejection sampling}.
Given a list of samples $xs$ of $X$,
\begin{equation}
	P~A \ \approx \ \frac{length~(filter~(\in A)~xs)}{length~xs}
\label{eqn:sampling-approx}
\end{equation}
Here, ``$\approx$'' roughly denotes probabilistic convergence as the length of $xs$ increases.
Combining~\eqref{eqn:conditional-probability} and~\eqref{eqn:sampling-approx} yields
\begin{equation}
\begin{aligned}
	Pr[A^* | A]
		&\ = \ P~(A^* \i A) / P~A
\\
		&\ \approx \ \frac{length~(filter~(\in A^* \i A)~xs) / length~xs}{length~(filter~(\in A)~xs) / length~xs}
\\
		&\ \approx \ \frac{length~(filter~(\in A^* \i A)~xs)}{length~(filter~(\in A)~xs)}
\end{aligned}
\label{eqn:sampling-approx-conditional}
\end{equation}
to compute approximate conditional probabilities.

The probability that any given element of $xs$ is in $A$ can be extremely small, so it would clearly be best to sample only within $A$.
While we cannot do that, we can quite easily sample $xs$ from a rectangular cover $A' \supseteq A$.

To compute these probabilities, we use the fact that $A$ and $A^*$ are preimages:
\begin{equation}
\begin{aligned}
	filter~(\in A)~xs
		&\ = \ filter~(\in preimage~(g~X)~B)~xs
\\
		&\ = \ filter~(\fun{a}{g~X~a \in B})~xs
\\
		&\ = \ filter~(\fun{a}{f~a \in B})~xs
\end{aligned}
\end{equation}
Here, $f : X \botto Y$ is the interpretation of the program as a bottom arrow computation.
Thus,
\begin{equation}
	Pr[A^* | A] \ \approx \ \frac{filter~(\fun{a}{f~a \in B^* \i B})~xs}{filter~(\fun{a}{f~a \in B})~xs}
\end{equation}
converges to the probability of $B^*$ given $B$ as the number of samples $xs$ from the rectangular cover $A'$ increases.

The rectangular cover $A'$ does not have to be enumerated: the rectangles that comprise it can be sampled.
Sampling from each sampled rectangle yields $xs$.

The preceeding discussion does not treat 

$P~(image~(fst~\arrowcomp~fst)~A)$

about computations of type $X \botto Y$, $X \mapto Y$ and $X \preto' Y$.
Sampling correctly from preimages computed by $X \ppreto' Y$ 


For programs interpreted using $\meaningofconv{\cdot}_\pbot$ and $\meaningofconv{\cdot}\ppre$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Unrelated Work}

XXX: todo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

XXX: todo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions and Future Work}

XXX: todo

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\appendix
%\section{Appendix Title}
%This is the text of the appendix, if you need one.

%\acks
%Acknowledgments, if needed.

\bibliographystyle{abbrvnat}
\bibliography{plt}

\end{document}
