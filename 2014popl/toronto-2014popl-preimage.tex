%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\input{local-macros.tex}

\mathversion{sans}

\newcommand{\arrow}{\rightsquigarrow}

\newcommand{\restrict}[1]{\lvert_{#1}}
\newcommand{\pto}{\rightharpoonup}
\newcommand{\Univ}{\mathbb{U}}
\newcommand{\Un}{\mathcal{U}}

\newcommand{\arrowlift}{\ensuremath{lift}}
\newcommand{\arrowarr}{\ensuremath{arr}}
\newcommand{\arrowcomp}{\ensuremath{{>}\mspace{-6mu}{>}\mspace{-6mu}{>}}}
\newcommand{\arrowpair}{\ensuremath{\mathit{\&\mspace{-7.5mu}\&\mspace{-7.5mu}\&}}}
\newcommand{\arrowif}{\ensuremath{ifte}}
\newcommand{\arrowlazy}{\ensuremath{lazy}}
\newcommand{\arrowapp}{\ensuremath{app}}
\newcommand{\arrowrun}{\ensuremath{run}}
\newcommand{\arrowget}{\ensuremath{get}}
\newcommand{\arrowerror}{\ensuremath{error}}
\newcommand{\arrowtrans}{\ensuremath{\eta}}

\newcommand{\gen}{_\mathrm{a}}
\newcommand{\genb}{_\mathrm{b}}
\newcommand{\genc}{_\mathrm{a^{\mspace{-2mu}*}}}
\newcommand{\gend}{_\mathrm{b^{\mspace{-2mu}*}}}

\DeclareMathOperator{\botto}{\arrow_{\mspace{-3mu}\bot}}
\newcommand{\arrbot}{\arrowarr_{\mspace{-3mu}\bot}}
\newcommand{\compbot}{\arrowcomp_{\mspace{-5mu}\bot}}
\newcommand{\pairbot}{\arrowpair_{\mspace{-3mu}\bot}}
\newcommand{\ifbot}{\arrowif_{\mspace{-2mu}\bot}}
\newcommand{\lazybot}{\arrowlazy_{\mspace{-2mu}\bot}}

\newcommand{\map}{_\mathrm{map}}
\DeclareMathOperator{\mapto}{\arrow_{\mspace{-21mu}\map}}
\newcommand{\liftmap}{\arrowlift\map}
\newcommand{\arrmap}{\arrowarr\map}
\newcommand{\compmap}{\arrowcomp\map}
\newcommand{\pairmap}{\arrowpair\map}
\newcommand{\ifmap}{\arrowif\map}
\newcommand{\lazymap}{\arrowlazy\map}

\newcommand{\pre}{_\mathrm{pre}}
\DeclareMathOperator{\preto}{\arrow_{\mspace{-19mu}\pre}}
\newcommand{\liftpre}{\arrowlift\pre}
\newcommand{\arrpre}{\arrowarr\pre}
\newcommand{\comppre}{\arrowcomp\pre}
\newcommand{\pairpre}{\arrowpair\pre}
\newcommand{\ifpre}{\arrowif\pre}
\newcommand{\lazypre}{\arrowlazy\pre}

\newcommand{\pbot}{{\bot^{\mspace{-4mu}*}}}
\DeclareMathOperator{\pbotto}{\arrow_{\mspace{-3mu}\pbot}}
\newcommand{\arrpbot}{\arrowarr_{\mspace{-3mu}\pbot}}
\newcommand{\comppbot}{\arrowcomp_{\mspace{-5mu}\pbot}}
\newcommand{\pairpbot}{\arrowpair_{\mspace{-3mu}\pbot}}
\newcommand{\ifpbot}{\arrowif_{\mspace{-2mu}\pbot}}
\newcommand{\lazypbot}{\arrowlazy_{\mspace{-2mu}\pbot}}

\newcommand{\pmap}{_\mathrm{map^{\mspace{-2mu}*}}}
\DeclareMathOperator{\pmapto}{\arrow_{\mspace{-22mu}_{\mathrm{map*}}}}
\newcommand{\liftpmap}{\arrowlift\pmap}
\newcommand{\arrpmap}{\arrowarr\pmap}
\newcommand{\comppmap}{\arrowcomp\pmap}
\newcommand{\pairpmap}{\arrowpair\pmap}
\newcommand{\ifpmap}{\arrowif\pmap}
\newcommand{\lazypmap}{\arrowlazy\pmap}

\newcommand{\ppre}{_\mathrm{pre^{\mspace{-2mu}*}}}
\DeclareMathOperator{\ppreto}{\arrow_{\mspace{-19mu}_{\mathrm{pre*}}}}
\newcommand{\liftppre}{\arrowlift\ppre}
\newcommand{\arrppre}{\arrowarr\ppre}
\newcommand{\compppre}{\arrowcomp\ppre}
\newcommand{\pairppre}{\arrowpair\ppre}
\newcommand{\ifppre}{\arrowif\ppre}
\newcommand{\lazyppre}{\arrowlazy\ppre}

\newcommand{\prepto}{\pto_{\mspace{-19mu}\pre}}


\begin{document}

\conferenceinfo{POPL '14}{January 22-24, 2014, San Diego, CA, USA}
\copyrightyear{2014}
\copyrightdata{[to be supplied]} 

%\titlebanner{banner above paper title}        % These are ignored unless
%\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Running Probabilistic Programs Backward}
%\subtitle{Subtitle Text, if any}

\authorinfo{Neil Toronto \and Jay McCarthy}
           {PLT @ Brigham Young University}
           {ntoronto@racket-lang.org \and jay@cs.byu.edu}
%\authorinfo{Chris Grant}
%           {Brigham Young University}
%           {grant@math.byu.edu}
\maketitle

\begin{abstract}
Problem: doesn't exist: Turing-equivalent, probabilistic programming language with a semantics and implementation that supports any kind of probability measure and---critically---can do probabilistic conditioning

Why problem:

Solution:

Why solution:
\end{abstract}

\category{XXX-CR-number}{XXX-subcategory}{XXX-third-level}

\terms
XXX, XXX

\keywords
XXX, XXX

\section{Introduction}



XXX: motivate conditional probability

XXX: densities fail:
\begin{enumerate}
	\item infinite or variable-length data
	\item discontinuities in random variables
	\item nonlinear conditions
\end{enumerate}

XXX: measures instead of densities, preimage measure

Purely functional languages do not allow effects, including randomness.
(Of course, to be Turing-equivalent, ``pure'' languages usually allow divergence, which is an applicative effect.)
Programmers must write probabilistic programs as functions from a random source to outputs.
Monads and other categorical classes such as idioms and arrows can make doing so easier.
The strength of this approach lies in how it factors each probabilistic program into an effectful part and a pure part.

It seems this approach should make interpreting probabilistic programs in measure theory easy.
For a probabilistic program $f : X \tto Y$, the probability measure over output sets $B \subseteq Y$ should be defined by preimages of $B$ under $f$ and the probability measure over $X$.
Unfortunately, it is difficult to make a compositional semantics from this simple-sounding idea, for the following reasons.
\begin{itemize}
	\item ``Preimage under $f$'' makes sense only if $f$ has an observable domain of definition. Lambdas do not.
	\item Probability measures can be defined only for \emph{measurable} subsets of $X$ and of $Y$. Thus, $f$ is constrained: preimages of measurable subsets of $Y$ must be measurable subsets of $X$. Proving the conditions under which this is true is difficult, especially when $f$ may diverge.
	\item Probability measures cannot be defined for arbitrary function spaces without imposing difficult restrictions.
\end{itemize}
Implementing a language based on such a semantics is complicated by these facts:
\begin{itemize}
	\item Contemporary mathematics is unlike any implementation's host language.
	\item It requires running Turing-equivalent programs backward, efficiently, on possibly uncountable sets of outputs.
\end{itemize}

XXX: rewrite the following

In this paper, we
\begin{enumerate}
	\item Define the \emph{bottom arrow}, type $X \botto Y$, as a compilation target for first-order functions that may raise errors.
	\item Derive the \emph{mapping arrow} from the bottom arrow, type $X \mapto Y$. Prove that its instances return extensional functions, or mappings, that compute the same values as corresponding bottom arrow computations.
	\item Derive the \emph{preimage arrow} from the mapping arrow, type $X \preto Y$. Prove that its instances compute preimages under corresponding mapping (or bottom) arrow instances.
	\item Define an arrow transformer via a natural transformation $\arrowtrans$ to thread random stores and avoid divergence.
	\item Define a computable approximation of the transformed preimage arrow and report on its implementation.
\end{enumerate}

Most of our correctness theorems can be described in terms of the following commutative diagram:
\begin{equation}
\begin{CD}
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y
\end{CD}
\label{eqn:roadmap-diagram}
\end{equation}
We prove that all the functions between arrow types in~\eqref{eqn:roadmap-diagram} are homomorphisms, which implies that $X \preto Y$ and $X \ppreto Y$ instances compute preimages correctly.

XXX: something about how we know the approximations are correct


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Operational Metalanguage}

From here on, significant terms are introduced in \keyword{bold}, and significant terms we invent are introduced in \mykeyword{bold italics}.

We write all of the programs in this paper in \lzfclang~\cite{cit:toronto-2012flops-lzfc}, an untyped, call-by-value lambda calculus designed for deriving implementable programs from contemporary mathematics.

Contemporary mathematics is generally done in \keyword{ZFC}: \keyword{Zermelo-Fraenkel} set theory extended with the axiom of \keyword{Choice} (equivalently unique \keyword{Cardinality}).
ZFC has only first-order functions and no general recursion, which makes implementing a language defined by a transformation into contemporary mathematics quite difficult.
The problem is exacerbated if implementing the language requires approximation.
Targeting \lzfclang instead allows creating a precise mathematical specification and deriving an approximating implementation without changing languages.

In \lzfclang, essentially every set is a value, as well as every lambda and every set of lambdas.
All operations, including operations on infinite sets, are assumed to complete instantly if they terminate.\footnote{An
example of a nonterminating \lzfclang function is one that attempts to decide whether other \lzfclang programs halt.}

Almost everything definable in contemporary mathematics can be formally defined by a finite \lzfclang program, except objects that most mathematicians would agree are nonconstructive.
More precisely, any object that \emph{must} be defined by a statement of existence and uniqueness without giving a bounding set is not definable by a \emph{finite} \lzfclang program.

Because \lzfclang includes an inner model of ZFC, essentially every contemporary theorem applies to \lzfclang's set values without alteration.
Further, proofs about \lzfclang's set values apply to contemporary mathematical objects.\footnote{Assuming the existence of an inaccessible cardinal.}

In \lzfclang, algebraic data structures are encoded as sets; e.g. a \mykeyword{primitive ordered pair} of $x$ and $y$ is $\set{\set{x},\set{x,y}}$.
Only the \emph{existence} of encodings into sets is important, as it means data structures inherit a defining characteristic of sets: strictness.
More precisely, the lengths of paths to data structure leaves is unbounded, but each path must be finite.
Less precisely, data may be ``infinitely wide'' (such as $\Re$) but not ``infinitely tall'' (such as infinite trees and lists).

We assume data structures, including pairs, are encoded as \emph{primitive} ordered pairs with the first element a unique tag, so that they can be distinguished by checking tags.
Accessors such as $fst$ and $snd$ are trivial to define.

\lzfclang is untyped so its users can define an auxiliary type system that best suits their application area.
For this work, we use an informal, manually checked, polymorphic type system characterized by these rules:
\begin{itemize}
	\item A free lowercase type variable is universally quantified.
	\item A free uppercase type variable is a set.
	\item A set denotes a member of that set.
	\item $x \tto y$ denotes a partial function.
	\item $\pair{x,y}$ denotes a pair of values with types $x$ and $y$.
	\item $Set~x$ denotes a set with members of type $x$.
\end{itemize}
The type $Set~X$ denotes the same values as the powerset $\powerset~X$, or \emph{subsets} of $X$.
Similarly, the type $\pair{X,Y}$ denotes the same values as the product set $X \times Y$.

We write \lzfclang programs in heavily sugared $\lambda$-calculus syntax, with an $if$ expression and these additional primitives:
\begin{equation}
\begin{aligned}
	\begin{aligned}
		true &: Bool \\
		false &: Bool \\
		\emptyset &: Set~x \\
		\omega &: Ord \\
		take &: Set~x \tto x \\
	\end{aligned}
	&\tab
	\begin{aligned}
		(\in) &: x \tto Set~x \tto Bool \\
		\powerset &: Set~x \tto Set~(Set~x) \\
		\U &: Set~(Set~x) \tto Set~x \\
		image &: (x \tto y) \tto Set~x \tto Set~y \\
		|\cdot| &: Set~x \tto Ord \\
	\end{aligned} \\
\end{aligned}
\label{eqn:lzfc-prims}
\end{equation}
Shortly, $\emptyset$ is the empty set, $\omega$ is the cardinality of the natural numbers, $take~\set{x}$ reduces to $x$ and diverges for non-singleton sets, $x \in A$ decides membership, $\powerset~A$ reduces to the set of subsets of $A$, $\U\A$ reduces to the union of the sets in $\A$, $image~f~A$ applies $f$ to each member of $A$ and reduces to the set of results, and $|A|$ reduces to the cardinality of $A$.

We assume literal set notation such as $\set{0,1,2}$ is already defined in terms of the set primitives.

We import contemporary theorems as lemmas.

\subsection{Internal and External Equality}

Set theory extends first-order logic with an axiom that defines equality to be extensional, and with axioms that ensure the existence of sets in the domain of discourse.
\lzfclang is defined the same way as any other operational $\lambda$-calculus: by (conservatively) extending the domain of discourse with expressions and defining a reduction relation.

While \lzfclang does not have an equality primitive, set theory's extensional equality can be recovered internally using $(\in)$.
\emph{Internal} extensional equality is defined by
\begin{equation}
	x = y \ := \ x \in \set{y}
\end{equation}
which means
\begin{equation}
	(=) \ := \ \fun{x}\fun{y}{x \in \set{y}}
\end{equation}
Thus, $1 = 1$ reduces to $1 \in \set{1}$, which reduces to $true$.\footnote{Technically, \lzfclang has a big-step semantics, and
the derivation tree for $1 = 1$ contains the derivation tree for $1 \in \set{1}$.}
Because of the particular way \lzfclang's lambda terms are defined, for two lambda terms $f$ and $g$, $f = g$ reduces to $true$ when $f$ and $g$ are structurally identical modulo renaming.
For example, $(\fun{x}{x}) = (\fun{y}{y})$ reduces to $true$, but $(\fun{x}{2}) = (\fun{x}{1+1})$ reduces to $false$.

We understand any \lzfclang term $\mathit{e}$ used as a truth statement as shorthand for ``$\mathit{e}$ reduces to $true$.''
Therefore, while the terms $(\fun{x}{x})~1$ and $1$ are (externally, extensionally) unequal, we can say that $(\fun{x}{x})~1 = 1$.

Any truth statement $\mathit{e}$ implies that $\mathit{e}$ converges.
In particular, the truth statement $\mathit{e}_1 = \mathit{e}_2$ implies that both $\mathit{e}_1$ and $\mathit{e}_2$ converge.
However, we often want to say that $\mathit{e}_1$ and $\mathit{e}_1$ are equivalent when they both diverge.
In these cases, we use a slightly weaker equivalence.

\begin{definition}[observational equivalence]
Two \lzfclang terms $\mathit{e_1}$ and $\mathit{e_2}$ are \keyword{observationally equivalent}, written $\mathit{e_1} \equiv \mathit{e_2}$, when $\mathit{e_1} = \mathit{e_2}$ or both $\mathit{e_1}$ and $\mathit{e_2}$ diverge.
\end{definition}

It might seem helpful to introduce even coarser notions of equivalence, such as applicative or logical bisimilarity.
However, we do not want internal equality and external equivalence to differ too much, and we want the flexibility of extending ``$\equiv$'' with type-specific rules.

\subsection{Additional Functions and Forms}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&domain : (X \pto Y) \tto Set~X \\
		&domain \ := \ image~fst \\
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&range : (X \pto Y) \tto Set~Y \\
		&range \ := \ image~snd
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&preimage : (X \pto Y) \tto Set~Y \tto Set~X \\
		&preimage~f~B \ :=\ \setb{a \in domain~f}{f~a \in B}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&restrict : (X \pto Y) \tto Set~X \tto (X \pto Y) \\
		&restrict~f~A \ := \ \fun{a \in (A \i domain~f)}{f~a}
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\map : (X \pto Y_1) \tto (X \pto Y_2) \tto (X \pto Y_1 \times Y_2) \\
		&\pair{g_1,g_2}\map \ := \ 
			\lzfclet{
				A & (domain~g_1) \i (domain~g_2)
			}{\fun{a \in A}{\pair{g_1~a,g_2~a}}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\map) : (Y \pto Z) \tto (X \pto Y) \tto (X \pto Z) \\
		&g_2 \circ\map g_1 \ := \ 
			\lzfclet{
				A & preimage~g_1~(domain~g_2)
			}{\fun{a \in A}{g_2~(g_1~a)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\map) : (X \pto Y) \tto (X \pto Y) \tto (X \pto Y) \\
		&g_1 \uplus\map g_2 \ := \ 
			\lzfclet{
				A & (domain~g_1) \uplus (domain~g_2)
			}{\fun{a \in A}{if~(a \in domain~g_1)~(g_1~a)~(g_2~a)}}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Operations on mappings.}
\label{fig:mapping-defs}
\end{figure*}

We assume a desugaring pass over \lzfclang expressions, which automatically curries (including for the two-argument primitives $(\in)$ and $image$), and interprets special binding forms such as indexed unions $\U_{\mathit{x} \in \mathit{e_A}}\mathit{e}$, destructuring binds as in $swap~\pair{x,y} := \pair{y,x}$, and comprehensions like $\setb{x \in A}{x \in B}$.
(We may be rather informal with the latter two binding forms when the meaning is clear.)
We assume we have logical operators, bounded quantifiers (unbounded quantifiers are not \lzfclang-definable), and typical set operations.

A less typical set operation we use is disjoint union:
\begin{equation}
\begin{aligned}
	&(\uplus) : Set~x \tto Set~x \tto Set~x \\
	&A \uplus B \ := \ if~(A \i B = \emptyset)~(A \u B)~(take~\emptyset)
\end{aligned}
\end{equation}
$A \uplus B$ diverges when $A$ and $B$ overlap.

In set theory, functions are encoded as sets of input-output pairs.
The increment function for the natural numbers, for example, is $\set{\pair{0,1},\pair{1,2},\pair{2,3},...}$.
To distinguish these huge tables from lambdas, we call them \mykeyword{mappings}, and use the word \keyword{function} to mean a lambda or mapping.

Syntax for defining unnamed mappings is defined by
\begin{align}
	&\fun{\mathit{x_a} \in \mathit{e_A}}{\mathit{e_b}} \ :\equiv\ mapping~(\fun{\mathit{x_a}}\mathit{e_b})~\mathit{e_A} \\
\nonumber\\[-6pt]
	&\begin{aligned}
		&mapping : (X \tto Y) \tto Set~X \tto (X \pto Y) \\
		&mapping~f~A \ := \ image~(\fun{a}{\pair{a,f~a}})~A
	\end{aligned}
\end{align}
For convenience, as with lambdas, we use adjacency (e.g. $(f~x)$) to apply mappings.

Figure~\ref{fig:mapping-defs} defines a few common operations on mappings: $domain$, $range$, $preimage$, $restrict$, pairing, composition, and disjoint union.
The latter three are particularly important in the preimage arrow's derivation, and $preimage$ is critical in measure theory's account of probability.
For symmetry with partial functions $x \tto y$, they are defined on $X \pto Y$, where $X \pto Y$ is the set of all partial mappings from any domain set $X$ to any codomain set $Y$.

The set $X \to Y$ contains all the \emph{total} mappings from $X$ to $Y$.
We use total mappings as possibly infinite vectors, with application for indexing.
Projections are produced by
\begin{equation}
\begin{aligned}
	&\pi : J \tto (J \to X) \tto X \\
	&\pi~j~f \ := \ f~j
\end{aligned}
\end{equation}
which is useful in combinators, when $f$ is unnamed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Arrows and First-Order Semantics}

XXX: really short arrow intro (XXX: cite Hughes, Lindley et al)

\subsection{Alternative Arrow Definitions}

For every arrow $a$ in this paper, we do not give a typical minimal definition.
Instead of $first\gen$, we define $(\arrowpair\gen)$---typically called \keyword{fanout}, but its use will be clearer if we call it \keyword{pairing}---which applies two functions to an input and returns the pair of their outputs.
Though $first\gen$ may be defined in terms of $(\arrowpair\gen)$ and vice-versa~\cite{cit:hughes-2005afp-arrows}, we give $(\arrowpair\gen)$ definitions in this paper because the applicable contemporary theorems are in terms of pairing functions.

One way to strengthen an arrow $a$ is to define an additional combinator $left\gen$, which can be used to choose an arrow computation based on the result of another.
Again, we define a different combinator, $\arrowif\gen$, to make it easier to apply contemporary theorems.

In a nonstrict $\lambda$-calculus, simply defining a choice combinator allows writing recursive functions using nothing but arrow combinators and lifted, pure functions.
However, any strict $\lambda$-calculus (such as \lzfclang) requires an extra combinator to defer computations in conditional branches.

For example, suppose we define the \keyword{function arrow} with choice, by defining
\begin{equation}
\begin{aligned}
	\arrowarr~f &\ := \ f \\
	(f_1~\arrowcomp~f_2)~a &\ := \ f_2~(f_1~a) \\
	(f_1~\arrowpair~f_2)~a &\ := \ \pair{f_1~a,f_2,a} \\
	\arrowif~f_1~f_2~f_3~a &\ := \ if~(f_1~a)~(f_2~a)~(f_3~a) \\
\end{aligned}
\label{eqn:function-arrow}
\end{equation}
and try to define the following recursive function:
\begin{equation}
	halt!on!true \ := \ \arrowif~(\arrowarr~id)~(\arrowarr~id)~halt!on!true
\end{equation}
The defining expression diverges in a strict $\lambda$-calculus.
In a nonstrict $\lambda$-calculus, it diverges only when applied to $false$.

Using $\arrowlazy~f~a := f~0~a$, which receives thunks and returns arrow computations, we can write $halt!on!true$ as
\begin{equation}
\begin{aligned}
	&halt!on!true \ := \ 
	\arrowif~(\arrowarr~id)~(\arrowarr~id)~(\arrowlazy~\fun{0}{halt!on!true})
\end{aligned}
\end{equation}
which diverges only when applied to $false$ in any $\lambda$-calculus.

\begin{definition}[arrow with choice]A binary type constructor $(\arrow\gen)$ and the combinators
\begin{equation}
\begin{aligned}
	\arrowarr\gen &: (x \tto y) \tto (x \arrow\gen y)
\\
	(\arrowcomp\gen) &: (x \arrow\gen y) \tto (y \arrow\gen z) \tto (x \arrow\gen z)
\\
	(\arrowpair\gen) &: (x \arrow\gen y) \tto (x \arrow\gen z) \tto (x \arrow\gen \pair{y,z})
\end{aligned}
\label{eqn:arrow-combinators}
\end{equation}
define an \keyword{arrow} if certain monoid, homomorphism, and structural laws hold.
The additional combinators
\begin{equation}
\begin{aligned}
	\arrowif\gen &: (x \arrow\gen Bool) \tto (x \arrow\gen y) \tto (x \arrow\gen y) \tto (x \arrow\gen y)
\\
	\arrowlazy\gen &: (1 \tto (x \arrow\gen y)) \tto (x \arrow\gen y)
\end{aligned}
\end{equation}
define an \keyword{arrow with choice} if certain additional homomorphism and structural laws hold.
\end{definition}

From here on, as all of our arrows are arrows with choice, we simply call them arrows.

The necessary homomorphism laws ensure that $\arrowarr\gen$ distributes over function arrow combinators.
These laws can be put in terms of more general homomorphism properties that deal with distributing an arrow-to-arrow lift, which we use extensively to prove correctness.

\begin{definition}[arrow homomorphism]
A function $lift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an \mykeyword{arrow homomorphism} from arrow $\mathrm{a}$ to arrow $\mathrm{b}$ if the following distributive laws hold for appropriately typed $f$, $f_1$, $f_2$ and $f_3$:
\begin{align}
	lift\genb~(\arrowarr\gen~f) &\ \equiv \ \arrowarr\genb~f
	\label{eqn:lift-distributes-over-arr}
\\
	lift\genb~(f_1~\arrowcomp\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowcomp\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-comp}
\\
	lift\genb~(f_1~\arrowpair\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowpair\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-pair}
\\
	\arrowlift\genb~(\arrowif\gen~f_1~f_2~f_3) &\ \equiv \ 
		\arrowif\genb~(lift\genb~f_1)~(lift\genb~f_2)~(lift\genb~f_3)
	\label{eqn:lift-distributes-over-if}
\\
	\arrowlift\genb~(\arrowlazy\gen~f) &\ \equiv \
		\arrowlazy\genb~\fun{0}{\arrowlift\genb~(f~0)}
	\label{eqn:lift-distributes-over-lazy}
\end{align}
\label{def:arrow-homomorphism}
\end{definition}

The homomorphism laws state that $\arrowarr\gen$ must be a homomorphism from the function arrow to arrow $a$.

The monoid and structural arrow laws play little role in our semantics or its correctness.
For the arrows we define, then, we elide the proofs of these arrow laws, and concentrate on homomorphisms.

XXX: actually, need to prove some of them, to prove that the natural transformation for the applicative store-passing arrow transformer is a homomorphism

\subsection{First-Order Let-Calculus Semantics}

Figure~\ref{fig:semantic-function} shows a transformation $\meaningof{\cdot}\gen$ from a first-order let-calculus to arrow computations for any arrow $a$.

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	\meaningof{\mathit{x} := \mathit{e};\ \cdots\ ; \mathit{e_{body}}}\gen &\ :\equiv\
		\mathit{x} := \meaningof{\mathit{e}}\gen;\ \cdots \ ; \meaningof{\mathit{e_{body}}}\gen \\
	\meaningof{\mathit{x}~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\pair{\mathit{e},\pair{}}}\gen~\arrowcomp\gen~\mathit{x}
\\
	\meaningof{let~\mathit{e}~\mathit{e_{body}}}\gen &\ :\equiv\ 
		(\meaningof{\mathit{e}}\gen~\arrowpair\gen~\arrowarr\gen~id)~
			\arrowcomp\gen~
		\meaningof{\mathit{e_{body}}}\gen
\\
	\meaningof{env~0}\gen &\ :\equiv\ \arrowarr\gen~fst
\\
	\meaningof{env~(\mathit{n}+1)}\gen &\ :\equiv\ \arrowarr\gen~snd~\arrowcomp\gen~\meaningof{env~\mathit{n}}\gen
\end{aligned}
&\tab
\begin{aligned}[t]
	\meaningof{\mathit{v}}\gen &\ :\equiv\ \arrowarr\gen~\fun{\underline{\ \ }}{\mathit{v}}
\\
	\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen &\ :\equiv\
		\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen
\\
	\meaningof{fst~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~fst
\\
	\meaningof{snd~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~snd
\\
	\meaningof{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\gen &\ :\equiv\
		\arrowif\gen~
			\meaningof{\mathit{e_c}}\gen~
			(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_t}}\gen})~
			(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_f}}\gen})
\end{aligned}
\end{align*}
\hrule
\caption{Transformation from a let-calculus with first-order definitions and De-Bruijn-indexed bindings to computations in arrow $\mathrm{a}$.
%The type of a transformed expression is $1 \arrow\gen X$, or an arrow from the empty stack $\gamma = 0$ to a value of type $X$.
}
\label{fig:semantic-function}
\end{figure*}

A program is a sequence of definition statements followed by a final expression.
$\meaningof{\cdot}\gen$ compositionally transforms each defining expression and the final expression into arrow computations.
Functions are named, but local variables and arguments are not.
Instead, variables are accessed by their De Bruijn index, where index $0$ refers to the innermost binding.

Perhaps unsurprisingly, the interpretation acts like a stack machine.
The final expression has type $\pair{} \arrow\gen y$, where $y$ is the type of the program's value, and $\pair{}$ denotes an empty list.
Let-bindings push values onto the stack.
First-order functions have type $\pair{x,\pair{}} \arrow\gen y$ where $x$ is the argument type.
Application sends a stack containing just $x$.

It is not difficult to allow named bindings, but it is better to do so in a separate semantic function.
Baking such support into $\meaningof{\cdot}\gen$ would complicate the simple proof of the following theorem, which underlies most of our semantic correctness claims.

\begin{theorem}[homomorphisms distribute over programs]
Let $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ be an arrow homomorphism.
For all programs $\mathit{e}$, $\meaningof{\mathit{e}}\genb \equiv \arrowlift\genb~\meaningof{\mathit{e}}\gen$.
\label{thm:homomorphism-implies-correct}
\end{theorem}
\begin{proof}
By structural induction on program terms.

Bases cases proceed by expansion and using $\arrowlift\genb \circ \arrowarr\gen \equiv \arrowarr\genb$~\eqref{eqn:lift-distributes-over-arr}. For example, for constants:
\begin{align*}
	\arrowlift\genb~\meaningof{\mathit{v}}\gen
		&\ \equiv\ \arrowlift\genb~(\arrowarr\gen~\fun{\underline{\ \ }}{v}) \\
		&\ \equiv\ \arrowarr\genb~\fun{\underline{\ \ }}{v} \\
		&\ \equiv\ \meaningof{\mathit{v}}\genb
\end{align*}
Inductive cases proceed by expansion, applying one or more distributive laws~(\ref{eqn:lift-distributes-over-comp}--\ref{eqn:lift-distributes-over-lazy}), and applying the inductive hypothesis on subterms.
For example, for pairing:
\begin{align*}
	\arrowlift\genb~\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen
		&\ \equiv\ \arrowlift\genb~(\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen) \\
		&\ \equiv\ (\arrowlift\genb~\meaningof{\mathit{e}_1}\gen)~\arrowpair\genb~(\arrowlift\genb~\meaningof{\mathit{e}_2}\gen) \\
		&\ \equiv\ \meaningof{\mathit{e}_1}\genb~\arrowpair\genb~\meaningof{\mathit{e}_2}\genb \\
		&\ \equiv\ \meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\genb
\end{align*}
It is not hard to check the remaining cases.
\end{proof}

If we assume that $\arrowlift\genb$ defines correct behavior for arrow $b$ in terms of arrow $a$, and prove that $\arrowlift\genb$ is a homomorphism, then by Theorem~\ref{thm:homomorphism-implies-correct}, $\meaningof{\cdot}\genb$ is correct.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bottom and Mapping Arrows}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&x \botto y \ ::= \ x \tto y_\bot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrbot : (x \tto y) \tto (x \botto y) \\
		&\arrbot~f \ := \ f
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compbot) : (x \botto y) \tto (y \botto z) \tto (x \botto z) \\
		&(f_1~\compbot~f_2)~a \ := \ if~(f_1~a = \bot)~\bot~(f_2~(f_1~a))
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairbot) : (x \botto {y_1}) \tto (x \botto {y_2}) \tto (x \botto \pair{y_1,y_2}) \\
		&(f_1~\pairbot~f_2)~a \ := \ if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifbot : (x \botto Bool) \tto (x \botto y) \tto (x \botto y) \tto (x \botto y) \\
		&\ifbot~f_1~f_2~f_3~a \ := \
			\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ else & \bot}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazybot : (1 \tto (x \botto y)) \tto (x \botto y) \\
		&\lazybot~f~a \ := \ f~0~a
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Bottom arrow definitions.}
\label{fig:bottom-arrow-defs}
\end{figure*}

We are certain that the preimage arrow correctly computes preimages under some function $f$ because we ultimately \emph{derive} it from a simpler arrow used to construct $f$.

One obvious candidate for the simpler arrow is the function arrow, defined in~\eqref{eqn:function-arrow}.
However, we will need to explicitly handle divergence as an error value, so we need a slightly more complicated arrow for which running computations may raise an error.

Figure~\ref{fig:bottom-arrow-defs} defines the \mykeyword{bottom arrow}.
Its computations are of type $x \botto y ::= x \tto y_\bot$, where the inhabitants of $y_\bot$ are the error value $\bot$ as well as the inhabitants of $y$. The type $Bool_\bot$, for example, denotes the members of $Bool \u \set{\bot}$.

\begin{theorem}
$\arrbot$, $(\pairbot)$, $(\compbot)$, $\ifbot$ and $\lazybot$ define an arrow.
\end{theorem}
\begin{proof}
The bottom arrow is the Maybe monad's Kleisli arrow with $Nothing = \bot$.
\end{proof}

\subsection{Deriving the Mapping Arrow}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrmap : (X \tto Y) \tto (X \mapto Y) \\
		&\arrmap \ := \ \liftmap \circ \arrbot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compmap) : (X \mapto Y) \tto (Y \mapto Z) \tto (X \mapto Z) \\
		&(g_1~\compmap~g_2)~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(range~g_1')
			}{g_2' \circ\map g_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairmap) : (X \mapto Y_1) \tto (X \mapto Y_2) \tto (X \mapto \pair{Y_1,Y_2}) \\
		&(g_1~\pairmap~g_2)~A \ := \ \pair{g_1~A,g_2~A}\map
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifmap : (X \mapto Bool) \tto (X \mapto Y) \tto (X \mapto Y) \tto (X \mapto Y) \\
		&\ifmap~g_1~g_2~g_3~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(preimage~g_1'~\set{true}) \\
				g_3' & g_3~(preimage~g_1'~\set{false})
			}{g_2' \uplus\map g_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazymap : (1 \tto (X \mapto Y)) \tto (X \mapto Y) \\
		&\lazymap~g~A \ := \ if~(A = \emptyset)~\emptyset~(g~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A \ := \ \setb{\pair{a,b} \in mapping~f~A}{b \neq \bot}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Mapping arrow definitions.}
\label{fig:mapping-arrow-defs}
\end{figure*}

The contemporary theorems we need are about mappings, not about lambdas that may raise errors.
As in intermediate step, then, we need an arrow whose computations produce mappings or are mappings themselves.

It is tempting to try to make the mapping arrow's computations mapping-valued; i.e. define it using $X \mapto Y ::= X \pto Y$, with $f_1~\compmap~f_2 := f_2 \circ\map f_1$ and $f_1~\pairmap~f_2 := \pair{f_1,f_2}\map$.
Unfortunately, we couldn't define $\arrmap : (X \tto Y) \tto (X \pto Y)$: to compute a mapping, we need a domain, but a lambda's domain is unobservable.

To parameterize mapping arrow computations on a domain, we define the \mykeyword{mapping arrow} computation type as
\begin{equation}
	X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
\end{equation}
Notice that $\bot$ is absent in $Set~X \tto (X \pto Y)$.
This will make it easier to exclude diverging inputs further on.
The absence of $\bot$ and the fact that the type parameters are sets will make it easier to apply contemporary theorems, which know nothing of error values and lambda types.

Further on, we will need every computation $g : X \mapto Y$ to meet the \mykeyword{mapping arrow restriction law}:
for all $A \subseteq X$ and $A' \subseteq A$ for which $g~A$ converges,
\begin{equation}
	g~A' \ = \ restrict~(g~A)~A'
\label{eqn:mapping-arrow-restriction-law}
\end{equation}
Roughly, $g$ must act as if it returns restricted mappings.

To use Theorem~\ref{thm:homomorphism-implies-correct} to prove that programs interpreted using $\meaningof{\cdot}\map$ behave correctly, we need to define correctness using a lift from the bottom arrow to the mapping arrow.
It is helpful to have a standalone function $domain_\bot$ that computes the subset of $A$ on which $f$ does not return $\bot$.
We define that first, and then define $\liftmap$ in terms of it:
\begin{align}
	&\begin{aligned}
		&domain_\bot : (X \botto Y) \tto Set~X \tto Set~X \\
		&domain_\bot~f~A \ := \ \setb{a \in A}{f~a \neq \bot}
	\end{aligned} \\
\nonumber \\[-6pt]
	&\begin{aligned}
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A \ := \ mapping~f~(domain_\bot~f~A)
	\end{aligned}
\end{align}
So $\liftmap~f~A$ is like $mapping~f~A$, but without inputs that produce errors---a good notion of correctness.

If $\liftmap$ is to be a homomorphism, mapping arrow computation equivalence needs to be more extensional than observational equivalence.

\begin{definition}[mapping arrow equivalence]
Two mapping arrow computations $g_1 : X \mapto Y$ and $g_2 : X \mapto Y$ are equivalent, or $g_1 \equiv g_2$, when $g_1~A \equiv g_2~A$ for all $A \subseteq X$.
\end{definition}

Clearly $\arrowarr\genb := lift\genb \circ \arrowarr\gen$ meets the first homomorphism identity~\eqref{eqn:lift-distributes-over-arr}, so we define $\arrmap$ as a composition.
The following subsections derive $(\pairmap)$, $(\compmap)$, $\ifmap$ and $\lazymap$ from their corresponding bottom arrow combinators, in a way that ensures $\liftmap$ is an arrow homomorphism.
Figure~\ref{fig:mapping-arrow-defs} contains the resulting definitions.

\subsubsection{Case: Pairing}

Starting with the left side of~\eqref{eqn:lift-distributes-over-pair}, we first expand definitions.
For any $f_1 : X \botto Y$, $f_2 : X \botto Z$, and $A \subseteq X$,
\begin{align*}
	&\liftmap~(f_1~\pairbot~f_2)~A
\\
	&\tab \equiv \ \liftmap~(\fun{a}{if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}})~A
\\
	&\tab \equiv \ 
		\lzfclet{
			f & \fun{a}{if~(f_1~a = \bot~or~f_2~a = \bot)~\bot~{\pair{f_1~a,f_2~a}}}
		}{mapping~f~(domain_\bot~f~A)}
\numberthis
\end{align*}
Next, we replace the definition of $A'$ with one that does not depend on $f$, and rewrite in terms of $\liftmap~f_1$ and $\liftmap~f_2$:
\begin{align*}
	&\liftmap~(f_1~\pairbot~f_2)~A
\\
	&\tab \equiv \ 
		\lzfclet{
			A_1 & (domain_\bot~f_1~A) \\
			A_2 & (domain_\bot~f_2~A) \\
			A' & A_1 \i A_2
		}{\fun{a \in A'}{\pair{f_1~a,f_2~a}}}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			g_2 & \liftmap~f_2~A \\
			A' & (domain~g_1) \i (domain~g_2)
		}{\fun{a \in A'}{\pair{g_1~a,g_2~a}}}
\\
	&\tab \equiv \ \pair{\liftmap~f_1~A, \liftmap~f_2~A}\map
\numberthis
\end{align*}
Substituting $g_1$ for $\liftmap~f_1$ and $g_2$ for $\liftmap~f_2$ gives a definition for $(\pairmap)$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsubsection{Case: Composition}

The derivation of $(\compmap)$ is similar to that of $(\pairmap)$ but a little more involved.

XXX: include it?

\subsubsection{Case: Conditional}

Starting with the left side of~\eqref{eqn:lift-distributes-over-if}, we expand definitions, and simplify $f$ by restricting it to a domain for which $f_1~a \neq \bot$:
\begin{align*}
	&\liftmap~(\ifbot~f_1~f_2~f_3)~A \\
	&\tab \equiv \ 
		\lzfclet{
			f & \fun{a}{\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ else & \bot}}
		}{mapping~f~(domain_\bot~f~A)} \\
	&\tab \equiv \ 
		\lzfclet{
			g_1 & mapping~f~A \\
			A_2 & preimage~g_1~\set{true} \\
			A_3 & preimage~g_1~\set{false} \\
			f & \fun{a}{if~(f_1~a)~(f_2~a)~(f_3~a)}
		}{mapping~f~(domain_\bot~f~(A_2 \uplus A_3))}
\numberthis
\end{align*}
We finish by converting bottom arrow computations to the mapping arrow and rewriting in terms of $(\uplus\map)$:
\begin{align*}
	&\liftmap~(\ifbot~f_1~f_2~f_3)~A \numberthis
\\
	&\tab \equiv \ 
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false}) \\
		A' & (domain~g_2) \uplus (domain~g_3)
	}{\fun{a \in A'}{if~(a \in domain~g_2)~(g_2~a)~(g_3~a)}}
\\
	&\tab \equiv \
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false})
	}{g_2 \uplus\map g_3}
\end{align*}
Substituting $g_1$ for $\liftmap~f_1$, $g_2$ for $\liftmap~f_2$, and $g_3$ for $\liftmap~f_3$ gives a definition for $\ifmap$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsubsection{Case: Laziness}

Starting with the left side of~\eqref{eqn:lift-distributes-over-lazy}, we first expand definitions:
\begin{align*}
	&\liftmap~(\lazybot~f)~A
\\
	&\tab \equiv \
		\lzfclet{
			A' & domain_\bot~(\fun{a}{f~0~a})~A
		}{mapping~(\fun{a}{f~0~a})~A'}
\end{align*}
\lzfclang does not have an $\eta$ rule (i.e. $\fun{\mathit{x}}{\mathit{e}~\mathit{x}} \not\equiv \mathit{e}$ because $\mathit{e}$ may diverge), but we can use weaker facts.
If $A \neq \emptyset$, then $domain_\bot~(\fun{a}{f~0~a})~A \equiv domain_\bot~(f~0)~A$.
Further, it diverges iff $f~0$ diverges, and iff $mapping~(f~0)~A'$ diverges.
Therefore, if $A \neq \emptyset$, we can replace $\fun{a}{f~0~a}$ with $f~0$.
If $A = \emptyset$, then $\liftmap~(\lazybot~f)~A = \emptyset$ (the empty mapping), so
\begin{align*}
	&\liftmap~(\lazybot~f)~A
\\
	&\tab \equiv \
		if~(A = \emptyset)~\emptyset~(mapping~(f~0)~(domain_\bot~(f~0)~A))
\\
	&\tab \equiv \
		if~(A = \emptyset)~\emptyset~(\liftmap~(f~0)~A)
\end{align*}
Substituting $g~0$ for $\liftmap~(f~0)$ gives a definition for $\lazymap$ (Figure~\ref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsection{Correctness}

\begin{theorem}[mapping arrow correctness]
$\liftmap$ is an arrow homomorphism.
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
For all programs $\mathit{e}$, $\meaningof{\mathit{e}}\map \equiv \liftmap~\meaningof{\mathit{e}}_\bot$.
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lazy Preimage Mappings}
\label{sec:lazy-preimage-mappings}

\begin{figure*}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \prepto Y ::= \pair{Set~Y, Set~Y \tto Set~X}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&pre : (X \mapto Y) \tto (X \prepto Y) \\
		&pre~g \ := \ \pair{range~g, \fun{B}{preimage~g~B}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&pre!ap : (X \prepto Y) \tto Set~Y \tto Set~X \\
		&pre!ap~\pair{Y',p}~B \ := \ p~(B \i Y') 
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&pre!range : (X \prepto Y) \tto Set~Y \\
		&pre!range \ := \ fst
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\pre : (X \prepto Y_1) \tto (X \prepto Y_2) \tto (X \prepto Y_1 \times Y_2) \\
		&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre \ := \ 
		\lzfclet{
			Y' & Y_1' \times Y_2' \\
			p & \fun{B}{\U\limits_{\pair{b_1,b_2} \in B}(p_1~\set{b_1}) \i (p_2~\set{b_2})} \\
		}{\pair{Y',p}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\pre) : (Y \prepto Z) \tto (X \prepto Y) \tto (X \prepto Z) \\
		&\pair{Z',p_2} \circ\pre h_1 \ := \ \pair{Z', \fun{C}{pre!ap~h_1~(p_2~C)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\pre) : (X \prepto Y) \tto (X \prepto Y) \tto (X \prepto Y) \\
		&\lzfcsplit{
			&h_1 \uplus\pre h_2 \ := \ 
			\lzfclet{
					Y' & (pre!range~h_1) \u (pre!range~h_2) \\
					p & \fun{B}{(pre!ap~h_1~B) \uplus (pre!ap~h_2~B)}
				}{\pair{Y',p}}
		}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Lazy preimage mappings and operations.}
\label{fig:preimage-mapping-defs}
\end{figure*}

On a computer, we do not often have the luxury of testing each function input to see whether it belongs to a preimage set.
Even for finite domains, doing so is often intractable.

If we wish to compute with infinite sets in the language implementation, we will need an abstraction that makes it easy to replace computation on points with computation on sets.
Therefore, in the preimage arrow, we will confine computation on points to instances of
\begin{equation}
	X \prepto Y \ ::= \ \pair{Set~Y, Set~Y \tto Set~X}
\end{equation}
Like a mapping, an $X \prepto Y$ has an observable domain---but computing the table of input-output pairs is delayed.
We therefore call these \mykeyword{lazy preimage mappings}.

Converting a mapping to a lazy preimage mapping requires constructing a delayed application of $preimage$:
\begin{equation}
\begin{aligned}
	&pre : (X \pto Y) \tto (X \prepto Y) \\
	&pre~g \ := \ \lzfclet{Y' & range~g \\ p & \fun{B}{preimage~g~B}}{\pair{Y',p}}
\end{aligned}
\end{equation}
Applying a preimage mapping to any subset of its codomain:
\begin{equation}
\begin{aligned}
	&pre!ap : (X \prepto Y) \tto Set~Y \tto Set~X \\
	&pre!ap~\pair{Y',p}~B \ := \ p~(B \i Y')
\end{aligned}
\end{equation}
The necessary property here is that using $pre!ap$ to compute preimages is the same as computing them from a mapping using $preimage$.

\begin{lemma}
Let $g \in X \pto Y$.
For all $B \subseteq Y$ and $Y'$ such that $range~g \subseteq Y' \subseteq Y$,
$preimage~g~(B \i Y') = preimage~g~B$.
\label{lem:preimage-restricted-range}
\end{lemma}

\begin{theorem}[$pre!ap$ computes preimages]
Let $g \in X \pto Y$. For all $B \subseteq Y$, $pre!ap~(pre~g)~B = preimage~g~B$.
\label{thm:pre-like-preimage}
\end{theorem}
\begin{proof}
Expand definitions and apply Lemma~\ref{lem:preimage-restricted-range} with $Y' = range~g$.
\end{proof}

Figure~\ref{fig:preimage-mapping-defs} defines more operations on preimage mappings, including pairing, composition, and disjoint union operations corresponding to the mapping operations in Figure~\ref{fig:mapping-defs}.
The next three theorems establish that $pre$ is a homomorphism (though not an arrow homomorphism): it distributes over mapping operations to yield preimage mapping operations.
We will use these facts to derive the preimage arrow from the mapping arrow.

First, we need preimage mappings to be equivalent when they compute the same preimages.

\begin{definition}[preimage mapping equivalence]
Two preimage mappings $h_1 : X \prepto Y$ and $h_2 : X \prepto Y$ are equivalent, or $h_1 \equiv h_2$, when $pre!ap~h_1~B \equiv pre!ap~h_2~B$ for all $B \subseteq Y$.
\end{definition}

The following subsections prove distributive laws for preimage mapping pairing, composition, and disjoint union.

\subsubsection{Preimage Mapping Pairing}

\begin{lemma}[$preimage$ distributes over $\pair{\cdot,\cdot}\map$ and $(\times)$]
Let $g_1 \in X \pto Y_1$ and $g_2 \in X \pto Y_2$.
For all $B_1 \subseteq Y_1$ and $B_2 \subseteq Y_2$, $preimage~\pair{g_1,g_2}\map~(B_1 \times B_2) = (preimage~g_1~B_1) \i (preimage~g_2~B_2)$.
\label{lem:preimage-under-pairing}
\end{lemma}

\begin{theorem}[$pre$ distributes over $\pair{\cdot,\cdot}\map$]
Let $g_1 \in X \pto Y_1$ and $g_2 \in X \pto Y_2$. Then $pre~\pair{g_1,g_2}\map \equiv \pair{pre~g_1,pre~g_2}\pre$.
\label{thm:preimage-mapping-pairing}
\end{theorem}
\begin{proof}
Let $\pair{Y_1',p_1} := pre~g_1$ and $\pair{Y_2',p_2} := pre~g_2$.
Starting from the right side, for all $B \in Y_1 \times Y_2$,
\begin{align*}
	&pre!ap~\pair{pre~g_1,pre~g_2}\pre~B 
\\
	&\tab\equiv \ 
		\lzfclet{
			Y' & Y_1' \times Y_2' \\
			p & \fun{B}{\U\limits_{\pair{y_1,y_2} \in B}(p_1~\set{y_1}) \i (p_2~\set{y_2})} \\
		}{p~(B \i Y')}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} (p_1~\set{y_1}) \i (p_2~\set{y_2})
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} (preimage~g_1~\set{y_1}) \i (preimage~g_2~\set{y_2})
\\
	&\tab\equiv \U\limits_{y \in B \i (Y_1' \times Y_2')} (preimage~\pair{g_1,g_2}\map~\set{y})
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~(B \i (Y_1' \times Y_2'))
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~B
\\
	&\tab\equiv \ pre!ap~(pre~\pair{g_1,g_2}\map)~B
\end{align*}
\end{proof}

\subsubsection{Preimage Mapping Composition}

\begin{lemma}[$preimage$ distributes over $(\circ\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in Y \pto Z$.
For all $C \subseteq Z$, $preimage~(g_2 \circ\map g_1)~C = preimage~g_1~(preimage~g_2~C)$.
\label{lem:preimage-under-composition}
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\circ\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in Y \pto Z$.
Then $pre~(g_2 \circ\map g_1) \equiv (pre~g_2) \circ\pre (pre~g_1)$.
\label{thm:preimage-mapping-composition}
\end{theorem}
\begin{proof}
Let $\pair{Z',p_2} := pre~g_2$.
Starting from the right side, for all $C \subseteq Z$,
\begin{align*}
	&pre!ap~((pre~g_2) \circ\pre (pre~g_1))~C
\\
	&\equiv\ 
		\lzfclet{
			h & \fun{C}{pre!ap~(pre~g_1)~(p_2~C)} \\
			}{h~(C \i Z')}
\\
	&\equiv\ pre!ap~(pre~g_1)~(p_2~(C \i Z'))
\\
	&\equiv\ pre!ap~(pre~g_1)~(pre!ap~(pre~g_2)~C)
\\
	&\equiv\ preimage~g_1~(preimage~g_2~C)
\\
	&\equiv\ preimage~(g_2 \circ\map g_1)~C
\\
	&\equiv\ pre!ap~(pre~(g_2 \circ\map g_1))~C
\end{align*}
\end{proof}

\subsubsection{Preimage Mapping Disjoint Union}

\begin{lemma}[$preimage$ distributes over $(\uplus\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in X \pto Y$ be disjoint mappings.
For all $B \subseteq Y$, $preimage~(g_1 \uplus\map g_2)~B = (preimage~g_1~B) \uplus (preimage~g_2~B)$.
\label{lem:preimage-under-piecewise}
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\uplus\map)$]
Let $g_1 \in X \pto Y$ and $g_2 \in X \pto Y$ have disjoint domains.
Then $pre~(g_1 \uplus\map g_2) \equiv (pre~g_1) \uplus\pre (pre~g_2)$.
\label{thm:piecewise-preimage-mappings}
\end{theorem}
\begin{proof}
Let $Y_1' := range~g_1$ and $Y_2' := range~g_2$.
Starting from the right side, for all $B \subseteq Y$,
\begin{align*}
	&pre!ap~((pre~g_1) \uplus\pre (pre~g_2))~B
\\
	&\equiv\ 
		\lzfclet{
			\! Y' & Y_1' \u Y_2' \\
			h & \fun{B}{(pre!ap~(pre~g_1)~B) \uplus (pre!ap~(pre~g_2)~B)}
		}{h~(B \i Y')}
\\
	&\equiv\ \lzfcsplit{&(pre!ap~(pre~g_1)~(B \i (Y_1' \u Y_2')))\ \uplus\\ &(pre!ap~(pre~g_2)~(B \i (Y_1' \u Y_2')))}
\\
	&\equiv\ \lzfcsplit{&(preimage~g_1~(B \i (Y_1' \u Y_2')))\ \uplus\\ &(preimage~g_2~(B \i (Y_1' \u Y_2')))}
\\
	&\equiv\ preimage~(g_1 \uplus\map g_2)~(B \i (Y_1' \u Y_2'))
\\
	&\equiv\ preimage~(g_1 \uplus\map g_2)~B
\\
	&\equiv\ pre!ap~(pre~(g_1 \uplus\map g_2))~B
\end{align*}
\end{proof}

\section{Deriving the Preimage Arrow}

We are ready to define an arrow that runs programs backward on sets of outputs.
Its computations should produce preimage mappings or be preimage mappings themselves.

As with the mapping arrow and mappings, we cannot have $X \preto Y ::= X \prepto Y$: we run into trouble trying to define $\arrpre$ because a preimage mapping needs an observable domain.
While a preimage mapping's domain is the \emph{range} of the mapping it computes preimages for, it is still easiest to parameterize preimage computations on a $Set~X$:
\begin{equation}
	X \preto Y \ ::= \ Set~X \tto (X \prepto Y)
\end{equation}
or $Set~X \tto \pair{Set~Y, Set~Y \tto Set~X}$.
To deconstruct the type, a preimage arrow computation computes a range first, and returns the range and a lambda that computes preimages.

To use Theorem~\ref{thm:homomorphism-implies-correct}, we need to define correctness using a lift from the mapping arrow to the preimage arrow:
\begin{equation}
\begin{aligned}
	&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
	&\liftpre~g~A \ := \ pre~(g~A)
\end{aligned}
\end{equation}
By Theorem~\ref{thm:pre-like-preimage}, for all $g : X \mapto Y$, $A \subseteq X$ and $B \subseteq Y$,
\begin{equation}
	pre!ap~(\liftpre~g~A)~B \equiv preimage~(g~A)~B
\end{equation}
Roughly, lifted mapping arrow computations compute correct preimages, exactly as we should expect them to.

We also need a coarser notion of equivalence.

\begin{definition}[Preimage arrow equivalence]
Two preimage arrow computations $h_1 : X \preto Y$ and $h_2 : X \preto Y$ are equivalent, or $h_1 \equiv h_2$, when 
$h_1~A \equiv h_2~A$ for all $A \subseteq X$.
\end{definition}

As with $\arrmap$, defining $\arrpre$ as a composition meets~\eqref{eqn:lift-distributes-over-arr}.
The following subsections derive $(\pairpre)$, $(\comppre)$, $\ifpre$ and $\lazypre$ from their corresponding mapping arrow combinators, in a way that ensures $\liftpre$ is an arrow homomorphism from the mapping arrow to the preimage arrow. Figure~\ref{fig:preimage-arrow-defs} contains the resulting definitions.

\begin{figure*}
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \preto Y ::= Set~X \tto (X \prepto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrpre : (X \tto Y) \tto (X \preto Y) \\
		&\arrpre \ := \ \liftpre \circ \arrmap
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\comppre) : (X \preto Y) \tto (Y \preto Z) \tto (X \preto Z) \\
		&(h_1~\comppre~h_2)~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(pre!range~h_1')
			}{h_2' \circ\pre h_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairpre) : (X \preto Y) \tto (X \preto Z) \tto (X \preto Y \times Z) \\
		&(h_1~\pairpre~h_2)~A \ := \ \pair{h_1~A,h_2~A}\pre
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifpre: (X \preto Bool) \tto (X \preto Y) \tto (X \preto Y) \tto (X \preto Y) \\
		&\ifpre~h_1~h_2~h_3~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(pre!ap~h_1'~\set{true}) \\
				h_3' & h_3~(pre!ap~h_1'~\set{false})
			}{h_2' \uplus\pre h_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazypre : (1 \tto (X \preto Y)) \tto (X \preto Y) \\
		&\lazypre~h~A \ := \ if~(A = \emptyset)~(pre~\emptyset)~(h~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
		&\liftpre~g~A \ := \ pre~(g~A)
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Preimage arrow definitions.}
\label{fig:preimage-arrow-defs}
\end{figure*}

\subsection{Case: Pairing}

Starting with the left side of~\eqref{eqn:lift-distributes-over-pair}, we expand definitions, apply Theorem~\ref{thm:preimage-mapping-pairing}, and rewrite in terms of $\liftpre$:
\begin{align*}
	&pre!ap~(\liftpre~(g_1~\pairmap~g_2)~A)~B
\\
	&\tab \equiv \ pre!ap~(pre~\pair{g_1~A, g_2~A}\map)~B
\\
	&\tab \equiv \ pre!ap~\pair{pre~(g_1~A), pre~(g_2~A)}\pre~B
\\
	&\tab \equiv \ pre!ap~\pair{\liftpre~g_1~A, \liftpre~g_2~A}\pre~B
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $pre!ap$ from both sides of the equivalence gives a definition of $(\pairpre)$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsection{Case: Composition}

Starting with the left side of~\eqref{eqn:lift-distributes-over-comp}, we expand definitions, apply Theorem~\ref{thm:preimage-mapping-composition} and rewrite in terms of $\liftpre$:
\begin{align*}
	&pre!ap~(\liftpre~(g_1~\compmap~g_2)~A)~C
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{pre!ap~(pre~(g_2' \circ\map g_1'))~C}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{pre!ap~((pre~g_1') \circ\pre (pre~g_2'))~C}
\\
	&\tab \equiv \
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(pre!range~h_1)
		}{pre!ap~(h_2 \circ\pre h_1)~C}
\numberthis
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $pre!ap$ from both sides of the equivalence gives a definition of $(\comppre)$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-comp} holds.

\subsection{Case: Conditional}

Starting with the left side of~\eqref{eqn:lift-distributes-over-if}, we expand terms, apply Theorem~\ref{thm:piecewise-preimage-mappings}, rewrite in terms of $\liftpre$, and apply Theorem~\ref{thm:pre-like-preimage} in the definitions of $h_2$ and $h_3$:
\begin{align*}
	&pre!ap~(\liftpre~(\ifmap~g_1~g_2~g_3)~A)~B
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{pre!ap~(pre~(g_2' \uplus\map g_3'))~B}
\\
	&\tab \equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{pre!ap~((pre~g_2') \uplus\pre (pre~g_3'))~B}
\\
	&\tab \equiv \ 
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(pre!ap~h_1~\set{true}) \\
			h_3 & \liftpre~g_3~(pre!ap~h_1~\set{false})
		}{pre!ap~(h_2 \uplus\pre h_3)~B}
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$, $h_2$ for $\liftpre~g_2$ and $h_3$ for $\liftpre~g_3$, and removing the application of $pre!ap$ from both sides of the equivalence gives a definition of $\ifpre$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsection{Case: Laziness}

Starting with the left side of~\eqref{eqn:lift-distributes-over-lazy}, expand definitions, distribute $pre$ over the branches of $if$, and rewrite in terms of $\liftpre~(g~0)$:
\begin{align*}
	&pre!ap~(\liftpre~(\lazymap~g)~A)~B
\\
	&\tab\equiv \
		\lzfclet{
			g' & if~(A = \emptyset)~\emptyset~(g~0~A)
		}{pre!ap~(pre~g')~B}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(pre~(g~0~A))
		}{pre!ap~h~B}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(\liftpre~(g~0)~A)
		}{pre!ap~h~B}
\end{align*}
Substituting $h~0$ for $\liftpre~(g~0)$ and removing the application of $pre!ap$ from both sides of the equivalence gives a definition for $\lazypre$ (Figure~\ref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsection{Correctness}

\begin{theorem}[preimage arrow correctness]
$\liftpre$ is an arrow homomorphism.
\label{thm:liftpre-homomorphism}
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
For all programs $\mathit{e}$, $\meaningof{\mathit{e}}\pre \equiv \liftpre~\meaningof{\mathit{e}}\map$.
\label{cor:preimage-arrow-correctness}
\end{corollary}

\section{Preimages Under Partial Functions}

Probabilistic functions that may diverge, but converge with probability 1, are common.
They come up not only when practitioners want to build data with random size or structure, but in simpler circumstances as well.

Suppose $random$ retrieves a number $r~j \in [0,1]$ from an implicit random source $r$.
The following recursive function, which defines the well-known \keyword{geometric distribution} with parameter $p$, counts the number of times $random < p$ is $false$:
\begin{equation}
	geometric~p \ := \ if~(random < p)~0~(1 + geometric~p)
\label{eqn:geometric-def}
\end{equation}
For any $p > 0$, $geometric~p$ may diverge, but the probability of always taking the false branch is $(1-p) \times (1-p) \times (1-p) \times \cdots = 0$.
Divergence with probability $0$ simply does not happen in practice.

Suppose we interpret~\eqref{eqn:geometric-def} as $h : R \preto \Nat$, a preimage arrow computation from random sources in $R$ to natural numbers, and that we have a probability measure $P \in \powerset~R \pto [0,1]$.
We could compute the probability of any output set $N \subseteq \Nat$ using $P~(h~R'~N)$, where $R' \subseteq R$ and $P~R' = 1$. We have three hurdles to overcome:
\begin{enumerate}
	\item Ensuring $h~R'$ converges.
	\item Ensuring each $r \in R$ contains enough random numbers.
	\item Determining how $random$ indexes numbers in $r$.
\end{enumerate}
Ensuring $h~R'$ converges is the most difficult, but doing the other two will provide structure that makes it much easier.

\subsection{Threading and Indexing}

We clearly need a new arrow that threads a random source through its computations.
To ensure it contains enough random numbers, the source should be infinite.

In a pure $\lambda$-calculus, random sources are typically infinite streams, threaded monadically: each computation receives and produces a random source.
A new combinator is defined that removes the head of the random source and passes the tail along.
This is likely preferred because pseudorandom number generators are almost universally monadic.

A little-used alternative is for the random source to be a tree, threaded applicatively:
each computation receives, but does not produce, a random source.
Multi-argument combinators split the tree and pass sub-trees to sub-computations.

We have tried both ways with arrows defined using pairing.
The resulting definitions are large, making them conceptually difficult and hard to manipulate.
Fortunately, assigning each sub-computation a unique index into a tree-shaped random source, and passing it unchanged, is relatively easy.

We need a way to assign unique indexes to expressions.

\begin{definition}[binary indexing scheme]
Let $J$ be an index set, $j_0 \in J$ a distinguished element, and $left : J \tto J$ and $right : J \tto J$ be total functions.
These define a \mykeyword{binary indexing scheme} if
\begin{itemize}
	\item Every finite composition $next$ of $left$ and $right$ is injective.
	\item For all $j \in J$, $j = next~j_0$ for some finite composition $next$.
\end{itemize}
\end{definition}

For example, let $J$ be the set of lists of $\set{0,1}$, $j_0 := \pair{}$, and $left~j := \pair{0,j}$ and $right~j := \pair{1,j}$.

Alternatively, let $J$ be the set of dyadic rationals in $(0,1)$ (i.e. those with power-of-two denominators), $j_0 := \tfrac{1}{2}$ and
\begin{equation}
\begin{aligned}
	left~(p/q) &\ := \ (p-\tfrac{1}{2})/q
\\
	right~(p/q) &\ := \ (p+\tfrac{1}{2})/q
\end{aligned}
\end{equation}
With this alternative, left-to-right evaluation order can be made to correspond with the natural order $(<)$ over $J$.

In any case, $J$ is always countable.

\subsection{Applicative, Associative Store}

XXX: \keyword{arrow transformer}: an arrow whose combinators are defined entirely in terms of another arrow

XXX: computations receive an index and return an arrow from a store of type $s$ paired with $x$, to $y$:
\begin{equation}
	AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
\end{equation}

XXX: motivational wurds for definition of lift:
\begin{equation}
\begin{aligned}
	&\arrowtrans\genc : (x \arrow\gen y) \tto AStore~s~(x \arrow\gen y) \\
	&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
\end{aligned}
\end{equation}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		x \arrow\genc y \ ::= \ AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowarr\genc : (x \tto y) \tto (x \arrow\genc y) \\
		&\arrowarr\genc \ := \ \arrowtrans\genc \circ \arrowarr\gen
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowcomp\genc) : (x \arrow\genc y) \tto (y \arrow\genc z) \tto (x \arrow\genc z) \\
		&(k_1~\arrowcomp\genc~k_2)~j \ := \\
			&\tab(\arrowarr\gen~fst~\arrowpair\gen~k_1~(left~j))~\arrowcomp\gen~k_2~(right~j)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowpair\genc) : (x \arrow\genc y_1) \tto (x \arrow\genc y_2) \tto (x \arrow\genc \pair{y_1,y_2}) \\
		&(k_1~\arrowpair\genc~k_2)~j \ := \ k_1~(left~j)~\arrowpair\gen~k_2~(right~j)
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\arrowif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowif\genc~k_1~k_2~k_3~j \ := \
			\lzfcsplit{\arrowif\gen~&(k_1~(left~j)) \\ &(k_2~(left~(right~j))) \\ &(k_3~(right~(right~j)))}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowlazy\genc : (1 \tto (x \arrow\genc y)) \tto (x \arrow\genc y) \\
		&\arrowlazy\genc~k~j \ := \ \arrowlazy\gen~\fun{0}{k~0~j}
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowtrans\genc : (x \arrow\gen y) \tto (x \arrow\genc y) \\
		&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{$AStore$ (associative store) arrow transformer definitions.}
\label{fig:astore-arrow-defs}
\end{figure*}

Figure~\ref{fig:astore-arrow-defs} defines the $AStore$ arrow transformer.
As with the other arrows, proving that its lift is a homomorphism allows us to prove that programs interpreted as its computations are correct.
Again, to do so, we need to extend equivalence to be more extensional for arrows $AStore~s~(x \arrow\gen y)$.

\begin{definition}[$AStore$ arrow equivalence]
Two $AStore$ arrow computations $k_1$ and $k_2$ are equivalent, or $k_1 \equiv k_2$, when $k_1~j \equiv k_2~j$ for all $j \in J$.
\end{definition}

XXX: need to define equivalence for the bottom arrow for this to make sense

\begin{theorem}[$AStore$ arrow correctness]
Let $x \arrow\genc y ::= AStore~s~(x \arrow\gen y)$.
Then $\arrowtrans\genc$ is an arrow homomorphism.
\end{theorem}
\begin{proof}
Defining $\arrowarr\genc$ as a composition clearly meets the first homomorphism identity~\eqref{eqn:lift-distributes-over-arr}.

\emph{Composition.} Starting with the right side of~\eqref{eqn:lift-distributes-over-comp}, expand definitions and use $(\arrowarr\gen~f~\arrowpair\gen~f_1)~\arrowcomp\gen~\arrowarr\gen~snd \equiv f_1$:
\begin{align*}
	&(\arrowtrans\genc~f_1~\arrowcomp\genc~\arrowtrans\genc~f_2)~j
\\
	&\tab\equiv\ 
		(\arrowarr\gen~fst~\arrowpair\gen~(\arrowarr\gen~snd~\arrowcomp\gen~f_1))~
		\arrowcomp\gen~
		\arrowarr\gen~snd~\arrowcomp\gen~f_2
\\
	&\tab\equiv\
		\arrowarr\gen~snd~\arrowcomp\gen~f_1~\arrowcomp\gen~f_2
\\
	&\tab\equiv\
		\arrowtrans\genc~(f_1~\arrowcomp\gen~f_2)~j
\end{align*}

\emph{Pairing.} Starting with the right side of~\eqref{eqn:lift-distributes-over-pair}, expand definitions and use the arrow law $\arrowarr\gen~f~\arrowcomp\gen~(f_1~\arrowpair\gen~f_2) \equiv (\arrowarr\gen~f~\arrowcomp\gen~f_1)~\arrowpair\gen~(\arrowarr\gen~f~\arrowcomp\gen~f_2)$:
\begin{align*}
	&(\arrowtrans\genc~f_1~\arrowpair\genc~\arrowtrans\genc~f_2)~j
\\
	&\tab\equiv\ 
		(\arrowarr\gen~snd~\arrowcomp\gen~f_1)~
		\arrowpair\gen~
		(\arrowarr\gen~snd~\arrowcomp\gen~f_2)
\\
	&\tab\equiv\
		\arrowarr\gen~snd~\arrowcomp\gen~(f_1~\arrowpair\gen~f_2)
\\
	&\tab\equiv\
		\arrowtrans\genc~(f_1~\arrowpair\gen~f_2)~j
\end{align*}

\emph{Conditional.} Starting with the right side of~\eqref{eqn:lift-distributes-over-if}, expand definitions and use the arrow law $\arrowarr\gen~f~\arrowcomp\gen~\arrowif\gen~f_1~f_2~f_3 \equiv \arrowif\gen~(\arrowarr\gen~f~\arrowcomp\gen~f_1)~(\arrowarr\gen~f~\arrowcomp\gen~f_2)~(\arrowarr\gen~f~\arrowcomp\gen~f_3)$:
\begin{align*}
	&(\arrowif\genc~(\arrowtrans\genc~f_1)~(\arrowtrans\genc~f_2)~(\arrowtrans\genc~f_3))~j
\\
	&\tab\equiv\ 
		\lzfcsplit{\arrowif\gen~
			&(\arrowarr\gen~snd~\arrowcomp\gen~f_1) \\ 
			&(\arrowarr\gen~snd~\arrowcomp\gen~f_2) \\
			&(\arrowarr\gen~snd~\arrowcomp\gen~f_3)}
\\
	&\tab\equiv\ 
		\arrowarr\gen~snd~\arrowcomp\gen~(\arrowif~f_1~f_2~f_3)
\\
	&\tab\equiv\ 
		\arrowtrans\genc~(\arrowif~f_1~f_2~f_3)~j
\end{align*}

\emph{Laziness.} Starting with the right side of~\eqref{eqn:lift-distributes-over-lazy}, expand definitions, $\beta$-expand within the outer thunk, and use the arrow law $\arrowarr\gen~f~\arrowcomp\gen~\arrowlazy\gen~f_1 \equiv \arrowlazy\gen~\fun{0}{\arrowarr\gen~f~\arrowcomp\gen~f_1~0}$:
\begin{align*}
	&(\arrowlazy\genc~\fun{0}{\arrowtrans\genc~(f~0)})~j
\\
	&\tab\equiv\ 
		\arrowlazy\gen~\fun{0}{(\fun{0}\fun{j}{\arrowarr\gen~snd~\arrowcomp\gen~f~0})~0~j}
\\
	&\tab\equiv\ 
		\arrowlazy\gen~\fun{0}{\arrowarr\gen~snd~\arrowcomp\gen~f~0}
\\
	&\tab\equiv\ 
		\arrowarr\gen~snd~\arrowcomp\gen~\arrowlazy\gen~f
\\
	&\tab\equiv\ 
		\arrowtrans\genc~(\arrowlazy\gen~f)~j
\end{align*}

XXX: all of these rely on arrow laws that aren't proved for the mapping and preimage arrows
\end{proof}

\begin{corollary}[semantic correctness]
Let $x \arrow\genc y ::= AStore~s~(x \arrow\gen y)$. If $\meaningof{\mathit{e}}\gen : x \arrow\gen y$, then $\arrowtrans\genc~\meaningof{\mathit{e}}\gen \equiv \meaningof{\mathit{e}}\genc$ and $\meaningof{\mathit{e}}\genc : x \arrow\genc y$.
\label{cor:astore-semantic-correctness}
\end{corollary}

In particular, Corollary~\ref{cor:astore-semantic-correctness} implies that, if a pure let-calculus expression is interpreted as a computation $k : AStore~S~(X \preto Y)$, then $k~j_0$ correctly computes preimages.
We still need to know that preimages under functions that access the store are computed correctly, which we will get to after defining stores and combinators that access them.


\subsection{Probabilistic Programs}

\begin{definition}[random source]
Let $R := J \to [0,1]$.
A \keyword{random source} is a total mapping $r \in R$; equivalently, an infinite vector of random numbers indexed by $J$.
\end{definition}

Let $x \arrow\genc y ::= AStore~R~(x \arrow\gen y)$.
The following combinator returns the number at its own index in the random source:
\begin{equation}
\begin{aligned}
	&random\genc : x \arrow\genc [0,1] \\
	&random\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~\pi~j)
\end{aligned}
\end{equation}
We extend the let-calculus semantic function with
\begin{equation}
	\meaningof{random}\genc :\equiv random\genc
\end{equation}
for arrows $a^*$ for which $random\genc$ is defined.

\subsection{Partial Programs}

%Divergence is an applicative effect, so it should not be too surprising that it can be modeled using an applicative store.
The most effective and ultimately implementable way we have found to avoid divergence in computing preimages is to use the store to dictate which branch of each conditional, if any, is allowed to be taken.

\begin{definition}[branch trace]
A \mykeyword{branch trace} is a total mapping (i.e. vector) $t \in J \to Bool_\bot$ such that $t~j = true$ or $t~j = false$ for no more than finitely many $j \in J$.
\end{definition}

Let $T \subset J \to Bool_\bot$ be the set of all branch traces, and $x \arrow\genc y ::= AStore~T~(x \arrow\gen y)$.
The following combinator returns $t~j$ using its own index $j$:
\begin{equation}
\begin{aligned}
	&branch\genc : x \arrow\genc Bool \\
	&branch\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~\pi~j)
\end{aligned}
\end{equation}
Using $branch\genc$, we define an additional if-then-else combinator, which ensures its conditional expression agrees with the branch trace:
\begin{align}
	&\begin{aligned}
		&agrees : \pair{Bool,Bool} \tto Bool_\bot \\
		&agrees~\pair{b_1,b_2} \ := \ if~(b_1 = b_2)~b_1~\bot
	\end{aligned} \\
\nonumber \\[-6pt]
	&\begin{aligned}
		&\arrowif\genc' : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowif\genc'~k_1~k_2~k_3 \ := \\ 
			&\tab\arrowif\genc~((k_1~\arrowpair\genc~branch\genc)~\arrowcomp\genc~\arrowarr\genc~agrees)~k_2~k_3
	\end{aligned}
\end{align}
If the branch trace agrees with the conditional expression, it computes a branch; otherwise, it returns an error.

Every computation defined using the let-calculus semantic function $\meaningof{\cdot}\gen$, whose \emph{defining expression} converges, must have its recurrences guarded by an $if$.
Thus, if we stick to well-defined let-calculus programs, we need only replace $\arrowif\genc$ with $\arrowif\genc'$ to ensure \emph{computations} always converge.
Therefore, we can define a semantic function $\meaningof{\cdot}\genc'$ for let-calculus programs whose computations always converge by overriding only the $if$ rule:
\begin{equation}
\begin{aligned}
	\meaningof{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\genc' &\ :\equiv\
		\lzfcsplit{\arrowif\genc'~
			&\meaningof{\mathit{e_c}}\genc \\
			&(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_t}}\genc}) \\
			&(\arrowlazy\gen~\fun{0}{\meaningof{\mathit{e_f}}\genc})}
\\
	\meaningof{\mathit{e}}\genc' &\ :\equiv\ \meaningof{\mathit{e}}\genc
\end{aligned}
\end{equation}

\subsection{Partial, Probabilistic Programs}

Let $S ::= R \times T$ and $x \arrow\genc y ::= AStore~S~(x \arrow\gen y)$, and update the $random\genc$ and $branch\genc$ combinators to reflect that the store is now a pair:
\begin{align}
	&\begin{aligned}
		&random\genc : x \arrow\genc [0,1] \\
		&random\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~fst~\arrowcomp~\pi~j)
	\end{aligned} \\
\nonumber\\[-6pt]
	&\begin{aligned}
		&branch\genc : x \arrow\genc Bool \\
		&branch\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~snd~\arrowcomp~\pi~j)
	\end{aligned}
\end{align}
The $\arrowif\genc'$ combinator's definition remains the same.

From here on, let $x \pbotto y ::= AStore~S~(x \botto y)$; similarly for $X \pmapto Y$ and $X \ppreto Y$.

\subsection{Correctness}

\begin{theorem}[natural transformation]
Let $x \arrow\genc y ::= AStore~s~(x \arrow\gen y)$ and $x \arrow\gend y ::= AStore~s~(x \arrow\gend y)$.
Let $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ be an arrow homomorphism, and
\begin{equation}
\begin{aligned}
	&\arrowlift\gend : (x \arrow\genc y) \tto (x \arrow\gend y) \\
	&\arrowlift\gend~f~j \ := \ \arrowlift\genb~(f~j)
\end{aligned}
\end{equation}
The following diagram commutes:
\begin{equation}
\begin{CD}
	x \arrow\gen y @>{\arrowlift\genb}>> x \arrow\genb y \\
	@V{\arrowtrans\genc}VV   @VV{\arrowtrans\gend}V \\
	x \arrow\genc y @>>{\arrowlift\gend}> x \arrow\gend y
\end{CD}
\end{equation}
i.e. for all $f : x \arrow\gen y$, $\arrowtrans\gend~(\arrowlift\genb~f) \equiv \arrowlift\gend~(\arrowtrans\genc~f)$.
Further, $\arrowlift\gend$ is an arrow homomorphism.
\end{theorem}
\begin{proof}
Starting from the right side of the equivalence, expand definitions and apply homomorphism identities~\eqref{eqn:lift-distributes-over-comp} and~\eqref{eqn:lift-distributes-over-arr} for $\arrowlift\genb$:
\begin{align*}
	\arrowlift\gend~(\arrowtrans\genc~f)
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd~\arrowcomp\gen~f)}
\\
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd)~\arrowcomp\genb~\arrowlift\genb~f}
\\
	&\ \equiv\ \fun{j}{\arrowarr\genb~snd~\arrowcomp\genb~\arrowlift\genb~f}
\\
	&\ \equiv\ \arrowtrans\gend~(\arrowlift\genb~f)
\end{align*}
Further, because $\arrowtrans\genc$, $\arrowtrans\gend$, and $\arrowlift\genb$ are homomorphisms, $\arrowlift\gend$ is a homomorphism by composition.

XXX: not sure I'm allowed to invoke converse of composition of homomorphisms without extra conditions
\end{proof}

\begin{corollary}[mapping* and preimage* arrow correctness]
The following diagram commutes:
\begin{equation}
\begin{CD}
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y
\end{CD}
\end{equation}
Further, $\liftpmap$ and $\liftppre$ are arrow homomorphisms.
\end{corollary}

\begin{corollary}[semantic correctness]
If $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$, then $\liftpmap~\meaningof{\mathit{e}}_\pbot \equiv \meaningof{\mathit{e}}\pmap$ and $\liftppre~\meaningof{\mathit{e}}\pmap \equiv \meaningof{\mathit{e}}\ppre$.
\end{corollary}

\begin{corollary}[semantic$'$ correctness]
If $\meaningof{\mathit{e}}_\pbot' : X \pbotto Y$, then $\liftpmap~\meaningof{\mathit{e}}_\pbot' \equiv \meaningof{\mathit{e}}\pmap'$ and $\liftppre~\meaningof{\mathit{e}}\pmap' \equiv \meaningof{\mathit{e}}\ppre'$.
\end{corollary}

In particular, $\meaningof{\mathit{e}}\ppre$ and $\meaningof{\mathit{e}}\ppre'$ correctly compute preimages under the interpretation of $\mathit{e}$ as a function from an implicit random source.
We will make stronger statements about $\meaningof{\cdot}\ppre'$ after proving its computations always converge.

\begin{theorem}[divergence implies error]
Let $f := \meaningof{\mathit{e}}_\pbot$ and $f' := \meaningof{\mathit{e}}_\pbot'$ converge, where $f : x \pbotto y$.
For all $r \in R$, $t \in T$ and $a : x$,
\begin{enumerate}
	\item If $~f~\pair{\pair{r,t},a} = b$, then $f'~\pair{\pair{r,t'},a} = b$ for some $t' \in T$.
	\item If $~f~\pair{\pair{r,t},a}$ diverges, $f'~\pair{\pair{r,t'},a} = \bot$ for all $t' \in T$.
\end{enumerate}
\end{theorem}
\begin{proof}
Let $m : J \tto J$ invertibly map sub-computation indexes in $f'$ to corresponding sub-computation indexes in $f$.
(Defining $m$ formally is tedious and unilluminating. XXX: check this)

\emph{Case 1.}
Define $t' \in J \to Bool_\bot$ such that $t'~j = z$ if the sub-computation with index $m~j$ in $f$ is an $if$ condition that returns $z$, otherwise $t'~j = \bot$.
Because $f~\pair{\pair{r,t},a}$ converges, $t'~j \neq \bot$ for at most finitely many $j$, so $t' \in T$.
Exists $t'$.

\emph{Case 2.}
Let $t' \in T$.
There exists an infinite \emph{suffix} $J' \subset J$ closed under $left$ and $right$, such that for all $j' \in J'$, $t'~j' = \bot$.
Because $f~\pair{\pair{r,t},a}$ diverges, the indexes of its $if$ conditions are unbounded; there is therefore a condition with index $j$ such that $m^{-1}~j \in J'$.
It returns $\bot$, so $f'~\pair{\pair{r,t'},a} = \bot$.
\end{proof}

To compare preimages computed by arrow instances produced by $\meaningof{\cdot}\ppre$ and $\meaningof{\cdot}\ppre'$, we need a set of inputs on which they should obviously always agree.

\begin{definition}[halting set]
A computation's \mykeyword{halting set} is the largest $A^* \subseteq (R \times T) \times X$ for which
\begin{itemize}
	\item For $f : X \pbotto Y$, $f~j_0~x \neq \bot$ for all $x \in A^*$.
	\item For $g : X \pmapto Y$, $domain~(g~j_0~A^*) = A^*$.
	\item For $h : X \ppreto Y$, $pre!ap~(h~j_0~A^*)~Y = A^*$.
\end{itemize}
Recall truth statements like $f~j_0~x \neq \bot$ imply convergence.
\end{definition}

That $\liftpmap$ and $\liftppre$ are arrow homomorphisms allows transporting halting set definitions and theorems between arrow types.

\begin{theorem}[halting set equality]
Let $f : X \pbotto Y$, and $g : X \pmapto Y$ and $h : X \ppreto Y$ such that $g \equiv \liftpmap~f$ and $h \equiv \liftppre~g$.
Then $f$, $g$ and $h$ have the same halting set.
\end{theorem}
\begin{proof}
XXX: do this
\end{proof}

\begin{corollary}[computed halting set]
Let $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$ converge.
Then $A^* = pre!ap~(\meaningof{\mathit{e}}\ppre'~j_0~(S \times X))~Y$.
\end{corollary}

\begin{corollary}[semantic correctness (final)]
Let $\meaningof{\mathit{e}}_\pbot : X \pbotto Y$ converge, with halting set $A^*$.
For $A \subseteq X$ and $B \subseteq Y$, $pre!ap~(\meaningof{\mathit{e}}\ppre'~j_0~A)~B = preimage~(\meaningof{\mathit{e}}\pmap~j_0~(A \i A^*))~B$.
\end{corollary}

In other words, preimages computed using $\meaningof{\cdot}\ppre'$ always converge, never include inputs that give rise to errors or divergence, and are correct.

\section{Measurability}

We have not assigned probabilities to any sets yet.

Recall that, for a mapping $g : X \pto Y$, the probability of an output set $B \subseteq Y$ is
\begin{equation}
	P~(preimage~g~B)
\end{equation}
where $P \in \powerset~X \pto [0,1]$ is a probability measure on $X$.
This was the motivation for defining arrows to compute preimages in the first place.
Note again that $P$ is a \emph{partial} function.
We had left $domain~P$, and which $B \subseteq Y$ have preimages in $domain~P$, as technical conditions to be proved later.
Now we determine those conditions.

To save space, we assume readers are familiar with either topology or measure theory.
(Readers unfamiliar with both may wish to skip to the next section.)
Many concepts in measure theory can be understood by analogy to topology.

The analogue of a topology is a $\sigma$-algebra.

\begin{definition}[$\sigma$-algebra, measurable set]
A collection of sets $\A \subseteq \powerset~X$ is called a \keyword{$\sigma$-algebra} on $X$ if it contains $X$ and is closed under complements and countable unions.
The sets in $\A$ are called \keyword{measurable sets}.
\end{definition}

$X \w X = \emptyset$, so $\emptyset \in \A$.
Additionally, it follows from De Morgan's law that $\A$ is closed under countable intersections.

The analogue of continuity is measurability.

\begin{definition}[measurable mapping]
Let $\A$ and $\B$ be $\sigma$-algebras respectively on $X$ and $Y$.
A mapping $g : X \pto Y$ is $\A!\B$-\keyword{measurable} if for all $B \in \B$, $preimage~g~B \in \A$.
\end{definition}

Measurability is usually a weaker condition than continuity.
For example, with respect to the $\sigma$-algebra generated from $\Re$'s standard topology, measurable $\Re \pto \Re$ functions may have countably many discontinuities.
Likewise, real equality and inequality functions are measurable.

Product spaces are defined the same way as in topology.

\begin{definition}[finite product $\sigma$-algebra]
Let $\A_1$ and $\A_2$ be $\sigma$-algebras on $X_1$ and $X_2$, and $X := \pair{X_1,X_2}$.
The \keyword{product $\sigma$-algebra} $\A_1 \otimes \A_2$ is the smallest $\sigma$-algebra for which $mapping~fst~X$ and $mapping~snd~X$ are measurable.
\label{def:finite-product-sigma-algebra}
\end{definition}

\begin{definition}[arbitrary product $\sigma$-algebra]
Let $\A$ be a $\sigma$-algebra on $X$.
The \keyword{product $\sigma$-algebra} $\A^{\otimes J}$ is the smallest $\sigma$-algebra for which, for all $j \in J$, $mapping~(\pi~j)~(J \to X)$ is measurable.
\label{def:arbitrary-product-sigma-algebra}
\end{definition}

\subsection{Measurable Pure Computations}

It is easier to prove measurability of pure computations than to prove measurability of probabilistic ones.
Further, we can use the resulting theorems to prove that probabilistic computations are measurable.

A single mapping arrow computation can produce many mappings, which, it seems, could complicate proving measurability.
Fortunately, we need only consider the mapping produced when applying a computation to its halting set.

\begin{definition}[halting set]
Let $g : X \mapto Y$. Its \keyword{halting set} is the largest $A^* \subseteq X$ for which $domain~(g~A^*) = A^*$.
\end{definition}

\begin{definition}[measurable mapping arrow computation]
Let $\A$ and $\B$ be $\sigma$-algebras on $X$ and $Y$.
A computation $g : X \mapto Y$ is $\A!\B$-\keyword{measurable} if $g~A^*$ is an $\A!\B$-measurable mapping, where $A^*$ is $g$'s halting set.
\end{definition}

The definition of halting set implies $preimage~(g~A^*)~Y = A^*$.
Because $Y \in \B$, $A^* \in \A$.

\begin{lemma}
Let $g : X \pto Y$ be an $\A!\B$-measurable mapping.
For any $A \in \A$, $restrict~g~A$ is $\A!\B$-measurable.
\label{lem:restricted-mappings-are-measurable}
\end{lemma}

\begin{theorem}
Let $g : X \mapto Y$ be an $\A!\B$-measurable mapping arrow computation.
Then for all $A \in \A$, $g~A$ is an $\A!\B$-measurable mapping.
\label{thm:restricted-computations-are-measurable}
\end{theorem}
\begin{proof}
Use the mapping arrow restriction law~\eqref{eqn:mapping-arrow-restriction-law} and Lemma~\ref{lem:restricted-mappings-are-measurable}.
\end{proof}

Roughly, if the largest mapping that can be produced by a computation is measurable, any mapping it can produce that we care about is measurable.

That all programs interpreted by $\meaningof{\cdot}\gen$ are measurable will be proved by structural induction.
We therefore need a case for each arrow combinator.

\subsubsection{Case: Composition}

Proving compositions are measurable takes the most work.
The main complication is that, under measurable mappings, while \emph{preimages} of measurable sets are measurable, \emph{images} of measurable sets may not be.
We need the following four extra theorems to get around this.

\begin{lemma}[images of preimages]
Let $g : X \pto Y$ and $B \subseteq Y$. Then $image~g~(preimage~g~B) \subseteq B$.
\label{lem:images-of-preimages}
\end{lemma}

\begin{lemma}[expanded post-composition]
Let $g_1 : X \pto Y$ and $g_2 : Y \pto Z$ such that $range~g_1 \subseteq domain~g_2$, and let $g_2' : Y \pto Z$ such that $g_2 \subseteq g_2'$.
Then $g_2 \circ\map g_1 = g_2' \circ\map g_1$.
\label{lem:composition-expansion}
\end{lemma}

\begin{theorem}[mapping arrow monotonicity]
Let $g : X \mapto Y$.
For any $A \subseteq X$, $domain~(g~A) \subseteq A$.
For any $A' \subseteq A$, $g~A' \subseteq g~A$.
\label{thm:mapping-arrow-monotonicity}
\end{theorem}
\begin{theorem}[halting subsets]
Let $g : X \mapto Y$ with halting set $A^*$. For any $A \subseteq A^*$, $domain~(g~A) = A$.
\label{thm:halting-subsets}
\end{theorem}
\begin{proof}
Use the mapping arrow restriction law~\eqref{eqn:mapping-arrow-restriction-law}.
\end{proof}

Now we can prove measurability.

\begin{lemma}[measurability under $\circ\map$]
If $g_1 : X \pto Y$ is $\A!\B$-measurable and $g_2 : Y \pto Z$ is $\B!\C$-measurable, then $g_2 \circ\map g_1$ is $\A!\C$-measurable.
\label{lem:compositions-are-measurable}
\end{lemma}

\begin{theorem}[measurability under $(\compmap)$]
If $g_1 : X \mapto Y$ is $\A!\B$-measurable and $g_2 : Y \mapto Z$ is $\B!\C$-measurable, then $g_1~\compmap~g_2$ is $\A!\C$-measurable.
\end{theorem}
\begin{proof}
Let $A^* \in \A$ and $B^* \in \B$ be respectively $g_1$'s and $g_2$'s halting sets.
The halting set of $g_1~\compmap~g_2$ is $A^{**} := preimage~(g_1~A^*)~B^*$, which is in $\A$.
By definition,
\begin{equation}
	(g_1~\compmap~g_2)~A^{**} \ = \ 
		\lzfclet{
			g_1' & g_1~A^{**} \\
			g_2' & g_2~(range~g_1')
		}{g_2' \circ\map g_1'}
\end{equation}
By Theorem~\ref{thm:restricted-computations-are-measurable}, $g_1'$ is an $\A!\B$-measurable mapping.
Unfortunately, $g_2'$ may not be $\B!\C$-measurable when $range~g_1' \not\in \B$.

Let $g_2'' := g_2~B^*$, which is a $\B!\C$-measurable mapping.
By Lemma~\ref{lem:compositions-are-measurable}, $g_2'' \circ\map g_1'$ is $\A!\C$-measurable.
We need only show that $g_2' \circ\map g_1' = g_2'' \circ\map g_1'$, which by Lemma~\ref{lem:composition-expansion} is true if $range~g_1' \subseteq domain~g_2'$ and $g_2' \subseteq g_2''$.

By Theorem~\ref{thm:halting-subsets}, $A^{**} \subseteq A^*$ implies $domain~g_1' = A^{**}$.
By Theorem~\ref{thm:mapping-arrow-monotonicity} and Lemma~\ref{lem:images-of-preimages},
\begin{align*}
	range~g_1'
		%&\ =\ image~g_1'~A^{**} \\
		&\ =\ image~(g_1~A^{**})~(preimage~(g_1~A^*)~B^*) \\
		&\ =\ image~(g_1~A^*)~(preimage~(g_1~A^*)~B^*) \\
		&\ \subseteq\ B^*
\end{align*}
$range~g_1' \subseteq B^*$ implies (by Theorem~\ref{thm:halting-subsets}) that $domain~g_2' = range~g_1'$, and (by Theorem~\ref{thm:mapping-arrow-monotonicity}) that $g_2' \subseteq g_2''$.
\end{proof}

\subsubsection{Case: Pairing}

\begin{lemma}[measurability under $\pair{\cdot,\cdot}\map$]
If $g_1 : X \pto Y_1$ is $\A!\B_1$-measurable and $g_2 : X \pto Y_2$ is $\A!\B_2$-measurable, then $\pair{g_1,g_2}\map$ is $\A!(\B_1 \otimes \B_2)$-measurable.
\label{lem:pairings-are-measurable}
\end{lemma}

\begin{theorem}[measurability under $(\pairmap)$]
If $g_1 : X \mapto Y_1$ is $\A!\B_1$-measurable and $g_2 : X \mapto Y_2$ is $\A!\B_2$-measurable, then $g_1~\pairmap~g_2$ is $\A!(\B_1 \otimes \B_2)$-measurable.
\end{theorem}
\begin{proof}
Let $A_1^*$ and $A_2^*$ be respectively $g_1$'s and $g_2$'s halting sets.
The halting set of $g_1~\pairmap~g_2$ is $A^{**} := A_1^* \i A_2^*$, which is in $\A$.
By definition, $(g_1~\pairmap~g_2)~A^{**} = \pair{g_1~A^{**},g_2~A^{**}}\map$, which by Lemma~\ref{lem:pairings-are-measurable} is $\A!(\B_1 \otimes \B_2)$-measurable.
\end{proof}

\subsubsection{Case: Conditional}

\begin{lemma}[measurability under $\uplus\map$]
If $g_1 : X \pto Y$ and $g_2 : X \pto Y$ are $\A!\B$-measurable and have disjoint domains, $g_1 \uplus\map g_2$ is $\A!\B$-measurable.
\label{lem:disjoint-unions-are-measurable}
\end{lemma}

\begin{theorem}[measurability under $\ifmap$]
If $g_1 : X \mapto Bool$ is $\A!(\powerset~Bool)$-measurable, and $g_2 : X \mapto Y$ and $g_3 : X \mapto Y$ are $\A!\B$-measurable, then $\ifmap~g_1~g_2~g_3$ is $\A!\B$-measurable.
\end{theorem}
\begin{proof}
Let $\A_1^*$, $\A_2^*$ and $\A_3^*$ be respectively $g_1$'s, $g_2$'s and $g_3$'s halting sets.
The halting set of $\ifmap~g_1~g_2~g_3$ is defined by
\begin{equation}
\begin{aligned}
	A_2^{**} &\ :=\ A_2^* \i preimage~(g_1~\A_1^*)~\set{true} \\
	A_3^{**} &\ :=\ A_3^* \i preimage~(g_1~\A_1^*)~\set{false} \\
	A^{**} &\ :=\ A_2^{**} \uplus A_3^{**}
\end{aligned}
\end{equation}
Because $preimage~(g_1~\A_1^*)~B \in \A$ for any $B \subseteq Bool$, $A^{**} \in \A$.
By definition,
\begin{equation}
	\ifmap~g_1~g_2~g_3~A^{**} \ = \ 
		\lzfclet{
			g_1' & g_1~A^{**} \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{g_2' \uplus\map g_3'}
\end{equation}
By hypothesis, $g_1'$, $g_2'$ and $g_3'$ are measurable mappings, and the mapping arrow restriction law~\eqref{eqn:mapping-arrow-restriction-law} imples $g_2'$ and $g_3'$ have disjoint domains.
Apply Lemma~\ref{lem:disjoint-unions-are-measurable}.
\end{proof}

\subsubsection{Case: Laziness}

\begin{lemma}[measurability of $\emptyset$]
For any $\sigma$-algebras $\A$ and $\B$, the empty mapping $\emptyset$ is $\A!\B$-measurable.
\label{lem:empty-mapping-measurable}
\end{lemma}

\begin{theorem}[measurability under $\lazymap$]
Let $g : 1 \tto (X \mapto Y)$. If $g~0$ is $\A!\B$-measurable, then $\lazymap~g$ is $\A!\B$-measurable.
\end{theorem}
\begin{proof}
The halting set $A^{**}$ of $\lazymap~g$ is the same as that of $g~0$.
By definition,
\begin{equation}
	\lazymap~g~A^{**} \ = \ if~(A^{**} = \emptyset)~\emptyset~(g~0~A^{**})
\end{equation}
If $A^{**} = \emptyset$, then $\lazymap~g~A^{**} = \emptyset$; apply Lemma~\ref{lem:empty-mapping-measurable}.
If $A^{**} \neq \emptyset$, then $\lazymap~g = g~0$, which is $\A!\B$-measurable.
\end{proof}

\subsection{Measurable Probabilistic Computations}

We are now prepared to lift the previous theorems to probabilistic computations.

\begin{definition}[measurable mapping* arrow computation]
Let $\A$ and $\B$ be $\sigma$-algebras on $(R \times T) \times X$ and $Y$.
A computation $g : X \pmapto Y$ is $\A!\B$-\keyword{measurable} if $g~j_0~A^*$ is an $\A!\B$-measurable mapping, where $A^*$ is $g$'s halting set.
\end{definition}

To make general measurability statements about computations, whether they have flat or product types, it helps to have a notion of a standard $\sigma$-algebra.

\begin{definition}[standard $\sigma$-algebra]
For a set $X$ used as a type, $\Sigma~X$ denotes its \mykeyword{standard $\sigma$-algebra}, which must be defined under the following constraints:
\begin{align}
	\Sigma~\pair{X_1,X_2} &\ = \ \Sigma~X_1 \otimes \Sigma~X_2
	\label{eqn:standard-finite-product-rule}
\\
	\Sigma~(J \to X) &\ = \ (\Sigma~X)^{\otimes J}
	\label{eqn:standard-arbitrary-product-rule}
\\
	|X| \le \omega \ \implies \ \Sigma~X &\ =\ \powerset~X
	\label{eqn:standard-countable-rule}
\\
	X' \in \Sigma~X \ \implies \ \Sigma~X' &\ =\ \setb{X' \i A}{A \in \Sigma~X}
	\label{eqn:standard-subset-rule}
\end{align}
The predicate ``is measurable'' means ``is measurable with respect to standard $\sigma$-algebras.''
\label{def:standard-sigma-algebra}
\end{definition}
Examples: by~\eqref{eqn:standard-countable-rule}, $\Sigma~Bool = \powerset~Bool$.
By both~\eqref{eqn:standard-finite-product-rule} and~\eqref{eqn:standard-countable-rule}, $\Sigma~\pair{Bool,Bool} = \powerset~(Bool \times Bool)$.
By~\eqref{eqn:standard-subset-rule}, $\Sigma~[0,1]$ is a sub-$\sigma$-algebra of $\Sigma~\Re$.

\begin{theorem}
$\arrpmap~fst$ and $\arrpmap~snd$ are measurable.
\end{theorem}
\begin{proof}
Follows from~\eqref{eqn:standard-finite-product-rule} and Definition~\ref{def:finite-product-sigma-algebra}.
\end{proof}

\begin{theorem}[$AStore$ measurability transfer]
Every $AStore$ arrow combinator produces measurable mapping* computations from measurable mapping* computations.
\label{thm:astore-measurability-transfer}
\end{theorem}
\begin{proof}
$AStore$'s combinators are defined in terms of the base arrow's combinators and $\arrpmap~fst$ and $\arrpmap~snd$.
\end{proof}

\begin{theorem}
For all $j \in J$, $\arrpmap~(\pi~j)$ is measurable.
\end{theorem}
\begin{proof}
Follows from~\eqref{eqn:standard-arbitrary-product-rule} and Definition~\ref{def:arbitrary-product-sigma-algebra}.
\end{proof}

\begin{corollary}
$random\pmap$ and $branch\pmap$ are measurable.
\end{corollary}

\begin{theorem}
$\ifpmap'$ is measurable.
\end{theorem}
\begin{proof}
$branch\pmap$ is measurable, and $\arrpmap~agrees$ is measurable by~\eqref{eqn:standard-countable-rule}.
\end{proof}

\begin{theorem}[probabilistic programs are measurable]
If $\meaningof{\mathit{e}}\pmap$ converges, it is measurable.
If $\meaningof{\mathit{e}}\pmap'$ converges, it is measurable.
\label{thm:everything-is-measurable}
\end{theorem}
\begin{proof}
By structural induction over $\mathit{e}$ and the above measurability theorems.
\end{proof}

Of course, Theorem~\ref{thm:everything-is-measurable} remains true when $\meaningof{\cdot}\gen$ is extended with any rule whose right side is measurable.
Examples include arithmetic, equalities and inequalities, and lambda expressions transformed into closures.

\subsection{Random Store Probabilities}

Preimages under probabilistic programs are measurable subsets of $(R \times T) \times X$.
While it is possible to put probability measures on such domains, doing so would be surprising for end-users.
For example, the probabilities of outputs of the $geometric$ function defined in~\eqref{eqn:geometric-def} would depend not only on the probability of $random < p$, but also on some arbitrary probability that each branch is taken.
It would not define the geometric distribution.

We therefore have to measure \emph{projections} of subsets of $(R \times T) \times X$.
Unfortunately, projected sets are generally not measurable.
Fortunately, ours is a special case: the excluded dimensions are countable.

As previously, we start with measuring the halting set.

\begin{definition}[standard probability measure]
For a type $X$, a \mykeyword{standard probability measure} is a probability measure $P \in \powerset~X \pto [0,1]$ where $domain~P = \Sigma~X$.
\end{definition}

\begin{definition}[halting probability]
Let $g : X \pmapto Y$ be measurable, with $A^*$ its halting set.
Let $P \in \powerset~R \pto [0,1]$ be a standard probability measure over random stores.
The \keyword{halting probability} of $g$ is $P~(image~(fst~\arrowcomp~fst)~A^*)$.
\end{definition}

\begin{theorem}[measurable finite projections]
Let $A \in \Sigma~\pair{X_1,X_2}$.
If $X_2$ is at most countable, $image~fst~A \in \A_1$.
\label{thm:measurable-projections}
\end{theorem}
\begin{proof}
Because $\Sigma~X_2 = \powerset~X_2$, $A$ is a countable union of rectangles of the form $A_1 \times \set{a_2}$, where $A_1 \in \Sigma~X_1$ and $a_2 \in X_2$.
Because $image~fst$ distributes over unions, $image~fst~A$ is a countable union of sets in $\Sigma~X_1$.
\end{proof}

\begin{theorem}
Let $g : X \pmapto Y$ be measurable.
If $X$ is at most countable, $g$'s halting probability is well-defined.
\end{theorem}
\begin{proof}
$T$ is countable; apply Theorem~\ref{thm:measurable-projections} twice.
\end{proof}

In particular, for programs interpreted using $\meaningof{\cdot}\pmap$, $X = \set{\pair{}}$ (the empty list/stack), so their halting probabilities are well-defined.

\begin{corollary}
Let $g : X \pmapto Y$ be measurable and $A \subseteq A^*$ a measurable set.
$P~(image~(fst~\arrowcomp~fst)~A)$, the probability of $A$, is well-defined.
\end{corollary}

In particular, for any converging $g := \meaningof{\mathit{e}}\pmap'$, preimages $A$ of measurable subsets $B$ are measurable, and the random store component of $A$ has a well-defined probability.


\section{Implementable Approximation}

XXX: $\arrpre$ is generally uncomputable, but we don't need that many lifts; Figure~\ref{fig:extra-preimage-arrow-defs} has the rest of the non-arithmetic ones we'll need

XXX: figure out a good way to present the following info

Figure~\ref{fig:mapping-arrow-defs}:
\begin{itemize}
	\item $pre$: can't implement
	\item $pre!ap$: need $\i$
	\item $\pair{\cdot,\cdot}\pre$: approximate; need $\times$ and $\i$
	\item $\circ\pre$: no change
	\item $\uplus\pre$: approximate; need join
\end{itemize}

Figure~\ref{fig:preimage-arrow-defs}:
\begin{itemize}
	\item $\arrpre$ (and $\liftpre$): can't implement
	\item $\comppre$: no change
	\item $\pairpre$: use approximating $\pair{\cdot,\cdot}\pre$
	\item $\ifpre$: need $\set{true}$ and $\set{false}$; use approximating $\uplus\pre$
	\item $\lazypre$: need $(= \emptyset)$, $(pre~\emptyset)$
\end{itemize}

Figure~\ref{fig:extra-preimage-arrow-defs}:
\begin{itemize}
	\item $id\pre$: no change
	\item $const\pre$: need $\set{y}$, $(= \emptyset)$, $\emptyset$
	\item $fst\pre$ and $snd\pre$: need projections, $i$, $\times$
	\item $\pi\pre$: need projections, $\i$, arbitrary products
\end{itemize}

\begin{figure*}[t]\centering
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&id\pre : X \preto X \\
		&id\pre~A \ := \ \pair{A,\fun{B}{B}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&const\pre : Y \tto X \preto Y \\
		&const\pre~y~A \ := \ \pair{\set{y},\fun{B}{if~(B = \emptyset)~\emptyset~A}}
	\end{aligned} \\
\\[-6pt]
&\begin{aligned}
	&\pi\pre : J \tto (J \to X) \preto X \\
	&\pi\pre~j~A \ := \ 
		\lzfclet{
			A_j & proj~j~A \\
			p & \fun{B}{A \i \prod_{i \in J} if~(j = i)~B~(proj~i~A)}
		}{\pair{A_j,p}}
\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&fst\pre : \pair{X,Y} \preto X \\
		&fst\pre~A \ := \ 
			\lzfclet{
				A_1 & image~fst~A \\
				A_2 & image~snd~A
			}{\pair{A_1,\fun{B}{A \i (B \times A_2)}}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&snd\pre : \pair{X,Y} \preto Y \\
		&snd\pre~A \ := \ 
			\lzfclet{
				A_1 & image~fst~A \\
				A_2 & image~snd~A
			}{\pair{A_2,\fun{B}{A \i (A_1 \times B)}}}
	\end{aligned}
\end{aligned}
\end{align*}
\hrule
\caption{Specific instances of $\arrpre~f$}
\label{fig:extra-preimage-arrow-defs}
\end{figure*}

\section{Computable Approximation}





%\appendix
%\section{Appendix Title}
%This is the text of the appendix, if you need one.

%\acks
%Acknowledgments, if needed.

\bibliographystyle{abbrvnat}
\bibliography{plt}

\end{document}
